{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Thank you to HOMLP and Andrej Karphathy (https://karpathy.github.io/2015/05/21/rnn-effectiveness/) for guide material."
      ],
      "metadata": {
        "id": "7P4Y7BQ6F2yc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "CvTV7URfkj56",
        "outputId": "f5a945db-4fb9-4ab9-8cdf-53af2398d1cd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "def download_shakespeare_text():\n",
        "  path = Path('datasets/shakespeare/shakespeare.txt')\n",
        "  return path.read_text()\n",
        "\n",
        "shakespeare_text = download_shakespeare_text()\n",
        "shakespeare_text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(shakespeare_text.lower()))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP3DnmVglj0L",
        "outputId": "1c27a15c-1562-4af9-a795-e16018942cf8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
        "id_to_char = {index: char for index, char in enumerate(vocab)}\n",
        "id_to_char[13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OOWQ3p5slpEk",
        "outputId": "334671c3-2c40-42a9-b1e8-fd58db527088"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def encode_text(text):\n",
        "  return torch.tensor([char_to_id[char] for char in text.lower()])\n",
        "\n",
        "def decode_text(char_ids):\n",
        "  return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])"
      ],
      "metadata": {
        "id": "duefXWzemEkH"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = encode_text(\"mama jama\")\n",
        "decode_text(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0QHu6QlQmpWW",
        "outputId": "f28c0ad8-1487-422c-cbd2-05aa69cf18dc"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mama jama'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ywKLh0BBlO_D",
        "outputId": "550a720b-1e7f-4ff7-a4d6-9ea87b4c0fee"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "  def __init__(self, text, window_length):\n",
        "    self.encoded_text = encode_text(text)\n",
        "    self.window_length = window_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.encoded_text) - self.window_length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      if idx >= len(self):\n",
        "          raise IndexError(\"Index out of range\")\n",
        "      end = idx + self.window_length\n",
        "      window = self.encoded_text[idx:end]\n",
        "      target = self.encoded_text[idx+1:end+1]\n",
        "      return window, target\n"
      ],
      "metadata": {
        "id": "jfpQqz0AndIi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_length = 50\n",
        "batch_size = 512\n",
        "\n",
        "train_set = CharDataset(shakespeare_text[:1_000_000], window_length)\n",
        "valid_set = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length)\n",
        "test_set = CharDataset(shakespeare_text[1_060_000:], window_length)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "xKgV0aJwoPkW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "sTC9cdUfojD9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
        "    self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, X):\n",
        "    embeddings = self.embed(X)\n",
        "    outputs, _states = self.gru(embeddings)\n",
        "    return self.output(outputs).permute(0,2,1)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = ShakespeareModel(len(vocab)).to(device)"
      ],
      "metadata": {
        "id": "QvqByKcHhzci"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reusing evaluate and train functions from past notebooks:\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "def evaluate_tm(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            metric.update(y_pred, y_batch)\n",
        "    return metric.compute()\n",
        "\n",
        "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader,\n",
        "          n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", patience=patience, factor=factor)\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.0\n",
        "        metric.reset()\n",
        "        model.train()\n",
        "        if epoch_callback is not None:\n",
        "            epoch_callback(model, epoch)\n",
        "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "            train_metric = metric.compute().item()\n",
        "            print(f\"\\rBatch {index + 1}/{len(train_loader)}\", end=\"\")\n",
        "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
        "            print(f\", {train_metric=:.2%}\", end=\"\")\n",
        "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
        "        history[\"train_metrics\"].append(train_metric)\n",
        "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
        "        history[\"valid_metrics\"].append(val_metric)\n",
        "        scheduler.step(val_metric)\n",
        "        print(f\"\\rEpoch {epoch + 1}/{n_epochs},                      \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.2%}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.2%}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "T593vfkep-1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1b18829b-48e5-431b-b168-90f298b38e61"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (26.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.10.0+cu128)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.24.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0.0->torchmetrics) (1.3.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "xentropy = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.NAdam(model.parameters())\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
        "                                 num_classes=len(vocab)).to(device)\n",
        "\n",
        "history = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,\n",
        "                n_epochs)"
      ],
      "metadata": {
        "id": "18HmcQ1mp-yS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "cae35929-50bd-401a-a2dd-17c2347c42fc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20,                      train loss: 1.6040, train metric: 51.28%, valid metric: 51.98%\n",
            "Epoch 2/20,                      train loss: 1.3843, train metric: 56.72%, valid metric: 52.83%\n",
            "Epoch 3/20,                      train loss: 1.3547, train metric: 57.46%, valid metric: 53.64%\n",
            "Epoch 4/20,                      train loss: 1.3403, train metric: 57.81%, valid metric: 53.45%\n",
            "Epoch 5/20,                      train loss: 1.3320, train metric: 58.02%, valid metric: 53.32%\n",
            "Epoch 6/20,                      train loss: 1.3264, train metric: 58.16%, valid metric: 53.79%\n",
            "Epoch 7/20,                      train loss: 1.3225, train metric: 58.26%, valid metric: 53.71%\n",
            "Epoch 8/20,                      train loss: 1.3193, train metric: 58.34%, valid metric: 54.09%\n",
            "Epoch 9/20,                      train loss: 1.3167, train metric: 58.40%, valid metric: 54.33%\n",
            "Epoch 10/20,                      train loss: 1.3148, train metric: 58.45%, valid metric: 54.08%\n",
            "Epoch 11/20,                      train loss: 1.3131, train metric: 58.48%, valid metric: 54.36%\n",
            "Epoch 12/20,                      train loss: 1.3117, train metric: 58.51%, valid metric: 54.23%\n",
            "Epoch 13/20,                      train loss: 1.3104, train metric: 58.55%, valid metric: 54.38%\n",
            "Epoch 14/20,                      train loss: 1.3094, train metric: 58.57%, valid metric: 54.34%\n",
            "Epoch 15/20,                      train loss: 1.3083, train metric: 58.59%, valid metric: 54.43%\n",
            "Epoch 16/20,                      train loss: 1.3075, train metric: 58.61%, valid metric: 54.50%\n",
            "Epoch 17/20,                      train loss: 1.3066, train metric: 58.63%, valid metric: 53.83%\n",
            "Epoch 18/20,                      train loss: 1.3058, train metric: 58.65%, valid metric: 54.68%\n",
            "Epoch 19/20,                      train loss: 1.3051, train metric: 58.68%, valid metric: 54.22%\n",
            "Epoch 20/20,                      train loss: 1.3048, train metric: 58.68%, valid metric: 54.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "pA-OMpXkp-vS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea59186-4fd9-4aa7-eedd-50f78d71587c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShakespeareModel(\n",
              "  (embed): Embedding(39, 10)\n",
              "  (gru): GRU(10, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (output): Linear(in_features=128, out_features=39, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"to be or n\"\n",
        "encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
        "with torch.no_grad():\n",
        "  Y_logits = model(encoded_text)\n",
        "  predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
        "  predicted_char = id_to_char[predicted_char_id]\n",
        "\n",
        "predicted_char"
      ],
      "metadata": {
        "id": "0rcybF6Sp-sK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70996c48-f610-4774-86a4-3be177e3333b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'o'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def next_char(model, text, temperature=1):\n",
        "  encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
        "  with torch.no_grad():\n",
        "    Y_logits = model(encoded_text)\n",
        "    Y_probas = F.softmax(Y_logits[0,:,-1]/temperature, dim=-1)\n",
        "    predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n",
        "  return id_to_char[predicted_char_id]"
      ],
      "metadata": {
        "id": "XSpvMtaup-mK"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_text(model, text, n_chars=200, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text+= next_char(model, text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "rkrLPJXcp-ja"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text(model, \"To be or n\", temperature=0.4))"
      ],
      "metadata": {
        "id": "RXkLOd9lp-gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec62118-3719-4669-af25-87098aa2a66e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not thy son,\n",
            "the common vault for his house of heart the bear.\n",
            "\n",
            "first citizen:\n",
            "and why the princes to my son, and the destroy'd them off.\n",
            "\n",
            "first servingman:\n",
            "come, come, sir, a court the people to them \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Greedy Decoding Version\n",
        "\n",
        "def next_greedy_char(model, text):\n",
        "  encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
        "  with torch.no_grad():\n",
        "    Y_logits = model(encoded_text)\n",
        "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
        "  return id_to_char[predicted_char_id]\n",
        "\n",
        "def extend_greedy_text(model, text, n_chars=200):\n",
        "  for _ in range(n_chars):\n",
        "    text+= next_greedy_char(model, text)\n",
        "  return text\n",
        "\n",
        "print(extend_greedy_text(model, \"Shakespeare is just like Cartman, only\"))"
      ],
      "metadata": {
        "id": "z6wSW8RNp-da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a89fc61-8104-456d-efb4-efc8a4a75405"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare is just like Cartman, only the country,\n",
            "and the common that shall be the county and the state of the county and the county\n",
            "and the county and the county and the county and the county and the county\n",
            "and the county and the count\n"
          ]
        }
      ]
    }
  ]
}
