# Goal

Continuing with going through HOMLP to get DL basics. Now we're on chapter 10.

# Debrief
### Questions
### Answers

# Notes

So, now I'll re-explain  first part of HOMLP chapter 10.

Basically pytorch is the current most popular library for deep learning created by meta, now maintained by open source community.

It's fundamental data structure is tensor. Tensor is a multidimensional array, think matrix. 

Tensor support several data types: integer, complex number (3+4j), float, binary. But it can be only one at a time. You can't have tensor with mixed data types, it would revert to the most general: complex > float > integer > binary.

Tensor doesn't support strings or objects. Floats in tensor are by default 32 bits, different from numpy ndarrays that use 64. This makes sense as for deep learning this saves you literally twice the memory.

You can upload tensors onto the gpu and run computations there, which usually gives you a speed up in performance compared to CPUs. Though sometimes loading data on GPUs can be the performance bottleneck and shallow operations are better run on CPU. GPU has memory that you need to keep track of, as it has limit, and you can't overload it. 游리 => gpu speed and ram both matter.

What makes pytorch great is autograd. Autograd is the thing that efficiently calculates backward pass gradients for each NNs parameters, which allows for overall efficient computation of backprop.

It does that by creating computation graph automatically everytime forward pass is done. This computation graph keeps track of the operation that were performed on some tensor variable, using the memory about those computations, it can efficiently calculate gradients during backward pass. 

This makes pytorch dynamic, allowing to use loops and conditionals.

Two quirks of pytorch is that when you calculate gradient for parameter twice, it would store its cumulative value (2x), so you need to empty it before running.

What's the other quirk? You should avoid in place operations that could save you memory, smtth like x+=1
You should create new variables that get copied separately: x=x+1

Why? Because pytorch wouldn't be able to store its computation graph (as it is already full with past operation). 

游릭 => Overall this is a good, accurate explanation. Going back to understanding.

---

So, I made a little further progress, let's integrate again.

So we stopped on pytorch using autograd to calculate computational graph to them calculate backprop.

Basically we know enough to implement a linear regression in pytorch.

What do we need?

First load and split the dataset.

Then scale it using standard scaler, we did it by hand to practice.

Then define number of epochs and learning rate.

Now generate random weights and set bias to zero.

Finally we can write a training function.

Here's pseudo code reimplementation:

for epoch in range(epoch):
    y_pred = X@w + b
    loss = ((y_pred-y_train)**2)/mean
    loss.backwards() # i.e. calculate gradient for each parameter with respect to loss, using autograd hell yeah

    (do that all without computational graph accounting*)
    w=w-learning_rate*gradient
    b=b-learning_rate*gradient

    (lastly, drop gradients to repeat)
    w.drop()
    b.drop()

Obviously the syntax is off, but it is correct!

What's next.

Well, model is trained we can make predictions.

Now let's talk higher level pytorch. Basically there's a class torch.nn.Module called Module. It is a fundamental class you use to train, build NNs. It stores all you need for it.

It has different types, like nn.Linear, depending on what model you need to train.

One useful thing it does is find and work with Parameters of the model, instead of any tensor within it.

Parameter is a subclass of torch.Tensor, so it inherits all of its functionality, with one difference that when defined inside .Module it will be treated as model's parameter, and thus be used to calculate gradients, update etc. This is useful as we avoid treating any tensor inside model as parameter, and sometimes we need to have them their, like for cache in RNN cases.

With that in mind, we can train our linear model just a bit simpler.

When defining random weights and biases it is done simpler, and they're instantly defined as parameters.

when training the model we use model.forward() to calculate prediction and get computation graph for parameters. Also loss has MSELoss method. backwards works the same, and it calculates it for parameters only.

changingg weights and biases is done nicer with model.optimizer()
and we still need to drop gradients at the end with model.drop()

游리 => Oh yes, and we run inference within torch.no_grad() to avoid calculating computational graph that we won't need, which makes computation faster and saves us some memory.

游릭 => Overall good recall, let's move further and build MLP with pytorch!

---







