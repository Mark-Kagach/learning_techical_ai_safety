# Goal

The goal is to integrate chapter 9, 10, and 11 from HOMLP.

Then I'll go back to understanding, specifically 13th chapter about RNNs!

# Debrief

üü¢ => Chapter 9 is good, solid recall.
üü¢ => Chapter 10 is good, recalled at the level of details that matter.
üü¢ => Chapter 11 nearly perfect, forgot to mention regularization, that say batch normalization and early stopping are already good and there are a few other techniques like dropout. (I think I'll add understanding at least what momentum optimier does and how it's different from vanial gd.)

Alright, good quick session, onto RNN chapter!

# Notes

So, neural networks. Chapter 9 is about basic stuff, like how pitts in 1943 invents NN architecture, how neurons are binary, organized in layers and connections are based on logic, showing that you can express any logical proposition that way. Neurons have a threshold that must be passed of k input neurons being active, say 2 or more.

In 1957 rosenblatt improves by making i/o numbers, connections have weights, neurons have bias terms, i.e. neurons do linear function of its input neurons and then apply a step-function activation. 

In 1969 Marvin Minsky points out that perceptrons can't even do basic stuff like model XOR, only linearly separable data, which is super true!

In 1970 linmaanaa (smth) invents autodiff for tracing contribution of each connection to the loss of the whole system.

in 1985 hinton and others apply autodiff to neural network, change activation function to logistic function and voila we have a backprop that can calculate gradient of each parameter with respect to models loss. I.e. you can tell how much each connections contributes to loss, and thus perform gradient descent on each parameter, and thus sensibly train a deep learning network!

Then chapter 9 talks about hyperparameter basics of NNs:

N of layers, even though theoreticall with enough neurons even a one layer network would be able to approximate any continuous function, we go for depth in layers because they are exponentially more efficient based on number of neurons (i.e. parameters) because NN can learn low level representations and then combine them into higher level ones!

For simple task 1-5 layers should be enough, for deep nn's we go for dozens if not hundreds.

N of neurons per layer. The approach is pants stretching, where we try to give the model definitely sufficient number of parameters so it is powerful enough, so we're not worried about it underfitting and not being able to learn complex enough representations, but then we decrease the number of neurons to fit the model just right, so it doesn't overfit. This is a better approarch compared to starting with too little neurons. (Also this avoids bottleneck layers that prevent whole network from properly learning.)
Past technique was to use pyramid style n of neurons where we do many at first, then decreasing, forcing model to compress and learn higher level representations, but empirically it seems same number of neurons in all layers seems to work just fine or better. 

Then we have learning rate

Then batch size, go for max for GPU RAM, and do learning rate warmup, if having hard time, do batch sive 32.

Then we also have activation functions and optimizers but this is more detailed.

---

So chapter 10, pytorch fundamentals! Woohoo!

So pytorch is the real deal library for dl. The main thing is that it automatically calculates the computational graph for parameters (i.e. keeping track of past operations to each parameter) which makes it sup efficient and calculating autodiff/ autograd during backprop. 

It's fundamental data structure is a tensor, multidimensional array, i.e. matrix. It can be several types, not one at a time: complex, float, integer, binary. So if you initialize tensor with several data types it would revert to the most general one as I have listed them descendingly. Tensor doesn't support objects and strings.

Pytorch has class called nn.Module which is used as the main building block for building models, either on its own or connecting them together say through nn.Sequential. It has several useful methods and parameters.

The main thing with nn is that it automatically detects what tensors are parameters and what not, it does so because we wrap parameters into subclass of tensor class tensor.Parameter (don't quote exactly the syntax‚ù§Ô∏è). It is useful to differentiate parameters from tensors as sometimes we need for training to store some other data as we'll see in rnn chapter.

once parameters are in nn we can do a few nice methods that make model training a bit cleaner. When training the model we first make prediction, then calculate loss, then do backwards for parameter gradient calculation (this is what autograd does!), then update parameters, then drop gradients to zero (as pytorch accumulates them, sup important), then move on to the next training iteration.

In pytorch we can also create custom modules, not just use nn, say when we want several inputs or several outputs, or we want wide and deep NNs that are not cleanly sequential which helps NNs to learn both high level representations, but also apply simple ones to the final output.

Oh and activation functions serve as the crucial step to making NNs all together useful and not just one big linear function, think of them as what enables us to piece neurons as lego bricks together to model the function of the data -- the more neurons, the more complex thing we can model.

mm what else.
We have torchmetrics smth library to conveniently evaluate model, and model inference should be run withoun no_grad condition so we won't calculate computational graph when doing inference for no reason, this makes pytorch faster.

Also foolish that I haven't mentioned it yet, but pytorch is really cool and we use it over sklearn because it supports gpu, tpus etc. for model training which makes it much faster than cpus due to parallel matrix calculations. We load the model onto GPU RAM (or as much as we can depending on its size, sometimes just the mini batch) and train the model there for speed.

we also have pytorch vision library for computer vision tasks, but i'm less interested in that.

Lastly i guess we have optimizing models through pytorch script which converts models to an efficient format made just for running models.

We also save and load models, we want to usually just download the weights tensor and put it inside our own structure for security and compatibility reasons, there are also libraries which make model loading simple.

mmmmmmmmmmmmmmm, idk idk, kind of it. let's do debrief!

---

So chapter 11, super duper cool, talking about training actually DEEP neural networks! Woohoo!

**Vanishing, exploding gradients.** -- Better weight initialization, better activation functions (not logistics!, also probably not relu due to dying out neurons), and batch normalization.
**Better optimizers** -- not just vanila gradient descent.
**Learning rate scheduling** -- warmups etc. usually start with a bigger one, then shrink it.
**Transfer learning** -- take lower level layers from a network performing similar task, probably keep them fixed, then keep flexible mid level layers from that network and initiate random top layers to learn higher level representations.

hmmm what else? is this it? must be at least one more thing, no?






