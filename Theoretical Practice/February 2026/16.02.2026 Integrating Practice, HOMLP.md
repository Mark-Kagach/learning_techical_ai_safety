# Goal

Continuing the HOMLP integrating practice. Chapter 9 and 10.

# Notes

So let's re-explain chapter 9 and 10.

Well, chapter 10 is about fundamentals of neural networks. How pitts in 1943 first introduced the architectured but had binary neurons with logical connections and thus could do any logical propositions. Then Rosenblatt in 1957 improved upon it by making i/o numbers, adding weights to connections and having bias terms. Hence neurons are doing linear function but they also apply activation function. In his architecture is was step wise function.

How minsky in 1969 criticized the architecture because it can't do even basic stuff like XOR because it can classify only linear data.
How solution to that is stacking several layers of multi-layer perceptrons. And how activation functions play a crucial role to make such architecture be able to approximate any continuous function. And how it is put together like stiches based on each neuron and its activation function.

How Linnmaana in 1970 smth created backpropagation, or specifically autodiff to efficiently calculate how much each parameter contributes to model's total loss, and thus be able to update each parameter separately through simple chain rule.

How hinton in 1985 implemented autodiff to neural networks and thus we have backpropagation and how they have changed activation function to logistics so one can take the derivative.

What else is there. Frankly, I think that's mostly it. 

ðŸŸ¡ => yes, this is all good, but talk about hyperparameter optimization.

So, hyperparameter optimization. NNs are very powerful models, hence prone to overfitting, hence have to be controlled.

Starting with number of layers, many times one layer is enough, sometimes 1-5 depending on the task, and if you're doing cutting-edge stuff maybe 100 layers (like in current llms).

With sufficient number of neurons you can approximate any continuous function even with one layer, which is pretty crazy! But neurons in depth, those at further layers, are exponentially more efficient architecture based on N of neurons, as model can build on top of each other, and create more complex representations from simpler ones. Say starting with noticing edges, then curves and straight lines, then shapes, then say faces. In fact, you can speed up training by taking the first initial layers from one network and using them for other task, say from face recognition to hairstyle recognition because these are general universal representations, also one of the reasons DL models are well generalizing. And then searching for good weights only in the last layers for hair styles.

So depth is important for building hierarchical representations.

Then we have number of neurons, past technique was pyramid with shriking n of neurons to force model to compress and remove noise, but nowadays we keep n of neurons the same across hidden layers. This is called streth pants technique where we give a lot of neurons to the model so we're sure it won't underfit and then decrease their number until finding good enough model. This is better than the opposite approach of giving too little neurons and then slowly increasing them.

Then we have the learning rate, which is crucial. You find a good learning rate around a half of that that would make model diverge and climb the loss  back up. You do it by recursively doing a few hundred iterations of model training starting from a very small loss and increasing it by some constant factor, say 10, and then plot the graph against the loss. You'll notice how loss would first fall, but then at one point would start to climb back up, as a rule of thumb you take 10x less when its starts to climb.

Size of training sets. Also different approaches of taking full batch so training time is faster as epochs are gone through faster (i.e. whatever you can fit on GPUs RAM), or mini batch, say 32 examples at a time. People were against full batch or GPU RAM maxxing, because it might lead to generalization issues due to poor learning rate handling, but if you warm it up, starting low and slowly increasing during training the issue isn't there. So try that first, if you still see issues, go for mini batch, specifically usually 32 is a good number.

What else. hmm, well there's activation functions and optimizers but those are extras if you have time, compute.

ðŸŸ¢ => good recall of chapter 9, this is sufficient, let's go to chapter 10.

---

So chapter 10 is about pytorch fundamentals.

sklearn is nice but if we're doing real deal deep learning we need pytorch library.

It's main thing is that it supports gpu trainings. It has fundamental data structure of tensor which is a multidimensional array i.e. matrix. 

Tensor can be several data types at a time, but not several at once, specifically: complex numbers, floats, integers, booleans.
This is the ordering of generalizability also for when you init a tensor with several types.

Tensors are nice, but what makes pytorch the real deal is autograd. Autograd is that autodiff implementation of linmaannaa but in pytorch.

Basically whenever you do operations to a tensor, especially when it is the training one about that in a minute, it will create a computational graph that would store all the past operations that have happened to tensor. This is done so aftewards it can easily calculate the backprop on a backward pass getting a gradient for each parameter. This is what makes pytorch so nice and useful. 

mmm, what else what else.

So let's get more concrete. Say we want to train a model, we need to specify that this tensor will be used for training so pytorch keeps track of its computational graph, then we do backprop, and then we need to manually drop gradients, as otherwise they would cumulate throughout training and produce garbage.

But this is pretty low level training implementation in pytorch, we can use higher level apis.

Specifically, we have nn.Module that is a building block for training any models, we can put them together with nn.Sequential block or train one at a time, but that likely are small models.

nn.Module is a class that has all useful methods and properties for training models. It has nn.Linear etc.

But to train a model we need to wrap its parameters in a special tensor class which would make nn.Module recognize them as parameters, we don't use regular tensor as that helps to differentiate things during more complex model trainings where we might want to store some additional info about model besides its parameters, say for RNNs.
I forgot the name of that class, but it is a subclass of torch.tensor so it inherits all its stuff with only difference of nn.Module parameter identification. => the name is nn.Parameter => Good name!

What else. When running inference we should keep model within .no_grad so pytorch runs faster because it doesn't keep track of computational graph.

Let's talk mini batch training and evaluation.

Oh, right, hardware acceleration. Basically this is about uploading our model onto gpu ram and training it there and using the parallel computation to get things done faster, like 29x faster! Just gotta watch out for gpu ram. => .to()

to use mini batch training, we have special class in pytorch called dataloaders. But it assumes the training dataset we'll give to it will have a few methods, so we wrap it around other pytorch class with those methods. The class is called TensorDataset (looked up the name.)

Oh and also model.train() vs model.eval

Ah yes, model evaluation is done with another library torchmetrics, and if we train our model in batches, we evaluate it in batches too, then aggregating our evaluations.

ðŸŸ¢ => overall pretty good recall, can move further in chapter 10.
