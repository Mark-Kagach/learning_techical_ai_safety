# Goal
Study and understand chapter 19 of AI modern approach (depending on how it goes pursue 20, 21, 22, 23).
The process is: 
1. make first round notes on the chapter
2. revisit the notes and making more cohesive ones
3. in last revision understand it all, connect to past knowledge and prepare everything for future maintenance via flashcards

# Notes

Today spend most of the day going over computational learning theory, actually understood what PAC learning is, what is sample complexity and why from the standpoint of PAC learning we need to restrict hypothesis space.

Attaching remade notes on the topic:
- How can we be sure that our learned hypothesis will predict well for previously unseen inputs? That is, how do we know that the hypothesis h is close to the target function f if we don’t know what f is? These questions have been pondered for centuries, by Ockham, Hume, and others. In recent decades, other questions have emerged: how many examples do we need to get a good h? What hypothesis space should we use? If the hypothesis space is very complex, can we even ﬁnd the best h, or do we have to settle for a local maximum? How complex should h be? How do we avoid overﬁtting? How can we be sure that our learned hypothesis will predict well for previously unseen inputs? That is, how do we know that the hypothesis h is close to the target function f if we don’t know what f is? These questions have been pondered for centuries, by Ockham, Hume, and others. In recent decades, other questions have emerged: how many examples do we need to get a good h? What hypothesis space should we use? If the hypothesis space is very complex, can we even ﬁnd the best h, or do we have to settle for a local maximum? How complex should h be? How do we avoid overﬁtting? This
- 
- How many examples do we need for learning 
- Are there general principles governing the number of examples needed?
- Such questions are addressed by Computational Learning Theory.
- The main idea is that any Hypothesis Function that is consistent with large set of training examples is unlikely to be seriously wrong: it must be Probably Approximately Correct.
- Learning Algorithm that returns Probably Approximately Correct hypotheses is called PAC Learning.
- 
- PAC Learning Theorems are Logical consequences of Axioms.
- PAC Learning Theorems assume that:
    - (1) future examples will be drawn from the same fixed distribution as past examples
    - (2) True Function f(x) is deterministic 
    - (3) and part of Hypothesis Space \mathcal{H} that is considered
    - 
- The simplest PAC Learning Theorems deal with Boolean Functions for which 0/1 Loss (error(h)) is appropriate. 
- Hypothesis Function h is approximately correct if error(h) ‒ probability that h misclassifies new example is less or equal to  \epsilon, which is some small constant: error(h) \leq \epsilon
- 
- From that we can find N, which is the number of training examples needed to say with high confidence that all still consistent functions are approximately correct.
- Approximately correct functions are "close" to the True Function f(x) in Hypothesis Space \mathcal{H}. We say they are inside the \mathcal{}\epsilon-ball around the true function. Hypothesis space outside of \mathcal{}\epsilon-ball is \mathcal{H_{bad}}.
- So \epsilon is maximum acceptable Error Rate, aka how close to True Function Hypothesis Function should be, how big the \mathcal{}\epsilon-ball is.
- We can use N and \epsilon to derive \delta ‒ the maximum acceptable probability that "seriously wrong" hypothesis (h_b) is consistent with the N shown examples: h_b \in \mathcal{H_b}
- 
- Sample Complexity is the number (N) of required examples for PAC Learning algorithm to return "approximately correct" Hypothesis Function that is within \epsilon \land \delta.
- 
- (Book sources)
- 
- \mathcal{H} is the set of all Boolean Functions for n attributes. | | represents Cardinality which means the number of functions within some set.
- 
|\mathcal{H}| = 2^{2^n}
 means that in the \mathcal{H} set of all Boolean Functions with n attributes, there are 2^{2^n}
 number of functions.
- (Boolean Functions ‒ functions that take boolean inputs and produce boolean output.)
- 
- Now we can connect that to Sample Complexity idea. So the question is: 
- How many training examples N do we need to say that Hypothesis Function h is "probably approximately correct" within \epsilon and \delta, when considering all Boolean Functions \mathcal{}\mathcal{H}?
- |\mathcal{H}| determines Sample Complexity N.
- Because |\mathcal{H}| =2^{2^n}, Sample Complexity N grows as 2^n. This is a huge number of examples needed to see. 
- Practically, you'll need to see nearly all possible Hypothesis Functions to make any PAC Learning approximation. So this is not generalizing, this is memorization.
- This is a serious problem for our Learning Algorithms, hence, we need to restrict Hypothesis Space \mathcal{H}.
- So all of this is to say that: 
    - Restricting Hypothesis Space \mathcal{H} is necessary not only because of Computational Intractability (enough resources for search). 
    - But also because of Sample Complexity (Sample Tractability) ‒ we want to generalize and use a sensible number of examples. (We want PAC Learning math work out as it gives rigorous foundation for how good at generalization we are.)
    - (Why making PAC Learning math work is important?)
    - 
- 
- PAC Learning (both theorems and math) is a rigorous statistical framework to quantify how good our models generalize. 
- 
- (More book sources)
- 
- SO!
- There are three ways of restricting Hypothesis Space \mathcal{H}:
- 
- So, let's see one such restricted hypothesis space in detail. Specifically, how to apply PAC Learning to a new Hypothesis Space: Decision Lists.
- Decision List is a series of tests, where each test is a conjunction of literals.
- Here's how we can apply PAC Learning to Decision Lists if we limit their Hypothesis Space \mathcal{H}:
- This shows that we can find probably approximately correct hypothesis function within a reasonable number of training examples.
- The next task is to create an efficient search algorithm that would find such good hypothesis function:
- 
