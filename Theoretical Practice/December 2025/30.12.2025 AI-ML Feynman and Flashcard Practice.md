# Goal

Reviewing all AI-ML knowledge I have learned in the past month.

## Questions

## Debrief
1. Atomic, structured, and factored representations can be used for both representing the environment and agent's internal knowledge about the environment.
2. Hard time remembering discrete vs continuous, static vs dynamic, known vs unknown.
3. Wasn't confident at explaining agent's main parts: sensors, program, actuators, function. And how the job is just to map sensors to actions.
4. Entirely forgot learning agent components: performance element, critic, learning element, problem generator.
5. I'm poor at recalling the general parts of search subfield.

# Notes

So as I remember I was struggling with environment types, and how to represent them.

Environment types:

1. Fully-observable vs partially observable -- do you have the full information of everything that is happening in the environment or not. Chess is fully observable, poker is partially.
2. Deterministic vs non-deterministic -- Does the next state in the environment fully reliant on the agent, or there are other factors? (Stochastic if you're dealing with explicit probabilities like 25% chance it will rain.)
3. Sequential vs episodic -- does agents past actions influence the future ones, or is it one off decision? Detecting falty parts in manufacturing is episodic. Chess is sequential.
4. Single vs multiagent (cooperative vs competitive) -- finding shortest route in graph is single player, chess is competitive multiagent, self-driving surrounded by other robot cars is cooperative multiagent.
5. 游리 are the states, actuators, and sensors smooth or one at a time ... => discrete vs continuous
6. 游리 ... Can agent have clear time to think without states changing, or he should account that states will change? Chess vs shooter. Online vs offline agent. => Static vs Dynamic
7. 游댮 => known vs unknown => whether the laws of environment (physics) are given to the agent, or whether they have to find them out on their own.

--

PEAS is a useful acronym to represent the problem agent faces. P is performance metrics (goal), e is environment (the problem, world agent has to navigate), actuators is what it has to steer the environment, sensors is what it has to model perceive it.

--

Environment class is the set of all possible states in some given environment.

--

It is actually not said whether atomic, structured, or factored representations are about representing environment in general, or specifically the agent's knowledge of the environment. I think it applies to both. => checked the book, can be used for both.

Atomic is black boxes with relations between each other, structured is where these black boxes also have variables with values, and factored is when there boxes can have graph structured within themselves, think recursion. 

--

Okay, next we can talk about types of agents. We have agents that do simple search, agents that have internal knowledge about the environment and manipulate (reason) about it before doing actions, and those that can learn about environment and what actions do better than others in manipulating it. The last piece represents the level of hand-craftedness of the agent. 

游리 Agents have 3 parts: sensors that give info about the environment, actuators that can manipulate the environment, and agent program that given sensors translates them into actions to achieve some end-goal.

游리 AI engineers job is to write an agent program that finds/ implements an agent function that translates sensors into actions that optimizes for performance measures.

Agents can be goal-based (binary), utility based (analog goals), knowledge-based (with internal representations), and learning based (where they can add knowledge besides just hand-crafted given one). 

--

游댮 Kind of entirely forgot the agent types: 
1. Simple reflex
2. Model-based agents
3. Goal-based agents
4. Utility-based agents.

--

游댮 Entirely forgot learning agent's components. => performance element (aka agent program), critic, (aka feedback from sensors and performance measure), learning element (component that can update performance element), and problem generator (that searches for good learning experiences, actions that would give a lot of information).

--

Ok.

Let's move into each subfield in more detail.

First subfiled is search.

Search in simple environments, in complex, constraint satisfaction problem that uses structured representation to differentiate between goal states and makes them non-binary, and adversarial search in competitive multiagent environments.

Search-agents operate reasonably simple environments where they have to find **__sequence of actions that reach their desired goal state__**. Search in simple environments is about environments that are: fully-observable, deterministic, static, single-agent, episodic, discrete, known.

Search in simple environments can be for example finding path between cities.

Search can be uninformed, when you're given no information about how far you're from the goal (think depth-first,breadth-first searches), or informed, with that information which is approximated by some way via heuristic function. The quality of heuristic function would largely determine the quality of informed search algorithm.

--

Search in complex environments operates in harder types of environemnts, for example non-determistic, partially observable, unknown, continuous.

Also in such cases you usually don't care about the path that led you to find the goal state, you just care about what the goal state is. For example, finding in finding the short path between points you care about the path. In finding how to optimally place 3 airports in romania you don't care what options were explored before. I differentiate between such types of searches by calling them path-finding search and state-space search respectively.

In continuous environments you'll do local search and hill-climbing, or gradient descent. In non-determistic environments you have to keep a conditional plan as you can't ideally controll all states (you have to have a plan regardless of how unlikely it is, which is an improvement for future, using decision theory = probability theory + utility theory). In partially observable environments you maintain a belief state.

--

Continue later with CSPs, knowledge-based hand-crafted agents, probabilistic reasoning, and eventually ML stuff.
