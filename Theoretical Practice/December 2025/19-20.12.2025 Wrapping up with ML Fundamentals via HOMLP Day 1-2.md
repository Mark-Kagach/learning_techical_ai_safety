# Goal

So I went through AIMA, and it gave a better theoretical understanding of ML fundamentals. 
In the next 10 days I want to wrap-up with ML fundamentals. 
I'll finish quickly going through HOMLP (latest Aurelien Geron's book), and make several [level 1 projects]([url](https://www.youtube.com/watch?v=Bx4BYXOE9SQ)) to practice/show my skills. 

From first dates of January I'll move on to studying Deep Learning and improving my project skills to [level 2 and 3]([url](https://www.youtube.com/watch?v=Bx4BYXOE9SQ)). All through AIMA, HOMLP, Deep Learning Textbook and so on.

# Notes

In those two days I went through chapter 3, 4 and 5 of HOMLP. I include some of the notes on the touched topics:

- Linear Function
    - Fundamentals
        - Linear Function take continuous-valued inputs. Both can be regression and classification. It poses significant constraints to the Hypothesis Space \mathcal{H}.
        - h_w(x) = w_1x+w_0
        - Finding h_w that best fits data is called Linear Regression. To fit a line we have to find \langle w_0, w_1 \rangle that minimize the Empirical Loss.
        - Weight Space ‒ the space defined by all possible values of Weights.
        - 
    - Univariate Linear Function
        - 
    - Multivariable Linear Function
        - Overfitting is a real problem for Multivariable Linear Function as in multidimensional Weight Space some trough might appear useful, when in fact it is just overfitting. Hence, to combat that we use Regularization.
        - L_1, L_2, Loss Function; Sparse Model ‒ model where many weights are zero, i.e. irrelevant.
        - 
        - Normal Equation/ Closed-Form Equation, using Data Matrix and Pseudoinverse:
        - Example from Supervised Machine Learning: Regression and Classification
        - 
    - Classifier Linear Function
        - Linear Function can be used for Classification. Decision Boundary is a line that separates two classes. Linear Decision Boundary is called Linear Separator. Data that can be separated like that are called Linearly Separable.
        - Alternatively, we can think of passing Classifier Linear Function through the Threshold Function.
        - To find appropriate weights interestingly we cannot use Gradient Descent, as nearly everywhere in this binary Classification Gradient will be zero.
        - Perceptron Learning Rule is how we still manage to find appropriate Weights that minimize Loss Function (also see its Training Curve):
        - 
        - Classifier Linear Function with Logistic Regression
        - Softmax Regression/  Multinomial Logistic Regression explained and implemented in sklearn (Multiclassification)
        - Cross Entropy
        - 
    - Regularization of Linear Functions 
        - What about linear models? Can we regularize them too? You may wonder why we may want to do that: aren’t linear models constrained enough already? Well, linear regression makes a few assumptions, including the fact that the true relationship between the inputs and the outputs is linear, the noise has zero mean, constant variance, and is independent of the inputs, plus the input matrix has full rank, meaning that the inputs are not colinear and there at least as many samples as parameters. In practice, some assumptions don’t hold perfectly. For example, some inputs may be close to colinear, which makes linear regression numerically unstable, meaning that very small differences in the training set can have a big impact on the trained model. Regularization can stabilize linear models and make them more accurate.What about linear models? Can we regularize them too? You may wonder why we may want to do that: aren’t linear models constrained enough already? Well, linear regression makes a few assumptions, including the fact that the true relationship between the inputs and the outputs is linear, the noise has zero mean, constant variance, and is independent of the inputs, plus the input matrix has full rank, meaning that the inputs are not colinear7 and there at least as many samples as parameters. In practice, some assumptions don’t hold perfectly. For example, some inputs may be close to colinear, which makes linear regression numerically unstable, meaning that very small differences in the training set can have a big impact on the trained model. Regularization can stabilize linear models and make them more accurate.
        - So how can we regularize a linear model? This is usually done by constraining its weights. In this section, we will discuss ridge regression, lasso regression, and elastic net regression, which implement three different ways to do that.So how can we regularize a linear model? This is usually done by constraining its weights. In this section, we will discuss ridge regression, lasso regression, and elastic net regression, which implement three different ways to do that.
        - 
        - Ridge Regression explained:
        - Ridge Regression implemented in sklearn:
        - Lasso Regression explained:
        - Lasso Regression implemented in sklearn:
        - Elastic Net Regression explained, and implemented in sklearn:
        - 
    - 
- Decision Tree
    - Fundamentals 
        - Decision Tree is a representation of a function that maps a vector of variable values to a single output value ‒ a "decision".
        - The "decision" is reached through a sequence of tests, starting at the root, and following the appropriate branch until final leaf with output value is reached.
        - Each test (node) checks the value of one of the input variables.
        - Input and output values can be discrete or continuous.
        - 
        - Example of a Decision Tree:
        - More
        - 
    - Training
        - Implementing Decision Tree in sklearn: 
        - Decision Tree Inference:
        - Estimating Class Probabilities (Multiclassification), sklearn:
        - 
        - CART Training Algorithm
        - Gini Impurity or Entropy usage for CART Training Algorithm
        - Greedy Divide-and-Conquer Strategy from AIMA
        - Our Decision Tree Learning Algorithm chooses variables with the highest IMPORTANCE. We measure it via Information Gain, which is defined through Entropy ‒ the fundamental quantity in Information Theory .
        - 
        - Decision Tree Computational Complexity
        - 
        - Regularization
        - Generalization and Overfitting: Decision Tree Pruning, \mathcal{X}^2 Pruning, Early Stopping
        - 
        - Regression
        - 
    - Limitations
        - Sensitivity to axis orientation 
        - High Variance
        - 
        - With real-valued attributes, the function y > A1 + A2 is hard to represent with a decision tree because the decision boundary is a diagonal line, and all decision tree tests divide the space up into rectangular, axis-aligned boxes. We would have to stack a lot of boxes to closely approximate the diagonal line. In other words, decision trees are good for some kinds of functions and bad for others.With real-valued attributes, the function y > A1 + A2 is hard to represent with a decision tree because the decision boundary is a diagonal line, and all decision tree tests divide the space up into rectangular, axis-aligned boxes. We would have to stack a lot of boxes to closely approximate the diagonal line. In other words, decision trees are good for some kinds of functions and bad for others.
        - Is there any kind of representation that is efﬁcient for all kinds of functions? (Hypothesis Function h, Hypothesis Space \mathcal{H}, Computational Learning Theory) Unfortunately, the answer is no—there are just too many functions to be able to represent them all with a small number of bits. Even just considering Boolean functions with n Boolean attributes, the truth table will have 2^n rows, and each row can output true or false, so there are 2^{2^n} different functions. With 20 attributes there are 2^{1,048,576} ≈ 10^{300,000} functions, so if we limit ourselves to a million-bit representation, we can’t represent all these functions.Is there any kind of representation that is efﬁcient for all kinds of functions? Unfortunately, the answer is no—there are just too many functions to be able to represent them all with a small number of bits. Even just considering Boolean functions with n Boolean attributes, the truth table will have 2n rows, and each row can output true or false, so there are 22n different functions. With 20 attributes there are 21,048,576 ≈ 10300,000 functions, so if we limit ourselves to a million-bit representation, we can’t represent all these functions.
        - 
    - More
        - Interpretability:
        - 
        - Wide application examples of Decision Trees:
        - 
        - We can evaluate the performance of a learning algorithm with a Learning Curve, as shown in Figure 19.7. For this ﬁgure we have 100 examples at our disposal, which we split randomly into a training set and a test set. We learn a hypothesis h with the training set and measure its accuracy with the test set. We can do this starting with a training set of size 1 and increasing one at a time up to size 99. For each size, we actually repeat the process of randomly splitting into training and test sets 20 times, and average the results of the 20 trials. The curve shows that as the training set size grows, the accuracy increases. (For this reason, learning curves are also called happy graphs.) In this graph we reach 95% accuracy, and it looks as if the curve might continue to increase if we had more data.We can evaluate the performance of a learning algorithm with a learning curve, as shown Learning curve in Figure 19.7. For this ﬁgure we have 100 examples at our disposal, which we split randomly into a training set and a test set. We learn a hypothesis h with the training set and measure its accuracy with the test set. We can do this starting with a training set of size 1 and increasing one at a time up to size 99. For each size, we actually repeat the process of randomly splitting into training and test sets 20 times, and average the results of the 20 trials. The curve shows that as the training set size grows, the accuracy increases. (For this reason, learning curves are also called happy graphs.) In this graph we reach 95% accuracy, and it looks as if the Happy graphs curve might continue to increase if we had more data.
        - 
    - 

- Performance Measures
    - Intro
        - Evaluation of Classification models is harder than Regression ones. 
        - Using simple accuracy of right or wrong can be problematic because if something happens only in 5% of all cases (say cancer), predicting this thing never happens will give you great outcome of being 95% correct! 
        - Also, what if one type of mistake is much more important than the other, like missing that someone had cancer (i.e. possibly killing them) vs doing false alarm that they had it (just giving them a scare).
        - Accuracy fails for all this cases, so we need better solutions.
        - 
    - Confusion Matrix: False Positives (Type I), False Negatives (Type II). 
        - Confusion Matrix shows what kind of mistakes your model is making: False Positives (Type I), or False Negatives (Type II):
            - https://remnote-user-data.s3.amazonaws.com/G_S2yyMdSJ3fNBUyd8UxW0Pxzbnk95kNt1Qo0OWaRkh6Csn2EYvcwB7IqRL3lbKGFnWVMekg3Bdb7XPmLbz0ASpDaj2VUn8tV_rOXdqI04FABHBnILPNDKbVwG4jmpWC.png
            - 
        - You can also calculate Sensitivity and Specificity ratios for them to show what % of actually positive, or actually negative cases your model classifies correctly.
        - 
        - Book Source
        - 
    - Specificity, Sensitivity, Precision, Recall, False Positive Rate, and F_1:  
        - https://remnote-user-data.s3.amazonaws.com/mZ655R6QVICkaH6A_PckVa4VR3PHhZU-l6iQPtURA90g2safTdpE80Pwp9LViDlZThHxVOYdNCovD33C-136ZpReKuTIEMK23ANqSpJpHyNTc_C3e_3z0T26pOl6kOSS.png
        - Depending on what mistakes you want to avoid more, you can evaluate models based on their Sensitivity and Specificity.
        - 
        - Specificity calculates how many out of all actually negative examples are identified correctly =
            - \frac{\text{True Negatives}}{\text{True Negatives + False Positives}}
            - (Also Specificity is just 1 - False Positive Rate.)
            - 
        - False Positive Rate calculates how many of our negative predictions are false positive = 
            - \frac{\text{False Positives}}{\text{True Negatives + False Positives}}
            - 
        - 
        - Recall (aka Sensitivity) calculates how many out of all actually positive examples we have identified correctly = 
            - \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            - 
        - Precision calculates how accurate our positive predictions are =
            - \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            - 
        - 
        - F_1 is a Harmonic Mean of Recall and Precision, it differs from a normal mean by giving more weight to low values. Hence you'll need both recall and precision high to get high F1, having only one won't cut it.
        - Book
        - 
    - Decision Threshold: ROC Curve/ Precision-Recall Tradeoff; and comparing models via AUC:
        - ROC Curve: plot of Recall vs False Positive Rate ( FP/(FP+TN) ) & AUC
        - AUC just calculates the area under the ROC curve, so we can simply compare two models (& their different decision thresholds). One with higher AUC makes less mistakes and is likely preferable (but depends on mistake type and your priorities.)
        - 
        - Besides the ROC Curve, we can make the same plot but instead of false positive rate use precision: 
        - Eventually we get into Precision-Recall Tradeoff: 
        - Deciding where to put the decision threshold in sklearn:
        - Besides choosing between the models, you might want to change Decision Threshold of some model, prioritizing different mistake type. Instead of doing many confusion matrices and comparing them, ROC Curve summarizes this data all in one graph.
        - 
    - Example comparing models using Precision/Recall curve and F_1:
    - 
    - 
    - We can also evaluate models based on their Learning Curves which plot Training Set Error and Validation Set Error throughout training iterations:
    - Bias-Variance Tradeoff:
    - 
