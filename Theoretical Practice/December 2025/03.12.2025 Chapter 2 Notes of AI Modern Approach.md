# Goal

Finished understanding (+notes) second chapter of AI Modern Approach. Next will follow the loop of explaining it (and first chapter), then remembering through cards, and applying when coding.

# Notes on the second chapter of aima:

- Agent's Basics 
    - Agents are a useful Level of Abstraction/ Mental Model  for navigating AI.
    - 
    - Agent, Environment, Sensors, Actuators, Percept Sequence, 
    - Agent Function maps percept sequence to an action, Agent Program is what is actually run on the agent, Agent Function is an abstract mathematical description.
    - 
    - Rational Agent, Performance Measures
    - Rationality  depends on four things:
        - Performance Measures (Goals).
        - Agents prior knowledge of the Environment.
        - Actions Agent can perform.
        - Agents Percept Sequence to date (akin to experiences of a human).
        - 
    - Hence the definition of a Rational Agent: 
        - For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.
        - 
    - 
    - Explore vs Exploit
    - 
    - Rationality  is not omniscience (/perfection). Rationality  maximizes expected performance, while perfection is about actual performance.
    - Rationality  is about choosing the right actions that maximize the Expected Value of achieving desired Goals based on the information available at the time. Obvious difference for anyone who played Poker: there's a difference between choosing the optimal actions and winning, one you can control, the other you can't.
    - Information Gathering, or Exploration is a crucial part in Rationality .
    - Learning  is about changing the initial (for example built-in) knowledge about the Environment and what actions could change it.
        - Fixed Action Pattern: !
        - 
    - Autonomy then, is about Agents ability to independently Learn  about Environment and change its Prior Knowledge when needed.
        - Interesting just how directly this maps onto humans and Parenting  for example.
        - 
    - 
- 
- Task Environments
    - , i.e. the problem agent has to solve.
    - PEAS ‒ Performance Measures, Environment, Actuators, Sensors.
        - https://remnote-user-data.s3.amazonaws.com/v-o52wMg5qDowYKuvu92zepPsjKfFoU0rhcKJizYsV9odrfI0JLiaYgIbxMrvG2QFg2aLbtS8aJG305mPMW9SZgFC9RdyaTvlGv4dG-dqhR_qXJji_vXBt5naSKLr7sf.pnghttps://remnote-user-data.s3.amazonaws.com/v-o52wMg5qDowYKuvu92zepPsjKfFoU0rhcKJizYsV9odrfI0JLiaYgIbxMrvG2QFg2aLbtS8aJG305mPMW9SZgFC9RdyaTvlGv4dG-dqhR_qXJji_vXBt5naSKLr7sf.png
        - 
    - 
    - Task Environments (i.e. problems) can be categorized based on several dimensions:
    - Problem Observability: Fully Observable vs Partially Observable (can be even Unobservable) ‒ whether Agents Sensors give it complete information about its Environment at all times. In chess yes, in poker no.
    - 
    - Single-Agent vs Multiagent, also has Competitive vs Cooperative dimensions. Yes this is about whether or not agent operates in an environment where there are other agents. But what constitutes other agent? In self-driving example, are other cars agents, or closer to rocks as objects in an environment? Fundamentally, it is about whether those other objects (i.e. cars) have Performance Measures they need to maximize, and whether its maximization depends on our main Agent's behavior.
    - 
    - Deterministic vs Non-Deterministic ‒ whether the next state of the Environment is completely determined by Current State and action executed by Agent. If it is, then it is deterministic, if not, it is Non-Deterministic. 
        - (Environment is Stochastic if it explicitly deals with probabilities of events (25% chance of rain), and Non-Deterministic if possibilities are listen without being quantified (there could be rain).) 
        - 
    - 
    - Episodic vs Sequential ‒ Do past decisions affect the future ones, or each decision is singular and separated from the rest? Chess and self-driving are Sequential because your past decisions affect the future ones. Spotting defective parts on an assembly line is Episodic ‒ each part is a unique case not connected to the previous one.
    - 
    - Static vs Dynamic ‒ whether Environment can change while agent in deliberating, Dynamic if it is, Static if it is not. We can problem is Semidynamic if agent's score changes based on passage of time, but environment itself doesn't ‒ think speed chess.
    - 
    - Discrete vs Continuous ‒ distinction of the Environment's states, the way time is handled, and how Perceptrons and actions of the agent work. Chess has discrete states, percepts and actions, self-driving is continuous. (Now this all is about useful definitions, obviously self-driving video is just images, which are just pixels, which are deep down just bits, which all deep down is quantum. But again, it is useful to differentiate discrete vs continuous.)
    - 
    - Known vs Unknown ‒ whether Agent (or designer) has full knowledge of the "laws of physics" governing the environment: Known is yes, Unknown is no. There's a difference between having complete information about everything happening in an environment and understanding perfectly the laws that govern it (imagine playing chess with fog of war, chess rules are Known, but environment is Partially Observable). 
    - 
    - Examples of Task Environment and their dimensions:
        - https://remnote-user-data.s3.amazonaws.com/5waEBjcp03wwWHNP1uG0JwXAevOUpbyN6ZbHvlU0S-pRwmjYttbtK0ug6kPVmN-U7FgvwWrLg-d24nJ6Yj4AOVfkpnVouV9--1Ono6lS3yjGiH7yrD5lKcEHd5lIkDRr.pnghttps://remnote-user-data.s3.amazonaws.com/5waEBjcp03wwWHNP1uG0JwXAevOUpbyN6ZbHvlU0S-pRwmjYttbtK0ug6kPVmN-U7FgvwWrLg-d24nJ6Yj4AOVfkpnVouV9--1Ono6lS3yjGiH7yrD5lKcEHd5lIkDRr.png
        - 
    - 
    - Life  is a Partially Observable, Multiagent, Non-Deterministic, Sequential, Dynamic, Continuous, Unknown problem.
    - 
    - Environment Class ‒ the total set of possible Environments, when running simulations assessing agent's performance we want to simulate multiple different environments to see agent's average performance. So we randomly pick them from Environment Class. (Cousin of Elad Gil's Monte Carlo Life Simulation.)
    - 
- 
- Types of Agent Structures
    - Now let's talk agent's insides. 
    - 
    - The job of AI engineers is to create an Agent Program that implements (if based on Learning Algorithm then finds) Agent Function that maps Percepts to actions (inputs to outputs). The Hardware with Sensors and Actuators that agents runs on is called Agent Architecture.
    - Agent = Agent Architecture + Agent Program
    - 
    - Difference between agent program and agent function regarding the fact that program always must take current percept as an input?
    - 
    - Four types of Agent Programs:
        - Simple Reflex Agents ‒ Agents that select actions based on the current Percept, ignoring the rest of percept history.
            - Condition-Action Rule
                - https://remnote-user-data.s3.amazonaws.com/naMUMEV0aVhMIbBEM9TLw13nEuek074OoF8mgkWxp6Jvnv8K7OTTwAIeKjmX7ZaT81Cr5_lUtoWJMSqQ7LSIofxavUdN5VonYbONZ48N-myRok28RouA927cRuW7CN9J.pnghttps://remnote-user-data.s3.amazonaws.com/jiQO3L-cgZM4MRaaKfFujEV2YN6cSk8GZY2eNIqP2Pa5TE4mFL6c6p4QBarde8Oydp_KHDc73qtJwKdgjSaI1SiibMX7-mHYS_IMi7029MexeqSUc3OvIVm_kTVdlYkQ.pnghttps://remnote-user-data.s3.amazonaws.com/jiQO3L-cgZM4MRaaKfFujEV2YN6cSk8GZY2eNIqP2Pa5TE4mFL6c6p4QBarde8Oydp_KHDc73qtJwKdgjSaI1SiibMX7-mHYS_IMi7029MexeqSUc3OvIVm_kTVdlYkQ.pnghttps://remnote-user-data.s3.amazonaws.com/naMUMEV0aVhMIbBEM9TLw13nEuek074OoF8mgkWxp6Jvnv8K7OTTwAIeKjmX7ZaT81Cr5_lUtoWJMSqQ7LSIofxavUdN5VonYbONZ48N-myRok28RouA927cRuW7CN9J.png
                - 
            - https://remnote-user-data.s3.amazonaws.com/Oj-OQfRpg9ZR3AebDaYDnEAnzfTXv0OUSSKnGYLk8IO5ndd7m8BNg29oG2lWCTM2FwxPDCQTee-ek7OcnVi0sBE1gerZW-icGaGh9AtM67PLUij8WuVXEDqMWa_gP364.pnghttps://remnote-user-data.s3.amazonaws.com/Oj-OQfRpg9ZR3AebDaYDnEAnzfTXv0OUSSKnGYLk8IO5ndd7m8BNg29oG2lWCTM2FwxPDCQTee-ek7OcnVi0sBE1gerZW-icGaGh9AtM67PLUij8WuVXEDqMWa_gP364.png
            - https://remnote-user-data.s3.amazonaws.com/PMnbsccSia0dLlQEQvRyMlcguy_kD5Cb48CFRmBoP5oLzO9pET6jvUjX1F6gs-2Rag1JXe3ExL0RfAL51wYbG4cvLhk4FRJAyDDNd4wzixcdP2l78Z3BuoGaNKsE3yXN.pnghttps://remnote-user-data.s3.amazonaws.com/Nz_J4BJU579cCq7IqPZWCvaLuRykxPz-1EhA9fQ_Ht1_e6g16VsVFaS1UVJ-BH80ckwj7DDGA5nVebv03qkBBpYrGHb6WwxCudc6AoVf5w0z5HTlFYG5oMVI31KnLioX.pnghttps://remnote-user-data.s3.amazonaws.com/pkBd-j-KxWrsR_pU5a9QVOTUPmnWv7r6hr6ZHeXoFbfkcsBAXTnPXt4xz_rtqvmzhfUXtM4qfqGrUOt0I3OesJ3lwCfyBvER0h6AFfqaB_b0Kx5AQCdK-sEZ1ewgrv5o.pnghttps://remnote-user-data.s3.amazonaws.com/pkBd-j-KxWrsR_pU5a9QVOTUPmnWv7r6hr6ZHeXoFbfkcsBAXTnPXt4xz_rtqvmzhfUXtM4qfqGrUOt0I3OesJ3lwCfyBvER0h6AFfqaB_b0Kx5AQCdK-sEZ1ewgrv5o.pnghttps://remnote-user-data.s3.amazonaws.com/Nz_J4BJU579cCq7IqPZWCvaLuRykxPz-1EhA9fQ_Ht1_e6g16VsVFaS1UVJ-BH80ckwj7DDGA5nVebv03qkBBpYrGHb6WwxCudc6AoVf5w0z5HTlFYG5oMVI31KnLioX.pnghttps://remnote-user-data.s3.amazonaws.com/PMnbsccSia0dLlQEQvRyMlcguy_kD5Cb48CFRmBoP5oLzO9pET6jvUjX1F6gs-2Rag1JXe3ExL0RfAL51wYbG4cvLhk4FRJAyDDNd4wzixcdP2l78Z3BuoGaNKsE3yXN.png
            - 
            - You can use Randomized Simple Reflex Agent as a trick to avoid basic problems deterministic agent can get into. Though Randomization has better reputation in Multiagent Task Environments where it is actually Rational .
            - 
        - Model-Based Reflex Agents ‒ agent that keeps track of the world it can't see now. It should maintain Internal State that depends on Percept Sequence (history). For example, self-driving car should keep track (remember) of the pedestrian that is crossing the street and is not visible at the moment behind a parked car.
            - https://remnote-user-data.s3.amazonaws.com/lomZ07g364tMH4fvbIzdcPsjhduu-vFS3r-GxcXf_mjubJRmvGJJJV-FdNmL6bNfX89y-QU6jGiseZ3KNW3Q2U4cCup4kQduiHFT2ZAne-giahSQZzK4tWQSutyOgR3P.pnghttps://remnote-user-data.s3.amazonaws.com/lomZ07g364tMH4fvbIzdcPsjhduu-vFS3r-GxcXf_mjubJRmvGJJJV-FdNmL6bNfX89y-QU6jGiseZ3KNW3Q2U4cCup4kQduiHFT2ZAne-giahSQZzK4tWQSutyOgR3P.pnghttps://remnote-user-data.s3.amazonaws.com/SBzkRRHhw2-9J-j6lDJvT61cz6tuvDt3w9GMjx4V9Fps4iWTiZ_rNXukAqaUBBs2vYiEzPHCH8UxeW2uTwmhDlBV8nk6xGYIvCoNAmdEMeGRLAxcz8eIyVwl1Uh5Sh9g.pnghttps://remnote-user-data.s3.amazonaws.com/SBzkRRHhw2-9J-j6lDJvT61cz6tuvDt3w9GMjx4V9Fps4iWTiZ_rNXukAqaUBBs2vYiEzPHCH8UxeW2uTwmhDlBV8nk6xGYIvCoNAmdEMeGRLAxcz8eIyVwl1Uh5Sh9g.png
            - 
            - To keep track of the external world (and update its internal state) agent needs: Transition Model and Sensor Model, usage of both makes it a Model-Based Reflex Agents.
            - Transition Model is knowledge of how the world changes over time, which is about: (1) the effects of the agent's actions, and (2) how the world evolves independently of the agent.
            - Sensor Model is the knowledge of how the state of the world appears in agent's Percepts. (For example a color blind person knowing their condition.) 
                - "For example, when the car in front initiates braking, one or more illuminated red regions appear in the forward-facing camera image, and, when the camera gets wet, droplet-shaped objects appear in the image partially obscuring the road.percepts. For example, when the car in front initiates braking, one or more illuminated red regions appear in the forward-facing camera image, and, when the camera gets wet, droplet-shaped objects appear in the image partially obscuring the road."
                - 
            - 
            - Together, the transition model and sensor model allow an agent to keep track of the state of the world—to the extent possible given the limitations of the agent’s sensors. An agent that uses such models is called a model-based agent.Together, the transition model and sensor model allow an agent to keep track of the state of the world—to the extent possible given the limitations of the agent’s sensors. An agent that uses such models is called a model-based agent.
            - Best-guesses of what the world is like:
                - https://remnote-user-data.s3.amazonaws.com/1VK2JNQu9to43DmvOvBty4yY0mfamocihxA7sj3kzJiKF5kkwGExPnrrl35OxhnlH1aO76-mRUAgfAU7us_HZskwr3QqIXGFIXVSf_AZn_eJgX_ddFBxPyu9sUUe-8AT.pnghttps://remnote-user-data.s3.amazonaws.com/S4JTfKTqRk1Q7t_lzwTVSG69C4tmMnKqz20kUMXMKZ8cpSQobi_kIwFZ7C3UPQqFdHAVPGfXMLBvlfIc2eygN3K7Y2BefSQiPhoQX0EnoGPK3XvFTpdCpJB52KuVkpDX.pnghttps://remnote-user-data.s3.amazonaws.com/S4JTfKTqRk1Q7t_lzwTVSG69C4tmMnKqz20kUMXMKZ8cpSQobi_kIwFZ7C3UPQqFdHAVPGfXMLBvlfIc2eygN3K7Y2BefSQiPhoQX0EnoGPK3XvFTpdCpJB52KuVkpDX.pnghttps://remnote-user-data.s3.amazonaws.com/1VK2JNQu9to43DmvOvBty4yY0mfamocihxA7sj3kzJiKF5kkwGExPnrrl35OxhnlH1aO76-mRUAgfAU7us_HZskwr3QqIXGFIXVSf_AZn_eJgX_ddFBxPyu9sUUe-8AT.png
            - 
        - Goal-Based Agents ‒ agents that track the world and pursue goal state. Especially hard in cases of long-term goals, requires Search and Planning.
            - https://remnote-user-data.s3.amazonaws.com/ePO9maaRaR8RTiRq6N3bpSqylMpTcVbqop9dcjJdEEMKBsUelw4HImZNb9n5SHMbW4TmDfOFwSznd5bavUkGsNBV1Wo6NHvT-0K42cyNLm58GRnyuqciq3nMg4vIYyH9.pnghttps://remnote-user-data.s3.amazonaws.com/ePO9maaRaR8RTiRq6N3bpSqylMpTcVbqop9dcjJdEEMKBsUelw4HImZNb9n5SHMbW4TmDfOFwSznd5bavUkGsNBV1Wo6NHvT-0K42cyNLm58GRnyuqciq3nMg4vIYyH9.png
            - https://remnote-user-data.s3.amazonaws.com/LpxIEBVjT_oTjOHmctSyw-Zzwrz5mBWnX3m-88g1XHKrXDCx_I6Ph671_A4aeNlcekMz8214DoE2Q3xMqcyw49I3Xg0AUcwp1QnxV0B2AMyHF_65pMJOLtS6rj19Qryt.pnghttps://remnote-user-data.s3.amazonaws.com/Xauc6SFBZMmAb4SuYczPnMjU8GrhvQUmdmWlaRhFh0yBroZZ8JS8xMOiQL4HpaHhVKHrFWfcTArjf5shrSpF_Y4WctAgM_UPpC6DG3qa9h02pXrM4UTjNb0zG0MpZth2.pnghttps://remnote-user-data.s3.amazonaws.com/Xauc6SFBZMmAb4SuYczPnMjU8GrhvQUmdmWlaRhFh0yBroZZ8JS8xMOiQL4HpaHhVKHrFWfcTArjf5shrSpF_Y4WctAgM_UPpC6DG3qa9h02pXrM4UTjNb0zG0MpZth2.pnghttps://remnote-user-data.s3.amazonaws.com/LpxIEBVjT_oTjOHmctSyw-Zzwrz5mBWnX3m-88g1XHKrXDCx_I6Ph671_A4aeNlcekMz8214DoE2Q3xMqcyw49I3Xg0AUcwp1QnxV0B2AMyHF_65pMJOLtS6rj19Qryt.png
            - 
        - Utility-Based Agents ‒ agents that track the world and pursue the best goal states based on their utility. In Goal-Based Agents goals are binary, in Utility-Based Agents goals are analogous. Agent's Utility Function is their internal model for external Performance Measures.
            - https://remnote-user-data.s3.amazonaws.com/EaObtjmzJe3F06m5ndZSaAbXPEIn3kbOMiBJJy3x-za-7PbIQNHT5ZMykcj7PR4Qk5_M9AmoDXxFy4xwDYQEN3e-aLH1pacvfQ3u6Gd0Inh1t4nEjye7ZFkHukUcir4h.pnghttps://remnote-user-data.s3.amazonaws.com/EaObtjmzJe3F06m5ndZSaAbXPEIn3kbOMiBJJy3x-za-7PbIQNHT5ZMykcj7PR4Qk5_M9AmoDXxFy4xwDYQEN3e-aLH1pacvfQ3u6Gd0Inh1t4nEjye7ZFkHukUcir4h.png
            - In Non-Deterministic Environments agents would use Expected Value calculations.
            - 
            - There are also Model-free Agents that never learn about environment, just finding right actions.
            - 
        - 
    - 
- 
- Learning Agents
    - Just insane level of insight by Turing, and boy how early:
    - Learning has another advantage, as we noted earlier: it allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow.Learning has another advantage, as we noted earlier: it allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow. In
    - 
    - Learning Agent can be divided into four components: Learning Element, Performance Element, Critic and Problem Generator.
        - https://remnote-user-data.s3.amazonaws.com/5OY7X5ZfPOh9A1tdHxrK6GIxj0gzf9yZPOQIGqksuz1vu21RkAHWPhNiWCMEJIRmN9er0iTUNGfjG1MTvTi2eSpNbJz6RdamYC1-T-jLcCPNLM9p5Hs6Grs5il_BRZNh.pnghttps://remnote-user-data.s3.amazonaws.com/5OY7X5ZfPOh9A1tdHxrK6GIxj0gzf9yZPOQIGqksuz1vu21RkAHWPhNiWCMEJIRmN9er0iTUNGfjG1MTvTi2eSpNbJz6RdamYC1-T-jLcCPNLM9p5Hs6Grs5il_BRZNh.png
        - 
        - Performance Element is what we previously considered the whole Agent Program. 
        - Now Learning Element can change it based on the feedback from Critic that tells how the agent is doing via sensors. (The feedback is based on sensors and set fixed performance standard.)
        - Problem Generator suggests actions that would be great for learning. Doing the same thing doesn't give new information, problem generator searches for actions that would give a lot of information: Explore vs Exploit. Problem generator is responsible for exploration.
        - 
        - https://remnote-user-data.s3.amazonaws.com/yJiGw7BdpXnwJNMF7FVFc4Gl6ghKJoET__k5tAdKfwS5HqUmlZsWEQQPLuPe_gEJwdwzikwY3Q8dTnM7CJSnWZKFKIIqk1cO_TtIZLA8Q4TPMW1Z7-hGdxgzJZkKGTlT.pnghttps://remnote-user-data.s3.amazonaws.com/zkmvCPq4MooFIA1ms8cbByFO21n53nsTponxZv4A67nOkUGDG6pjWb0FK07jSNPi_YLyLCX9SJjKpfzVgp9xJsp4nqxj7RRi023iRNdgYpK-pIcIFjjIU6U5P6tSI5ZI.pnghttps://remnote-user-data.s3.amazonaws.com/56oCOKdPgIpEyv7eOq3IWmHTYwh-L_jYqN2yu6keLCizQaKx5ICAUzbTHLSSBqu-H1cSEKgVBxz1IsrIbpa0QKG5UkKbxgzDMO2euyymPr38DyDp2pbDNWlMF8y5Qvac.pnghttps://remnote-user-data.s3.amazonaws.com/56oCOKdPgIpEyv7eOq3IWmHTYwh-L_jYqN2yu6keLCizQaKx5ICAUzbTHLSSBqu-H1cSEKgVBxz1IsrIbpa0QKG5UkKbxgzDMO2euyymPr38DyDp2pbDNWlMF8y5Qvac.pnghttps://remnote-user-data.s3.amazonaws.com/zkmvCPq4MooFIA1ms8cbByFO21n53nsTponxZv4A67nOkUGDG6pjWb0FK07jSNPi_YLyLCX9SJjKpfzVgp9xJsp4nqxj7RRi023iRNdgYpK-pIcIFjjIU6U5P6tSI5ZI.pnghttps://remnote-user-data.s3.amazonaws.com/yJiGw7BdpXnwJNMF7FVFc4Gl6ghKJoET__k5tAdKfwS5HqUmlZsWEQQPLuPe_gEJwdwzikwY3Q8dTnM7CJSnWZKFKIIqk1cO_TtIZLA8Q4TPMW1Z7-hGdxgzJZkKGTlT.png
        - 
    - 
- 
- How components of Agent Programs work
    - The question is: How these "components" represent Agent's Environment? 
    - Largely speaking, there are three approaches: Atomic Component Approach, Factored Component Approach, Structured Component Approach:
    - https://remnote-user-data.s3.amazonaws.com/anO6FJALzdfHPYRTjpYMGCLmpL0VC0Ip42nx0ev8aZiOIIH3QvQYue_aLk855l0zTyqQ_bV8Gse9XXOb4ZxtrgKfhZEcPR7l1mPnDmQZEO7bufQiUg976posUfTWyZy2.pnghttps://remnote-user-data.s3.amazonaws.com/anO6FJALzdfHPYRTjpYMGCLmpL0VC0Ip42nx0ev8aZiOIIH3QvQYue_aLk855l0zTyqQ_bV8Gse9XXOb4ZxtrgKfhZEcPR7l1mPnDmQZEO7bufQiUg976posUfTWyZy2.png
    - Atomic Component Approach ‒ each state of the world is indivisible (it has no internal structure).
        - In an atomic representation each state of the world is indivisible—it has no internal Atomic representation structure. Consider the task of ﬁnding a driving route from one end of a country to the other via some sequence of cities (we address this problem in Figure 3.1 on page 82). For the purposes of solving this problem, it may sufﬁce to reduce the state of the world to just the name of the city we are in—a single atom of knowledge, a “black box” whose only discernible property is that of being identical to or different from another black box. The standard algorithms underlying search and game-playing (Chapters 3, 4, and 6), hidden Markov models (Chapter 14), and Markov decision processes (Chapter 16) all work with atomic representations.In an atomic representation each state of the world is indivisible—it has no internal Atomic representation structure. Consider the task of ﬁnding a driving route from one end of a country to the other via some sequence of cities (we address this problem in Figure 3.1 on page 82). For the purposes of solving this problem, it may sufﬁce to reduce the state of the world to just the name of the city we are in—a single atom of knowledge, a “black box” whose only discernible property is that of being identical to or different from another black box. The standard algorithms underlying search and game-playing (Chapters 3, 4, and 6), hidden Markov models (Chapter 14), and Markov decision processes (Chapter 16) all work with atomic representations.
        - 
    - Factored Component Approach ‒ each state is split into variables and attributes.
        - A factored representation splits up each state into a ﬁxed set of variables or attributes, Factored representation Variable Attribute each of which can have a value. Consider a higher-ﬁdelity description for the same driving Value problem, where we need to be concerned with more than just atomic location in one city or another; we might need to pay attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil warning light is working, how much money we have for tolls, what station is on the radio, and so on. While two different atomic states have nothing in common—they are just different black boxes—two different factored states can share some attributes (such as being at some particular GPS location) and not others (such as having lots of gas or having no gas); this makes it much easier to work out how to turn one state into another. Many important areas of AI are based on factored representations, including constraint satisfaction algorithms (Chapter 5), propositional logic (Chapter 7), planning (Chapter 11), Bayesian networks (Chapters 12, 13, 14, 15, and 18), and various machine learning algorithms.A factored representation splits up each state into a ﬁxed set of variables or attributes, Factored representation Variable Attribute each of which can have a value. Consider a higher-ﬁdelity description for the same driving Value problem, where we need to be concerned with more than just atomic location in one city or another; we might need to pay attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil warning light is working, how much money we have for tolls, what station is on the radio, and so on. While two different atomic states have nothing in common—they are just different black boxes—two different factored states can share some attributes (such as being at some particular GPS location) and not others (such as having lots of gas or having no gas); this makes it much easier to work out how to turn one state into another. Many important areas of AI are based on factored representations, including constraint satisfaction algorithms (Chapter 5), propositional logic (Chapter 7), planning (Chapter 11), Bayesian networks (Chapters 12, 13, 14, 15, and 18), and various machine learning algorithms.
        - 
    - Structured Component Approach ‒ states are split into variables and attributes, and have objects that relate to each other.
        - For many purposes, we need to understand the world as having things in it that are related to each other, not just variables with values. For example, we might notice that a large truck ahead of us is reversing into the driveway of a dairy farm, but a loose cow is blocking the truck’s path. A factored representation is unlikely to be pre-equipped with the attribute TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with value true or false. Instead, we would need a structured representation, in which objects such as cows Structured representation and trucks and their various and varying relationships can be described explicitly (see Figure 2.16(c)). Structured representations underlie relational databases and ﬁrst-order logic (Chapters 8, 9, and 10), ﬁrst-order probability models (Chapter 18), and much of natural language understanding (Chapters 24 and 25). In fact, much of what humans express in natural language concerns objects and their relationships.For many purposes, we need to understand the world as having things in it that are related to each other, not just variables with values. For example, we might notice that a large truck ahead of us is reversing into the driveway of a dairy farm, but a loose cow is blocking the truck’s path. A factored representation is unlikely to be pre-equipped with the attribute TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with value true or false. Instead, we would need a structured representation, in which objects such as cows Structured representation and trucks and their various and varying relationships can be described explicitly (see Figure 2.16(c)). Structured representations underlie relational databases and ﬁrst-order logic (Chapters 8, 9, and 10), ﬁrst-order probability models (Chapter 18), and much of natural language understanding (Chapters 24 and 25). In fact, much of what humans express in natural language concerns objects and their relationships.
        - 
    - These approaches are progressing in their Expressiveness.
        - As we mentioned earlier, the axis along which atomic, factored, and structured representations lie is the axis of increasing expressiveness. Roughly speaking, a more expressive Expressiveness representation can capture, at least as concisely, everything a less expressive one can capture, plus some more. Often, the more expressive language is much more concise; for example, the rules of chess can be written in a page or two of a structured-representation language such as ﬁrst-order logic but require thousands of pages when written in a factored-representation language such as propositional logic and around 1038 pages when written in an atomic language such as that of ﬁnite-state automata. On the other hand, reasoning and learning become more complex as the expressive power of the representation increases. To gain the beneﬁts of expressive representations while avoiding their drawbacks, intelligent systems for the real world may need to operate at all points along the axis simultaneously.As we mentioned earlier, the axis along which atomic, factored, and structured representations lie is the axis of increasing expressiveness. Roughly speaking, a more expressive Expressiveness representation can capture, at least as concisely, everything a less expressive one can capture, plus some more. Often, the more expressive language is much more concise; for example, the rules of chess can be written in a page or two of a structured-representation language such as ﬁrst-order logic but require thousands of pages when written in a factored-representation language such as propositional logic and around 1038 pages when written in an atomic language such as that of ﬁnite-state automata. On the other hand, reasoning and learning become more complex as the expressive power of the representation increases. To gain the beneﬁts of expressive representations while avoiding their drawbacks, intelligent systems for the real world may need to operate at all points along the axis simultaneously.
    - 
    - Another important thing for representing Agent's Environment is how concepts are stored in memory. 
    - It can be Localist Representation (1-1 mapping between concept and memory), or Distributed Representation (spread over many memory locations, each location is used to represent many different concepts; that way similar concepts are close by).
    - Distributed Representation is more robust because any noise or error in Localist Representation would lead to entirely new concept (think from Truck getting error to Truce). But in Distributed Representation it is a point on a multidimensional space (just think 3d), so error would just mean slight movement from it (think from Truck getting to Car).
        - Another axis for representation involves the mapping of concepts to locations in physical memory, whether in a computer or in a brain. If there is a one-to-one mapping between concepts and memory locations, we call that a localist representation. On the other hand, if the representation of a concept is spread over many memory locations, and each memory location is employed as part of the representation of multiple different concepts, we call that a distributed representation. Distributed representations are more robust against noise and information loss. With a localist representation, the mapping from concept to memory location is arbitrary, and if a transmission error garbles a few bits, we might confuse Truck with the unrelated concept Truce. But with a distributed representation, you can think of each concept representing a point in multidimensional space, and if you garble a few bits you move to a nearby point in that space, which will have similar meaning.if the representation of a concept is spread over many memory locations, and each memory location is employed as part of the representation of multiple different concepts, we call that a distributed representation. Distributed representations are more robust against noiseDistributed representation and information loss. With a localist representation, the mapping from concept to memory location is arbitrary, and if a transmission error garbles a few bits, we might confuse Truck with the unrelated concept Truce. But with a distributed representation, you can think of each concept representing a point in multidimensional space, and if you garble a few bits you move to a nearby point in that space, which will have similar meaning.Another axis for representation involves the mapping of concepts to locations in physical memory, whether in a computer or in a brain. If there is a one-to-one mapping between concepts and memory locations, we call that a localist representation. On the other hand,
    - 
- 
- On the key challenge of Artificial Intelligence engineers, nothing new and groundbreaking, but I like the clarity:
    - The key challenge for AI is to ﬁnd out how to write programs that, to the extent possible, produce rational behavior from a smallish program rather than from a vast table. We have many examples showing that this can be done successfully in other areas: for example, the huge tables of square roots used by engineers and schoolchildren prior to the 1970s have now been replaced by a ﬁve-line program for Newton’s method running on electronic calculators. The question is, can AI do for general intelligent behavior what Newton did for square roots? We believe the answer is yes.The key challenge for AI is to ﬁnd out how to write programs that, to the extent possible, produce rational behavior from a smallish program rather than from a vast table. We have many examples showing that this can be done successfully in other areas: for example, the huge tables of square roots used by engineers and schoolchildren prior to the 1970s have now been replaced by a ﬁve-line program for Newton’s method running on electronic calculators. The question is, can AI do for general intelligent behavior what Newton did for square roots? We believe the answer is yes.
    - 
- 
