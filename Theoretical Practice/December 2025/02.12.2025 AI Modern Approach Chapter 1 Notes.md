# Goal

Want to finish notes on the first chapter of AIMA as part of establishing AI foundations.

(Can imagine spending 49th week finishing first 2 chapters of AIMA for AI foundations. It seems deeper than some other resources like Hands on ML, or online courses.)

I can see scenario where I would study semi-deeply (based on HML) basic ML algorithms, but when comes to neural networks, deep learning and RL would go deeper through AI Modern Approach and Deep Learning textbook, papers.

In getting a general grasp of AI as a field and frameworks around it I want to go very deeply.

Also listening to the history of the field deeper through several books: genius makers, The Quest for AI: History of Ideas and Achievements, Architects of Intelligence, Supremacy, AI: A Guide for Thinking Humans.
And experimenting with uploading the chapter of AI Modern Approach to notebook lm and making audio overview when on the go, or using chat for questions.

--

Finished AI modern approachh first chapter, started second chapter.


# Notes

(I just copy past them from remnote. I know pictures aren't visible on Github.)

- History Of Artificial Intelligence
    - Inception of AI (1943-1956) 
    - Early Enthusiasm, Great Expectations (1952-1969) 
    - A Dose of Reality (1966-1973) 
    - Expert Systems (1969-1986) 
    - Return of Neural Networks (1986-present) 
    - Probabilistic Reasoning and Machine Learning (1987-present) 
        - Neats vs Scruffies and maturity of the field:
            - Some have characterized this change as a victory of the neats—those who think that AI theories should be grounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some programs, and then assess what seems to be working. Both approaches are important. A shift toward neatness implies that the ﬁeld has reached a level of stability and maturity. The present emphasis on deep learning may represent a resurgence of the scrufﬁes.Some have characterized this change as a victory of the neats—those who think that AI theories should be grounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some programs, and then assess what seems to be working. Both approaches are important. A shift toward neatness implies that the ﬁeld has reached a level of stability and maturity. The present emphasis on deep learning may represent a resurgence of the scrufﬁes.
            - 
        - Hidden Markov Models:
            - The ﬁeld of speech recognition illustrates the pattern. In the 1970s, a wide variety of different architectures and approaches were tried. Many of these were rather ad hoc and fragile, and worked on only a few carefully selected examples. In the 1980s, approaches using hidden Markov models (HMMs) came to dominate the area. Two aspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This allowed speech researchers to build on several decades of mathematical results developed in other ﬁelds. Second, they are generated by a process of training on a large corpus of real speech data. This ensures that the performance is robust, and in rigorous blind tests HMMs improved their scores steadily. As a result, speech technology and the related ﬁeld of handwritten character recognition made the transition to widespread industrial and consumer applications. Note that there was no scientiﬁc claim that humans use HMMs to recognize speech; rather, HMMs provided a mathematical framework for understanding and solving the problem. We will see in Section 1.3.8, however, that deep learning has rather upset this comfortable narrative.The ﬁeld of speech recognition illustrates the pattern. In the 1970s, a wide variety of different architectures and approaches were tried. Many of these were rather ad hoc and fragile, and worked on only a few carefully selected examples. In the 1980s, approaches using hidden Markov models (HMMs) came to dominate the area. Two aspects of HMMs are relevant. Hidden Markov models First, they are based on a rigorous mathematical theory. This allowed speech researchers to build on several decades of mathematical results developed in other ﬁelds. Second, they are generated by a process of training on a large corpus of real speech data. This ensures that the performance is robust, and in rigorous blind tests HMMs improved their scores steadily. As a result, speech technology and the related ﬁeld of handwritten character recognition made the transition to widespread industrial and consumer applications. Note that there was no scientiﬁc claim that humans use HMMs to recognize speech; rather, HMMs provided a mathematical framework for understanding and solving the problem. We will see in Section 1.3.8, however, that deep learning has rather upset this comfortable narrative.
            - 
        - Bayesian Network:
            - 1988 was an important year for the connection between AI and other ﬁelds, including statistics, operations research, decision theory, and control theory. Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new acceptance of probability and decision theory in AI. Pearl’s development of Bayesian networks yielded a rigorous and efﬁcient formalism for representing uncertain knowledge as well as practical algorithms for probabilistic reasoning. Chapters 12, 13, 14, 15, and 18 cover this area, in addition to more recent developments that have greatly increased the expressive power of probabilistic formalisms; Chapter 21 describes methods for learning Bayesian networks and related models from data.1988 was an important year for the connection between AI and other ﬁelds, including statistics, operations research, decision theory, and control theory. Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new acceptance of probability and decision theory in AI. Pearl’s development of Bayesian networks yielded a rigorous and efﬁcient Bayesian network formalism for representing uncertain knowledge as well as practical algorithms for probabilistic reasoning. Chapters 12, 13, 14, 15, and 18 cover this area, in addition to more recent developments that have greatly increased the expressive power of probabilistic formalisms; Chapter 21 describes methods for learning Bayesian networks and related models from data.
            - 
        - Richard Sutton, Reinforcement Learning and Markov Decision Process:
            - A second major contribution in 1988 was Rich Sutton’s work connecting reinforcement learning—which had been used in Arthur Samuel’s checker-playing program in the 1950s— to the theory of Markov decision processes (MDPs) developed in the ﬁeld of operations research. A ﬂood of work followed connecting AI planning research to MDPs, and the ﬁeld of reinforcement learning found applications in robotics and process control as well as acquiring deep theoretical foundations.A second major contribution in 1988 was Rich Sutton’s work connecting reinforcement learning—which had been used in Arthur Samuel’s checker-playing program in the 1950s— to the theory of Markov decision processes (MDPs) developed in the ﬁeld of operations research. A ﬂood of work followed connecting AI planning research to MDPs, and the ﬁeld of reinforcement learning found applications in robotics and process control as well as acquiring deep theoretical foundations.
            - 
        - Machine Learning reunifying sub-fields of AI:
            - One consequence of AI’s newfound appreciation for data, statistical modeling, optimization, and machine learning was the gradual reuniﬁcation of subﬁelds such as computer vision, robotics, speech recognition, multiagent systems, and natural language processing that had become somewhat separate from core AI. The process of reintegration has yielded signiﬁcant beneﬁts both in terms of applications—for example, the deployment of practical robots expanded greatly during this period—and in a better theoretical understanding of the core problems of AI.become somewhat separate from core AI. The process of reintegration has yielded signiﬁcant beneﬁts both in terms of applications—for example, the deployment of practical robots expanded greatly during this period—and in a better theoretical understanding of the core problems of AI.One consequence of AI’s newfound appreciation for data, statistical modeling, optimization, and machine learning was the gradual reuniﬁcation of subﬁelds such as computer vision, robotics, speech recognition, multiagent systems, and natural language processing that had
            - 
        - 
    - Big Data (2001-present) 
        - Due to the rise of Deep Learning, the three main variables of Artificial Intelligence field are: AI Algorithms, AI Data and AI Compute. You can achieve better results by leveraging advances in one or the other. For example after 2001 there was much more AI Data available then before, which inferior AI Algorithms for little data all of a sudden produce viable results when given a lot of data.
        - Remarkable advances in computing power and the creation of the World Wide Web have facilitated the creation of very large data sets—a phenomenon sometimes known as big data.These data sets include trillions of words of text, billions of images, and billions of hours of speech and video, as well as vast amounts of genomic data, vehicle tracking data, clickstream data, social network data, and so on. 

This has led to the development of learning algorithms specially designed to take advantage of very large data sets. Often, the vast majority of examples in such data sets are unlabeled; for example, in Yarowsky’s (1995) inﬂuential work on word-sense disambiguation, occurrences of a word such as “plant” are not labeled in the data set to indicate whether they refer to ﬂora or factory. With large enough data sets, however, suitable learning algorithms can achieve an accuracy of over 96% on the task of identifying which sense was intended in a sentence. Moreover, Banko and Brill (2001) argued that the improvement in performance obtained from increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm. 

A similar phenomenon seems to occur in computer vision tasks such as ﬁlling in holes in photographs—holes caused either by damage or by the removal of ex-friends. Hays and Efros (2007) developed a clever method for doing this by blending in pixels from similar images; they found that the technique worked poorly with a database of only thousands of images but crossed a threshold of quality with millions of images. Soon after, the availability of tens of millions of images in the ImageNet database (Deng et al., 2009) sparked a revolution in the ﬁeld of computer vision. 

The availability of big data and the shift towards machine learning helped AI recover commercial attractiveness (Havenstein, 2005; Halevy et al., 2009). Big data was a crucial factor in the 2011 victory of IBM’s Watson system over human champions in the Jeopardy! quiz game, an event that had a major impact on the public’s perception of AI.Remarkable advances in computing power and the creation of the World Wide Web have facilitated the creation of very large data sets—a phenomenon sometimes known as big data.Big data These data sets include trillions of words of text, billions of images, and billions of hours of speech and video, as well as vast amounts of genomic data, vehicle tracking data, clickstream data, social network data, and so on. This has led to the development of learning algorithms specially designed to take advantage of very large data sets. Often, the vast majority of examples in such data sets are unlabeled; for example, in Yarowsky’s (1995) inﬂuential work on word-sense disambiguation, occurrences of a word such as “plant” are not labeled in the data set to indicate whether they refer to ﬂora or factory. With large enough data sets, however, suitable learning algorithms can achieve an accuracy of over 96% on the task of identifying which sense was intended in a sentence. Moreover, Banko and Brill (2001) argued that the improvement in performance obtained from increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm. A similar phenomenon seems to occur in computer vision tasks such as ﬁlling in holes in photographs—holes caused either by damage or by the removal of ex-friends. Hays and Efros (2007) developed a clever method for doing this by blending in pixels from similar images; they found that the technique worked poorly with a database of only thousands of images but crossed a threshold of quality with millions of images. Soon after, the availability of tens of millions of images in the ImageNet database (Deng et al., 2009) sparked a revolution in the ﬁeld of computer vision. The availability of big data and the shift towards machine learning helped AI recover commercial attractiveness (Havenstein, 2005; Halevy et al., 2009). Big data was a crucial factor in the 2011 victory of IBM’s Watson system over human champions in the Jeopardy! quiz game, an event that had a major impact on the public’s perception of AI.
        - 
    - Deep Learning (2011-present) 
        - The term deep learning refers to machine learning using multiple layers of simple, adjustable computing elements. Experiments were carried out with such networks as far back as the 1970s, and in the form of convolutional neural networks they found some success in handwritten digit recognition in the 1990s (LeCun et al., 1995). It was not until 2011, however, that deep learning methods really took off. This occurred ﬁrst in speech recognition and then in visual object recognition. 

In the 2012 ImageNet competition, which required classifying images into one of a thousand categories (armadillo, bookshelf, corkscrew, etc.), a deep learning system created in Geoffrey Hinton’s group at the University of Toronto (Krizhevsky et al., 2013) demonstrated a dramatic improvement over previous systems, which were based largely on handcrafted features. Since then, deep learning systems have exceeded human performance on some vision tasks (and lag behind in some other tasks). Similar gains have been reported in speech recognition, machine translation, medical diagnosis, and game playing. The use of a deep network to represent the evaluation function contributed to ALPHAGO’s victories over the leading human Go players (Silver et al., 2016, 2017, 2018). 

These remarkable successes have led to a resurgence of interest in AI among students, companies, investors, governments, the media, and the general public. It seems that every week there is news of a new AI application approaching or exceeding human performance, often accompanied by speculation of either accelerated success or a new AI winter. 

Deep learning relies heavily on powerful hardware. Whereas a standard computer CPU can do 10^9 or 10^10 operations per second. a deep learning algorithm running on specialized hardware (e.g., GPU, TPU, or FPGA) might consume between 10^14 and 10^17 operations per second, mostly in the form of highly parallelized matrix and vector operations. Of course, deep learning also depends on the availability of large amounts of training data, and on a few algorithmic tricks (see Chapter 22).The term deep learning refers to machine learning using multiple layers of simple, adjustableDeep learning computing elements. Experiments were carried out with such networks as far back as the 1970s, and in the form of convolutional neural networks they found some success in handwritten digit recognition in the 1990s (LeCun et al., 1995). It was not until 2011, however, that deep learning methods really took off. This occurred ﬁrst in speech recognition and then in visual object recognition. In the 2012 ImageNet competition, which required classifying images into one of a thousand categories (armadillo, bookshelf, corkscrew, etc.), a deep learning system created in Geoffrey Hinton’s group at the University of Toronto (Krizhevsky et al., 2013) demonstrated a dramatic improvement over previous systems, which were based largely on handcrafted features. Since then, deep learning systems have exceeded human performance on some vision tasks (and lag behind in some other tasks). Similar gains have been reported in speechrecognition, machine translation, medical diagnosis, and game playing. The use of a deep network to represent the evaluation function contributed to ALPHAGO’s victories over the leading human Go players (Silver et al., 2016, 2017, 2018). These remarkable successes have led to a resurgence of interest in AI among students, companies, investors, governments, the media, and the general public. It seems that every week there is news of a new AI application approaching or exceeding human performance, often accompanied by speculation of either accelerated success or a new AI winter. Deep learning relies heavily on powerful hardware. Whereas a standard computer CPU can do 109 or 1010 operations per second. a deep learning algorithm running on specialized hardware (e.g., GPU, TPU, or FPGA) might consume between 1014 and 1017 operations per second, mostly in the form of highly parallelized matrix and vector operations. Of course, deep learning also depends on the availability of large amounts of training data, and on a few algorithmic tricks (see Chapter 22).
        - 
    - State of the Art for 2020
        - State of the field as of 2020
            - https://remnote-user-data.s3.amazonaws.com/sEONwDD7E6mKu6ocHmP06S1J6zewcxODkih-yXUq0AfW59Yi8C17YpaDm7yppfaXc1X78F3wKJnxuU5MOjdnGU410gnw_YTsAK7-IyPgUG6CUMxCtmW8fJSc-Aolx5Gd.pnghttps://remnote-user-data.s3.amazonaws.com/tjnO6UFX3JTf2C1yP-hBgXD7jCtIAZHUFQHM0RVYmM_5B_SuLfzCeUGGZ3r26rQZdnSc9u28P7ojkbAQTGG-WmPMuR9QlXQubTrD9onrXn3sE68-Nj8nficlCjI2sOLL.pnghttps://remnote-user-data.s3.amazonaws.com/tjnO6UFX3JTf2C1yP-hBgXD7jCtIAZHUFQHM0RVYmM_5B_SuLfzCeUGGZ3r26rQZdnSc9u28P7ojkbAQTGG-WmPMuR9QlXQubTrD9onrXn3sE68-Nj8nficlCjI2sOLL.pnghttps://remnote-user-data.s3.amazonaws.com/sEONwDD7E6mKu6ocHmP06S1J6zewcxODkih-yXUq0AfW59Yi8C17YpaDm7yppfaXc1X78F3wKJnxuU5MOjdnGU410gnw_YTsAK7-IyPgUG6CUMxCtmW8fJSc-Aolx5Gd.png
            - 
        - On what model we'll use in the future, or to achieve human-level (and better) level of performance on most economic tasks:
            - https://remnote-user-data.s3.amazonaws.com/R-RCrgX2TTAfb6TVgKz7xgZ4w5wdaHJ-vnY7XnrIG2njCTlxr9dpcO7hTCjtG7Z8uTCrkda8-eZ72W0oJ_7uxK8a2bSY5wEzoa2ILNej_2rTMTk7oDn5UcmG74TjYsWm.pnghttps://remnote-user-data.s3.amazonaws.com/R-RCrgX2TTAfb6TVgKz7xgZ4w5wdaHJ-vnY7XnrIG2njCTlxr9dpcO7hTCjtG7Z8uTCrkda8-eZ72W0oJ_7uxK8a2bSY5wEzoa2ILNej_2rTMTk7oDn5UcmG74TjYsWm.png
            - 
        - What AI could do as of 2020
            - https://remnote-user-data.s3.amazonaws.com/z6BGSHUPidDNI6hG7KzxlZBZ7qOuNGnVSJOMFak5i4jWvlU_zy02Pu6lcAdIkuLfnCEZtst1-qv9Y6Fxnr6RWQWNaQuHY9JfN9PsRawP7TL24_V7FC1EOEvlJDEXXg4b.pnghttps://remnote-user-data.s3.amazonaws.com/9Uc2sjPLLtXgqQoh3Ks-guRnIKZIaZlBaTKq0CpWOpmMOd-vawXSIYv7RE_M5gnxMoWXNcgEGe1Q_1bDAy_NmAY1vwWO6ZFqU1Gmh6Z0gryYNGjk5tMVlLtZh5EfJHZn.pnghttps://remnote-user-data.s3.amazonaws.com/rmFGyO90OqRHHCQdEXnXW-fSNhI8dliuhhU5VR4UpDURWS1Jnz_CQL4kct_3RpscVIckfKYiPicqL_dSPJs4R0niiSvBQI9gG7yCFEHP_g-c-2L9k-pSvZJJA0bOHRmL.pnghttps://remnote-user-data.s3.amazonaws.com/rmFGyO90OqRHHCQdEXnXW-fSNhI8dliuhhU5VR4UpDURWS1Jnz_CQL4kct_3RpscVIckfKYiPicqL_dSPJs4R0niiSvBQI9gG7yCFEHP_g-c-2L9k-pSvZJJA0bOHRmL.pnghttps://remnote-user-data.s3.amazonaws.com/9Uc2sjPLLtXgqQoh3Ks-guRnIKZIaZlBaTKq0CpWOpmMOd-vawXSIYv7RE_M5gnxMoWXNcgEGe1Q_1bDAy_NmAY1vwWO6ZFqU1Gmh6Z0gryYNGjk5tMVlLtZh5EfJHZn.pnghttps://remnote-user-data.s3.amazonaws.com/z6BGSHUPidDNI6hG7KzxlZBZ7qOuNGnVSJOMFak5i4jWvlU_zy02Pu6lcAdIkuLfnCEZtst1-qv9Y6Fxnr6RWQWNaQuHY9JfN9PsRawP7TL24_V7FC1EOEvlJDEXXg4b.png
            - 
        - 
    - 
    - One cannot miss the pattern in History Of Artificial Intelligence of: (1) some innovation happening, (2) it solving some genuine problems, (3) massive flux of investment based on real impact, (4) overpromise of this innovation solving "most problems under the sun", (5) disappointment in under-delivery of this innovation and massive investment outflux.
    - 
- 
- Risks and Benefits of AI 
    - https://remnote-user-data.s3.amazonaws.com/m4Dk3jN1IsoELpRK7fiGizIs-Db9XRbNO1De5Cj_ovcaVc-3Bsizy0pL5pn0UTSuhC0K_2ttemDXClewvQ-iUoX1_oz3Lrjs6wxjzyIj1QSiaz3or_-UtfX-CG5zGKPJ.pnghttps://remnote-user-data.s3.amazonaws.com/m4Dk3jN1IsoELpRK7fiGizIs-Db9XRbNO1De5Cj_ovcaVc-3Bsizy0pL5pn0UTSuhC0K_2ttemDXClewvQ-iUoX1_oz3Lrjs6wxjzyIj1QSiaz3or_-UtfX-CG5zGKPJ.png
    - ...
    - https://remnote-user-data.s3.amazonaws.com/XpZrtY6JQhVTWZhFme16gUcE85qOuUj-pcncwAGWghubVL8LgJrAPDPhzeNvc25LAcxiL8s8uLKahueyeKsyT2JRnZt0f505jLMWK_fzA9AAzW4mZzbexIJloYhCvl1D.pnghttps://remnote-user-data.s3.amazonaws.com/_64p_wx5-3BJ40w-bLwoVyrjNgUmViGZnStvigJiKRMQmjmcF5tPdWNgiqLCCrFeQ8zsfmdzz5Zltw7HWB1wI08YAgv7RDNn7j3ncdjnYaRC9vZpZp6iKWmuWXEBgX52.pnghttps://remnote-user-data.s3.amazonaws.com/FsPUt3FRjBZM8MpyXPANSHyO852d3MOOwNz_gWS8X0JqwNODl7yZqDByz00sfWPv6fsjV38yYcRIJSNyVBv6Zk6XrGw1N25M9jszpbOutqA1mQieHSlIDUsf-A3O_6jr.pnghttps://remnote-user-data.s3.amazonaws.com/FsPUt3FRjBZM8MpyXPANSHyO852d3MOOwNz_gWS8X0JqwNODl7yZqDByz00sfWPv6fsjV38yYcRIJSNyVBv6Zk6XrGw1N25M9jszpbOutqA1mQieHSlIDUsf-A3O_6jr.pnghttps://remnote-user-data.s3.amazonaws.com/_64p_wx5-3BJ40w-bLwoVyrjNgUmViGZnStvigJiKRMQmjmcF5tPdWNgiqLCCrFeQ8zsfmdzz5Zltw7HWB1wI08YAgv7RDNn7j3ncdjnYaRC9vZpZp6iKWmuWXEBgX52.pnghttps://remnote-user-data.s3.amazonaws.com/XpZrtY6JQhVTWZhFme16gUcE85qOuUj-pcncwAGWghubVL8LgJrAPDPhzeNvc25LAcxiL8s8uLKahueyeKsyT2JRnZt0f505jLMWK_fzA9AAzW4mZzbexIJloYhCvl1D.png
    - 
    - 
- Summary of the Artificial Intelligence: a Modern Approach first chapter: 
    - https://remnote-user-data.s3.amazonaws.com/SnC1aK0nHSZGO6anV1H8JFQJVbDUF1pTRLf74ir64jx-Y5ANU468fl0RThrtseGBbSLMvhn8Wq2iCAagEy3wu-YYhvOCNRJPEgfKn-puJ7d1hQGYc0UWWX98w8bD_Xs6.pnghttps://remnote-user-data.s3.amazonaws.com/zlMAZ6Q2H27LV0bDL39Z61xStkmAmO1QhlhTVHmsIfeBJ5VpmkC6zu5vu09RoFoqpfFEb2hkGayNYL2LG4PtRb0R4EwFeY258jYYxU1-tRiGM89iBCghj6QVqZ6kUKFk.pnghttps://remnote-user-data.s3.amazonaws.com/zlMAZ6Q2H27LV0bDL39Z61xStkmAmO1QhlhTVHmsIfeBJ5VpmkC6zu5vu09RoFoqpfFEb2hkGayNYL2LG4PtRb0R4EwFeY258jYYxU1-tRiGM89iBCghj6QVqZ6kUKFk.pnghttps://remnote-user-data.s3.amazonaws.com/SnC1aK0nHSZGO6anV1H8JFQJVbDUF1pTRLf74ir64jx-Y5ANU468fl0RThrtseGBbSLMvhn8Wq2iCAagEy3wu-YYhvOCNRJPEgfKn-puJ7d1hQGYc0UWWX98w8bD_Xs6.png
    - 
    

