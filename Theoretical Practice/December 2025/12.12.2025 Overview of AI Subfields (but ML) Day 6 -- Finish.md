# Goal

Finished first two chapters of AI modern approach, they give good mental models for understanding the whole field. 

While my focus is on ML, DL, RL, I think it make sense to have __some clue__ (very exact word choice) about other subfields of AI. 

Think: search in simple and complex environments, adversarial search, constraint satisfaction problems, logical agents, knowledge representation, automated planning, decision making under uncerainty in simple and complex enviroments (bayesian networks, markov models), robotics, computer vision.

__Some clue__ is exactly the level of understanding I want to have. I'm not interested in deep understanding of those subfields because my goal is to focus on ML,DL,RL. But it also seems wrong to have absolutely no idea what those subfields are about.

So it seems to me that a good balance is to spend next 3-5 days understanding their main ideas, but not go into details. The end-goal is to be able to explain those subfields' main ideas simply and how they fit-in with all of the AI field.

How would I do that? AI modern approach seems like the right source. Read most chapters intros and summaries. Make notes on them, use some llms and other online sources. Maybe also listen to notebook lm audio versions of chapters.

Once I'm done with that, I should have both solid mental models for the whole AI field, and general understanding of most subfiels. That should be enough to confidently jump deeply into ML,DL,RL, which would be the next step in syllabus.

(And I'm continuing with reading/listening to non-fiction books (anything that is not a textbook) about the history of the field and anything related when eating, driving, working out. [Getting 25 books under my belt❤️](https://nymag.com/intelligencer/2020/01/kushner-can-make-mid-east-peace-because-hes-read-25-books.html) (in no particular order):
1. The Alignment Problem
2. Genesis
3. Life 3.0
4. The Scaling Era
5. The Big Score
6. The Master Algorithm
7. Genius Makers
8. Supremacy: AI, ChatGPT, and the race that will change the world
9. AI: A Guide for Thinking Humans
10. The Quest for AI: History of Ideas and Achievements
11. Architects of Intelligence
12. Human Compatible
13. AI Superpowers: China, Silicon Valley and the New World 
14. If anyone builds it, everyone dies
15. The Worlds I see
16. AI 2041
17. Atlas of AI
18. A Brief History of Intelligence
19. Nexus
20. ✅Empire of AI
21. ✅Sam Altman, The Optimist
22. ✅Superintelligence ‒ Nick Bostrom
)

# Notes

After 5-6 days of getting acquainted with other subfields of AI, I won't pretend my understanding is deep. It is certified shallow. But I think I understand how subfields connect with each other and why each one exists. I have the hanger on which I can put each subfield. And if I want to go deeper into any particular one, I understand how it connects to the big picture -- I can study it without loosing the forest.

So, now I'll give you my feynman explanation of it all, and I'll keep it up through flashcards. Adding to it when learning more about the subfields.

--

So it all starts with computers which is a tool for transforming information, from inputs to outputs via our instructions.

Writing those instructions is software engineering and this is the part AI is concerned with. The difference of AI with traditional software engineering is in problems it addresses and techniques it uses, but deep down it is all the same instructing computers to do useful transformations.

So, we are interested in solving some real life problem using computers, specifically AI. How would we do it?

We need to give a simplified and valid representation of our real-life problem to computer. 

This is because real life is too hard to navigate for our current computers, we don't have both the right software and debatably right hardware to do it.

So we create simpler versions of our problems, but make sure that they are still valid and we can use the answer arrived with computer for our real-life cases.

Consider navigating US. For example you want to get from SF to NY. The simplest representation you could use is a graph of nodes of major cities with distance between them. (ATM let's not get into the details of exact road navigation.)

Then you write a program that finds the shortest path between the SF and NY.

The representation of real life problem is called an environment, and the program that navigates it is called an agent. 

The type of real life problem we have, determines the type of environments we have, and thus the type of agents (programs) we use to successfully navigate it.

It all starts with real life problem and then cascades downwards. AI subfields are mainly talking about different agent programs one can use, but it is always useful to connect them to environments they operate in. The need for agent variety comes from the environment variety.

--

First it makes sense to talk about what environment types there are, how can they be represented, and what are agent types. Through these types we could see all the rest of the subfields.

Types of environments:
1. Fully observable vs partially obvervable
2. Deterministic vs non-determintistic vs stochastic
3. single-agent vs multi-agent (cooperative/ competitive)
4. episodic vs sequential 
5. static vs dynamic
6. discrete vs continuous
7. known vs unknown

Well that's a lot of environment types.

We also can represent those types differently. The three main (broadly) types are:
1. Atomic -- states are black boxes.
2. Factored -- states have factors (i.e. variables) with values.
3. Structured -- state can have inside it multiple states with factors and relationships between each other (think little recursion)

Now this is the environment, what about agents (programs)?

There are four types of agents:
1. Simple reflex based agent -- just acts on the last percept without keeping any internal states, super simple
2. Model-based reflex agent -- has internal representations of the environment through which it reasons to then decide what to do
3. Goal based agent -- binary goal states
4. Utility based agent -- goal states can be ranked.

You can also differentiate programs (agents) based on offline vs online acting.

In AIMA units they broadly talk about problem-solving agents (i.e. search-based agents), knowledge based agents (that have internal representations about environment and reason through them, they talk mainly about hand-crafted expert systems), and utility-based agents (that can handle uncertainty through probability.)

And also you have learning agents that learn/ improve of how to do their task => ML.

--

ML is different from most of other subfields because it is less explicit, and more implicit -- we recognize some transformations are really hard to specify for computers, so we create algorithms that would hopefully learn them on their own. (But also ML, DL is not the right approach for all environments, sometimes it is like bringing a tank to a knife fight -- very resource inefficient!)

Those other more explicit, hand-crafted subfields are: Search, Logic/Knowledge-based agents, and uncertain knowledge & reasoning (hand-crafted agents that can deal with uncertainty).

So, let's talk about each subfield.

Search is the simplest of all, mainly because it navigates the simplest of all environments.

Just comeback to our example with getting from SF to NY. We have a graph with the main cities and we're trying to find the fastest path. This is a "simple" environment as it is fully observable, episodic, deterministic, single agent, discrete and known. There are two ways to go about of finding the solution. Either uninformed search, or informed. Uninformed has no information about how well it is doing, how far from the goal state it is, it just search for the path and goal state node one by one, say depth-first search, or breadth first search. But, we can also add a heuristic function, which would give information, like how far from goal state node a node is, and then our search can be done faster, this is called informed search. It really depends on finding a good heuristic function.

Then you have search in complex environments, which means our environment is not as "vanilla" as it was before. For example it can be non-deterministic, partially observable, unknown or have continuous states. Also in this type of search we usually don't care about the path to the goal state (like with SF to NY), but we just care about what the goal state is. Think like finding the appropriate weights for a neural network, or right placement of hospitals, or portfolio management distribution. This type of search is called state space search, and local search is somehow closely related to it, but I'm not sure exactly how, and there's state-space landscape which is about values we search through on the x-axis, and how well they perform on the objective function on the y-axis, and you search that landscape to find the optimal values. You can do that through simulated annealing, hill climbing, and a few other algorithms. There's also online search where agent has to both act and search for solution, say vacuum cleaner robot understanding what the room map is. While the opposite of that is offline search where program first finds full solution, and only then does any action, think like finding optimal path between SF and NY, and then pursuing it.

Then we have constraint satisfaction problem, where states are not binary yes/ no goal state, but have variables and values (environment is represented through factored approach), and program needs to find states that satisfy constraints, like goal state must have variable X above 10, variable Y must be 1 (binary) etc. Optimization problem then is about finding not just goal state that is good enough on all those constraints, but that is the best of all good enough solutions, so maximizing. You also can have heuristic functions when doing this search. I'm not super solid in constraint satisfaction problem, but I'll study it more in the long-run.

Now we have adversarial search. Which is basically about environments that are multiagent and competitive, think chess, poker, go. Depending on whether it is a fully observable or not environment we'll of course use different programs. I haven't studied adversarial search more than that, I know about minimax, alpha-beta pruning, but not more. 

I think for ML, DL, RL search in complex environments is the most improtant one, and I'll study it deeply as the need arises. In general I think understanding search unit is important. It's pretty fundamental and is used in ML,DL,RL, so I plan to be well-versed in it.

--

Alright, let's talk logic-knowledge based agents.

The issue is that search-based agents are pretty narrow. We can have much more general agents that can solve more complex environments if we give them internal representation of their environment, so they can reason about it before acting. 

The question then is how to do that? How to give internal representations to agents?

First, we need some way to give representations, and then some way to reason about them. There are quite a few ways, but AIMA book mainly focuses on logic.

Logic is a formal language (which means more strict and explicit) for defining statements and reasoning about them. Logic is a great candidate for exactly the job we need to do of giving agents some way to _define and reason_ about representations. There are different types of logic, the simplest is propositional logic, and the more expressive one is first-order logic which not only asserts existance of objects, but also how they connect to each other.

Some agreed upon way to define and reason about representations is called knowledge representation language. Besides these logic types, AI scientists are trying to create a formal language that would be universal enough so that one agent can be created that reason about myriad of topics at once.

So those agents we create are called knowledge based agents. They have a knowldege base which is a set of sentences. Sentences are formal assertions about the environment expressed in knowledge representation language. Programmer gives a bunch of such sentences which are considered axioms, and then programs reasons (inference) about them to deduce some new interesting assertions.

AIMA has in-depth chapters of how to do inference for different knowledge representation languages, what are the more efficient algorithms, what knowledge representation languages there are, how to construct one that fits the problem, how to construct appropriate knowledge base for the agent so it is useful etc. It also talks about solving planning problems with those types of agents, using special knowledge representation language. But all of that I quickly glanced over and didn't go into any detail, and until demand for learning that arises, I don't mind whatsover this level of understanding.

--

Alright, lastly let's talk uncertaitn knowledge and reasoning agents, or basically hand-crafted agents that use probabilities and utility theory to navigate environments (model based agents + utility based ones).

Basically our previous approach to non-deterministic environments (where agent isn't always certain of the state it is in, or will be when doing actions) is to keep track of belief state (where agent possibly is), and then all other state he could be in, regardless of how unlikely, and what would be the plan to navigate them if he's there.

So we care about what are all _possible_ states, regardless of how _probable_ each state is. Obviously that's not that efficient. Moreover, all our agents have hard time navigating uncertainty, they actually can't handle it whatsoever.

Obviously a shit load of environments have uncertainty, and it would be very useful to be able to navigate them. So, we introduce probability to our agents. Now they could not only reason about all possible environments, but consider how probable they are.

To top it off, we also introduce utility theory. Basically states now are not binary "yes/no" goal state, but states have utility -- some better than others. You might say this is analogous goals states. 

Putting those together we get decision theory!

Decision theory = probability theory + utility theory

Now our agents can navigate environment based on how likely states are, and how much utility they have. This way they are maximizing the expected utility.

This is the important part I understand about "probability and utility based agents". We still heavily hand craft those programs, like bayesian networks or markov models, and which one we choose still heavily relies on the environment we're navigating. But, I haven't studied in any reasonable depth any of those algorithms, and for now that's okay. I know they are there, I know why they are there, and atm that's enough. Probably in the long-run I'll learn them better. 

--

This is super broadly, super quickly the main ideas I have learned making an overview of AIMA in a few days. Now I'll go more seriously into ML,DL,RL and maybe comeback to those subfields when needed to get more understanding.
