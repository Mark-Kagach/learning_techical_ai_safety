# Goal

Finished first two chapters of AI modern approach, they give good mental models for understanding the whole field. 

While my focus is on ML, DL, RL, I think it make sense to have __some clue__ (very exact word choice) about other subfields of AI. 

Think: search in simple and complex environments, adversarial search, constraint satisfaction problems, logical agents, knowledge representation, automated planning, decision making under uncerainty in simple and complex enviroments (bayesian networks, markov models), robotics, computer vision.

__Some clue__ is exactly the level of understanding I want to have. I'm not interested in deep understanding of those subfields because my goal is to focus on ML,DL,RL. But it also seems wrong to have absolutely no idea what those subfields are about.

So it seems to me that a good balance is to spend next 3-5 days understanding their main ideas, but not go into details. The end-goal is to be able to explain those subfields' main ideas simply and how they fit-in with all of the AI field.

How would I do that? AI modern approach seems like the right source. Read most chapters intros and summaries. Make notes on them, use some llms and other online sources. Maybe also listen to notebook lm audio versions of chapters.

Once I'm done with that, I should have both solid mental models for the whole AI field, and general understanding of most subfiels. That should be enough to confidently jump deeply into ML,DL,RL, which would be the next step in syllabus.

(And I'm continuing with reading/listening to non-fiction books (anything that is not a textbook) about the history of the field and anything related when eating, driving, working out. [Getting 25 books under my belt❤️](https://nymag.com/intelligencer/2020/01/kushner-can-make-mid-east-peace-because-hes-read-25-books.html) (in no particular order):
1. The Alignment Problem
2. Genesis
3. Life 3.0
4. The Scaling Era
5. The Big Score
6. The Master Algorithm
7. Genius Makers
8. Supremacy: AI, ChatGPT, and the race that will change the world
9. AI: A Guide for Thinking Humans
10. The Quest for AI: History of Ideas and Achievements
11. Architects of Intelligence
12. Human Compatible
13. AI Superpowers: China, Silicon Valley and the New World 
14. If anyone builds it, everyone dies
15. The Worlds I see
16. AI 2041
17. Atlas of AI
18. A Brief History of Intelligence
19. Nexus
20. ✅Empire of AI
21. ✅Sam Altman, The Optimist
22. ✅Superintelligence ‒ Nick Bostrom
)

# Notes

Today I have finished chapter 3, 4, 5, 6 (with part of 7). In my AI Algorithms folder, these are all the notes under Search folder:

- Fundamentals
    - Everything regarding the definition of search. 
        - Agent is an entity that perceives its environment and acts upon that environment. State ‒ a configuration of the agent and its environment. Initial State ‒ the state in which the agent begins. Actions ‒ choices that can be made in a state. 
        - Transition Model
        - State Space
        - Goal State
        - Path Cost
        - Optimal Solution
        - Node
        - Frontier
        - 
        - https://remnote-user-data.s3.amazonaws.com/TY-qeEu8Ya2VgsFQOLd7KJXh7qzLYKSJ4oXQtPbTE01W4bZdL_QASktYwZRlGUm4TYifv1n9accuFg48fWkucsWgnq3LhL8bGXUVdT5ZqOS8oQ1h7TGvC1L7F2ge6Gog.pnghttps://remnote-user-data.s3.amazonaws.com/TY-qeEu8Ya2VgsFQOLd7KJXh7qzLYKSJ4oXQtPbTE01W4bZdL_QASktYwZRlGUm4TYifv1n9accuFg48fWkucsWgnq3LhL8bGXUVdT5ZqOS8oQ1h7TGvC1L7F2ge6Gog.png
        - 
    - Problem-Solving Agents use Atomic Representation Approach, while Agents that use Factored Representation Approach and Structured Representation Approach are called Planning Agents.
    - The four step process Problem-Solving Agent (that operates in Fully Observable Environment) follows:
    - This chapter has introduced search algorithms that an agent can use to select action sequences in a wide variety of environments—as long as they are episodic, single-agent, fully observable, deterministic, static, discrete, and completely known. There are tradeoffs to be made between the amount of time the search takes, the amount of memory available, and the quality of the solution. We can be more efﬁcient if we have domain-dependent knowledge in the form of a Heuristic Function that estimates how far a given state is from the goal, or if we precompute partial solutions involving patterns or landmarks.form of a heuristic function that estimates how far a given state is from the goal, or if we precompute partial solutions involving patterns or landmarks.This chapter has introduced search algorithms that an agent can use to select action sequences in a wide variety of environments—as long as they are episodic, single-agent, fully observable, deterministic, static, discrete, and completely known. There are tradeoffs to be made between the amount of time the search takes, the amount of memory available, and the quality of the solution. We can be more efﬁcient if we have domain-dependent knowledge in the
    - Search algorithms are judged on the basis of Search Completeness, Search Cost Optimality, Time Complexity, and Space Complexity.Search algorithms are judged on the basis of completeness, cost optimality, time complexity, and space complexity.
    - 
    - Archive 
    - 
- 
- Search in Simple Environments
    - We distinguish between informed algorithms, in which the agent can estimate how far it is from the goal, and uninformed algorithms, where no such estimate is available.We distinguish between informed algorithms, in which the agent can estimate how far it is from the goal, and uninformed algorithms, where no such estimate is available. (Path Finding Search)
    - Uninformed Search
        - A search algorithm that doesn't use problem-specific knowledge to guide the search. Algorithms differ based on which node they expand first.
        - Best-ﬁrst Search selects nodes for expansion using an evaluation function. 
        - Breadth-first Search expands the shallowest nodes ﬁrst; it is complete, optimal for unit action costs, but has exponential space complexity. 
        - Uniform-cost Search expands the node with lowest path cost, g(n), and is optimal for general action costs. 
        - Depth-ﬁrst Search expands the deepest unexpanded node ﬁrst. It is neither complete nor optimal, but has linear space complexity. Depth-limited search adds a depth bound. 
        - Iterative Deepening Search calls depth-ﬁrst search with increasing depth limits until a goal is found. It is complete when full cycle checking is done, optimal for unit action costs, has time complexity comparable to breadth-ﬁrst search, and has linear space complexity. 
        - Bidirectional Search expands two frontiers, one around the initial state and one around the goal, stopping when the two frontiers meet.Best-ﬁrst search selects nodes for expansion using an evaluation function. – Breadth-ﬁrst search expands the shallowest nodes ﬁrst; it is complete, optimal for unit action costs, but has exponential space complexity. – Uniform-cost search expands the node with lowest path cost, g(n), and is optimal for general action costs. – Depth-ﬁrst search expands the deepest unexpanded node ﬁrst. It is neither complete nor optimal, but has linear space complexity. Depth-limited search adds a depth bound. – Iterative deepening search calls depth-ﬁrst search with increasing depth limits until a goal is found. It is complete when full cycle checking is done, optimal for unit action costs, has time complexity comparable to breadth-ﬁrst search, and has linear space complexity. – Bidirectional search expands two frontiers, one around the initial state and one around the goal, stopping when the two frontiers meet.
        - 
    - Informed Search
        - A search algorithm that uses problem-specific knowledge to guide the search process. Think coordinates of initial state and goal state in a 2-dimension grid maze.
        - 
        - Greedy Best-First Search expands nodes with minimal h(n). It is not optimal but is often efﬁcient. 
        - A* Search expands nodes with minimal f (n) = g(n) + h(n). A∗ is complete and optimal, provided that h(n) is admissible. The space complexity of A∗ is still an issue for many problems. 
        - Bidirectional A∗ Search is sometimes more efﬁcient than A∗ itself. 
        - IDA∗ (Iterative Deepening A∗ Search) is an iterative deepening version of A∗, and thus adresses the space complexity issue. 
        - RBFS (Recursive Best-ﬁrst Search) and SMA∗ (Simpliﬁed Memory-bounded A∗) are robust, optimal search algorithms that use limited amounts of memory; given enough time, they can solve problems for which A∗ runs out of memory.Greedy best-ﬁrst search expands nodes with minimal h(n). It is not optimal but is often efﬁcient. – A∗ search expands nodes with minimal f (n) = g(n) + h(n). A∗ is complete and optimal, provided that h(n) is admissible. The space complexity of A∗ is still an issue for many problems. – Bidirectional A∗ search is sometimes more efﬁcient than A∗ itself. – IDA∗ (iterative deepening A∗ search) is an iterative deepening version of A∗, and thus adresses the space complexity issue. – RBFS (recursive best-ﬁrst search) and SMA∗ (simpliﬁed memory-bounded A∗) are robust, optimal search algorithms that use limited amounts of memory; given enough time, they can solve problems for which A∗ runs out of memory.
        - Beam Search puts a limit on the size of the frontier; that makes it incomplete and suboptimal, but it often ﬁnds reasonably good solutions and runs faster than complete searches. 
        - Weighted A∗ Search focuses the search towards a goal, expanding fewer nodes, but sacriﬁcing optimality.Beam search puts a limit on the size of the frontier; that makes it incomplete and suboptimal, but it often ﬁnds reasonably good solutions and runs faster than complete searches. – Weighted A∗ search focuses the search towards a goal, expanding fewer nodes, but sacriﬁcing optimality.
        - 
        - The performance of heuristic search algorithms depends on the quality of the Heuristic Function. One can sometimes construct good heuristics by relaxing the problem deﬁnition, by storing precomputed solution costs for subproblems in a pattern database, by deﬁning landmarks, or by learning from experience with the problem class.The performance of heuristic search algorithms depends on the quality of the heuristic function. One can sometimes construct good heuristics by relaxing the problem deﬁnition, by storing precomputed solution costs for subproblems in a pattern database, by deﬁning landmarks, or by learning from experience with the problem class.
        - 
    - 
- Search in Complex Environments
    - Search in Complex Environments considers Environments that are Non-Deterministic, Partially Observable, Unknown, and have Continuous states (not just Discrete).
    - 
    - In many Search problems you don't care about the path to the Goal State, you just are interested in what the goal state is. 
    - For example how to optimally place three airports in Romania, what portfolio distribution to have, what weights for Neural Network to have (aka Gradient Descent)! (So you search through State Space to find the one that perform great on an Objective Function.)
    - I'll differentiate those Search algorithms as State Space Search, while those that care about the path as Path Finding Search algorithm.
    - 
    - State Space Search and State-Space Landscape
        - To understand Local Search, consider the states of a problem laid out in a state-space landscape, as shown in Figure 4.1. Each point (state) in the landscape has an “elevation,” defined by the value of the Objective Function. If elevation corresponds to an objective function, then the aim is to find the highest peak—a Global Maximum—and we call the process Hill Climbing. If elevation corresponds to cost, then the aim is to find the lowest valley—a Global Minimum—and we call it Gradient Descent . (Local Maximum, Shoulder)then the aim is to find the highest peak—a global maximum—and we call the process hill Global maximum climbing. If elevation corresponds to cost, then the aim is to find the lowest valley—a global minimum—and we call it gradient descent
        - https://remnote-user-data.s3.amazonaws.com/Wp_WNNn-MhdXu_x1xrBvcMj4B08sZdJd1nEJXCseKfahB8OyQIZxlY4B-Itr9buTAeoYXDfLxhVgfHhC9JKQtmIG_s6NOyhzAYzs1gemnOkeBUyvEZ9_lfdjMJZZt6rT.pnghttps://remnote-user-data.s3.amazonaws.com/Wp_WNNn-MhdXu_x1xrBvcMj4B08sZdJd1nEJXCseKfahB8OyQIZxlY4B-Itr9buTAeoYXDfLxhVgfHhC9JKQtmIG_s6NOyhzAYzs1gemnOkeBUyvEZ9_lfdjMJZZt6rT.png
        - 
        - Optimization Problem
        - Traveling Salesman Problem
        - 
    - 
    - Local Search
        - algorithms operate by searching from a start state to neighboring states, without keeping track of the paths, nor the set of states that have been reached. That means they are not systematic—they might never explore a portion of the search space where a solution actually resides. However, they have two key advantages: (1) they use very little memory; and (2) they can often find reasonable solutions in large or infinite state spaces for which systematic algorithms are unsuitable.Local search algorithms operate by searching from a start state to neighboring states,Local search without keeping track of the paths, nor the set of states that have been reached. That means they are not systematic—they might never explore a portion of the search space where a solution actually resides. However, they have two key advantages: (1) they use very little memory; and (2) they can often find reasonable solutions in large or infinite state spaces for which systematic algorithms are unsuitable.
        - 
        - Compared to Search in Simple Environmentss which are mostly interested in finding a path to the goal, local search has no path, it is interested in finding the end-goal itself.
        - For example, instead of a path in a maze, it might be interested in optimal hospital placement on a map.
        - A good example of neighbors is to right away consider their State-Space Landscape:
        - 
    - Hill Climbing
        - search algorithm is shown in Figure 4.2. It keeps track of one current state and on each iteration moves to the neighboring state with highest value—that is, it heads in the direction that provides the steepest ascent. It terminates when it reaches a “peak” where no neighbor has a higher value. Hill climbing does not look ahead beyond the immediate neighbors of the current state.The hill-climbing search algorithm is shown in Figure 4.2. It keeps track of one current state Hill climbing and on each iteration moves to the neighboring state with highest value—that is, it heads in the direction that provides the steepest ascent. It terminates when it reaches a “peak” where Steepest ascent no neighbor has a higher value. Hill climbing does not look ahead beyond the immediate neighbors of the current state.
        - (The simplest Local Search algorithm.)
        - https://remnote-user-data.s3.amazonaws.com/6j6XTYwjC1epQ8nvAUazJfoC77Aj-AKV-cFz1EQk5Ny4DGOcdp4UU3xrxEhU33-6n-GrQXYHOQFl1plHVHIXRt3o5qnS2JR98BeHzM6pXU8AwrFYmtIUk-hA7mzY-7hz.png
        - Example on finding hospitals optimal solution
        - 
        - Variations
        - Stochastic Hill Climbing
        - First-Choice Hill Climbing
        - Random-Restart Hill Climbing
        - Local Beam Search Hill Climbing
        - 
    - Simulated Annealing
        - You start with making more of random moves, and with time make less random moves. 
        - A hill-climbing algorithm that never makes “downhill” moves toward states with lower value (or higher cost) is always vulnerable to getting stuck in a local maximum. In contrast, a purely random walk that moves to a successor state without concern for the value will eventually stumble upon the global maximum, but will be extremely inefficient. Therefore, it seems reasonable to try to combine hill climbing with a random walk in a way that yields both efficiency and completeness. 

Simulated annealing is such an algorithm. In metallurgy, annealing is the process used to temper or harden metals and glass by heating them to a high temperature and then gradually cooling them, thus allowing the material to reach a low-energy crystalline state. To explain simulated annealing, we switch our point of view from hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a ping-pong ball into the deepest crevice in a very bumpy surface. If we just let the ball roll, it will come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the local minimum—perhaps into a deeper local minimum, where it will spend more time. The trick is to shake just hard enough to bounce the ball out of local minima but not hard enough to dislodge it from the global minimum. The simulated-annealing solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the intensity of the shaking (i.e., lower the temperature).A hill-climbing algorithm that never makes “downhill” moves toward states with lower value (or higher cost) is always vulnerable to getting stuck in a local maximum. In contrast, a purely random walk that moves to a successor state without concern for the value will eventually stumble upon the global maximum, but will be extremely inefficient. Therefore, it seems reasonable to try to combine hill climbing with a random walk in a way that yields both efficiency and completeness. Simulated annealing is such an algorithm. In metallurgy, annealing is the process usedSimulated annealing to temper or harden metals and glass by heating them to a high temperature and then gradually cooling them, thus allowing the material to reach a low-energy crystalline state. To explain simulated annealing, we switch our point of view from hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a ping-pong ball into the deepest crevice in a very bumpy surface. If we just let the ball roll, it will come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the local minimum—perhaps into a deeper local minimum, where it will spend more time. The trick is to shake just hard enough to bounce the ball out of local minima but not hard enough to dislodge it from the global minimum. The simulated-annealing solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the intensity of the shaking (i.e., lower the temperature).
        - 
        - https://remnote-user-data.s3.amazonaws.com/9iqHzrKkCNLr1O0Stp2JRZjLemimB82SghITaiYqpAWWgRel46clhD8btGTn5Ank6B40OKVf9-ldpi4lp3Aa1xhPhQv2ARrAArz88PuDXfyYr3KJIJPyb0WS1M21XpMQ.png
        - 
        - Is fundamentally similar to Investment in Loss: occasionally you want to choose loosing options as in the long-run it can lead you to better ones. 
        - 
    - Evolutionary State Space Search
        - Evolutionary algorithms treat the population of states as organisms undergoing natural selection. The fittest (highest-value) individuals produce offspring through crossover and mutation:​
            - Crossover: Combine parts of two parent solutions to create offspring (analogous to sexual reproduction)
            - Mutation: Randomly alter offspring to introduce variation
            - Selection: Choose parents probabilistically based on fitness
        - Genetic algorithms work best when useful building blocks (schemas) exist—parts of the solution that can be effectively combined.
        - 
        - https://remnote-user-data.s3.amazonaws.com/E4rChVlKsA1AtY5jNb8kLXyKjhyzYP7eLjo7fNlDgBw32BAOY2uMcid3whtGR_wCbMmP6Fds_h1xs4bMlgYwboXt8CEvLN4WBV5wTHtviOWpb515N39tzjkp8Inu0zFQ.pnghttps://remnote-user-data.s3.amazonaws.com/WRPntYEUGXujZLn8bU7wJZ0BecKnuj_3jJuW6Cwz40Q86kjQQXpWx5_IR9S1Koe0AQ5g4sI33ys6-XyJ-zUQZJnqwag0o3mp3EHY3fvY25BsO7NRaeCEcch_mgSc5FKr.pnghttps://remnote-user-data.s3.amazonaws.com/WRPntYEUGXujZLn8bU7wJZ0BecKnuj_3jJuW6Cwz40Q86kjQQXpWx5_IR9S1Koe0AQ5g4sI33ys6-XyJ-zUQZJnqwag0o3mp3EHY3fvY25BsO7NRaeCEcch_mgSc5FKr.pnghttps://remnote-user-data.s3.amazonaws.com/E4rChVlKsA1AtY5jNb8kLXyKjhyzYP7eLjo7fNlDgBw32BAOY2uMcid3whtGR_wCbMmP6Fds_h1xs4bMlgYwboXt8CEvLN4WBV5wTHtviOWpb515N39tzjkp8Inu0zFQ.png
        - 
    - 
    - State Space Search in Continuous Environments
        - Empirical Gradient Descent
        - From the book generally on continuous environments and state space search:
        - Constrained Optimization, Linear Programming, Convex Optimization (also vs Constraint Satisfaction Problem)
        - 
    - State Space Search in Non-Deterministic Environments
        - Belief State, Conditional Plan (also for Partially Observable State Space Search)
            - https://remnote-user-data.s3.amazonaws.com/_VQkFZmVBRwZI-ik4sm2-3AzrbcQ6mWXPDkJZwFk4EqEJQmgxx3cORfBZxrqHy26xkhtEGrmx-XrdsdAsZjCStu1ObJmQ176ZBv5GrPfZN1vSDXEmthtjfg29npnvcVt.pnghttps://remnote-user-data.s3.amazonaws.com/_VQkFZmVBRwZI-ik4sm2-3AzrbcQ6mWXPDkJZwFk4EqEJQmgxx3cORfBZxrqHy26xkhtEGrmx-XrdsdAsZjCStu1ObJmQ176ZBv5GrPfZN1vSDXEmthtjfg29npnvcVt.png
            - 
        - AND-OR Search Tree
        - Cyclic Solution
        - Perplexity summary
        - 
    - State Space Search in Partially Observable Environments
        - Perplexity summary
        - 
    - Online Search
        - Offline Search is when an agents compute full solution before taking their first action. Online Search takes actions and then observes its Environment. 
        - Online Search is useful for Dynamic and Semi-Dynamic Environments.
        - Perfect example for Online Search is Mapping Problem where robot has to build a map of the space it operates in ‒ exactly what vacuum robots do.
        - Perplexity summary
        - 
    - 
- Constraint Satisfaction Problem
    - Fundamentals
    - 
    - Constraint Propagation: Inference in CSPs
        - A number of inference techniques use the constraints to rule out certain variable assignments. These include node, arc, path, and k-consistency.A number of inference techniques use the constraints to rule out certain variable assignments. These include node, arc, path, and k-consistency.
        - 
        - Node Consistency
        - Arc Consistency
        - Path Consistency
        - k-Consistency
        - Global Constraint
        - 
        - Perplexity summary
        - 
    - Backtracking Search for CSPs
        - Backtracking search is a type of a search algorithm that takes into account the structure of a constraint satisfaction search problem. In general, it is a recursive function that attempts to continue assigning values as long as they satisfy the constraints. If constraints are violated, it tries a different assignment. CS50ai
            - https://remnote-user-data.s3.amazonaws.com/a3te6f9_BRLDgDrLWp2ZI7MMf9bnM5YgNpHyn_t2o6POCcY5MwkC6R7acAY2PfoG5RIEvad8oKAXeZJMu0aaOo490vc06G1GlHVd9_zLzk1eoefYRhNYrbON0bJRrY5y.png
            - https://remnote-user-data.s3.amazonaws.com/_0NBEA5znD1IKuaqV_LbrIh7eu2peC-97pCw-_Lsq7Up1pIqeHJceR6wp1TY6LjITXTxNiAT0BiE9m-IKKSSnFL-FqgFpsgfj3YkrEJScnmSal_WGrmGw78aQl33azNW.png
            - https://remnote-user-data.s3.amazonaws.com/KgPjhc4UNV4FnJkY5-V__GT4C3rcWgZwSIziUXE02Z3-DTUTRtjir-Evs6Agnx1SQSeRxhhzw-xTvVvQGh5U5sEPouPVwXlHeoBJEVM0gXWdeCtUKgjLFHq4j2DGi69U.pnghttps://remnote-user-data.s3.amazonaws.com/KgPjhc4UNV4FnJkY5-V__GT4C3rcWgZwSIziUXE02Z3-DTUTRtjir-Evs6Agnx1SQSeRxhhzw-xTvVvQGh5U5sEPouPVwXlHeoBJEVM0gXWdeCtUKgjLFHq4j2DGi69U.png
            - 
        - 
        - Maintaining Arc Consistency
        - 
    - Local Search for Constraint Satisfaction Problems
        - https://remnote-user-data.s3.amazonaws.com/M5udQ0d_xzbrllv2GlJjJT2RdODrfAjEZMghJzJqpAk2xxup3SqCc_3YWiRs12xGUqAfSdV8b6EFnv2WXFZz8XL0Vn1BEaFRaEkc2DCFOd01eSdOwu48burh2ObgcPym.pnghttps://remnote-user-data.s3.amazonaws.com/M5udQ0d_xzbrllv2GlJjJT2RdODrfAjEZMghJzJqpAk2xxup3SqCc_3YWiRs12xGUqAfSdV8b6EFnv2WXFZz8XL0Vn1BEaFRaEkc2DCFOd01eSdOwu48burh2ObgcPym.png
        - 
    - Heuristics for Constraint Satisfaction Problems
        - https://remnote-user-data.s3.amazonaws.com/VfzOKI2aT_t_QvZQpJqc7nV4iBc1pdL-ZsljwTS7J8-odAlIQcJNuRVMV8fEZQDFbk4hx1hSaea8YfFygpGhFFNd2T3ccsGhTFA6Vxt2z5XCTKZam__bDnDmNznPYueA.png
        - 
    - Constrained Optimization in Constraint Satisfaction Problems
        - That means we're trying to find solution that both (1) satisfies our constraints, and (2) performs the best on the Objective Function.
        - Linear Programming, Convex Optimization
        - https://remnote-user-data.s3.amazonaws.com/VLEU5kvxA_0YceU4wwwi9OjhZka_e96okkSzuvibr8UKMuZD4YpY878q3nzAjsMX7EpjBp_VSiKmCsivIgM0aqjraJ4d7LGfx81AWIIIYlBXYDgPU06MfupwKQLykjKk.png
        - 
    - Structure of CSP problems
        - https://remnote-user-data.s3.amazonaws.com/urq35jHcPasPG49bk6-lfjBUe1LZFfF6sw9V56I0RcwZkmaGvNRU0NKorIak43maxKYjsHW_xIUKXVYeAkD9dwFqR4QPyQUhAEUKO8k0oKXRPRjHZfYY6PdQp96tzq2J.pnghttps://remnote-user-data.s3.amazonaws.com/urq35jHcPasPG49bk6-lfjBUe1LZFfF6sw9V56I0RcwZkmaGvNRU0NKorIak43maxKYjsHW_xIUKXVYeAkD9dwFqR4QPyQUhAEUKO8k0oKXRPRjHZfYY6PdQp96tzq2J.png
        - 
    - 
    - Panoramic view of Constraint Satisfaction Problems, and summary
        - https://remnote-user-data.s3.amazonaws.com/9h2xFIVo6hpsRhFbhmMstmT3XmUXBPYux-2gWHkyNLlrF3Xaiu8_PyJQyP-2vrk_w6tuEbRKGmU32r4kTjfB7b1qGJ-apvKXl2Tw26mbeu8KbkgsZFjWz_D9i-_uBTRH.png
        - https://remnote-user-data.s3.amazonaws.com/AQYmWWXoOj6gD5o_pcyDW6xHFwSnfvxTQfw1-9IEABNbJi6EkUAPC3B-JoMf6bKr6IN0iqn2Gwid0AnhQSs4fX4ag7aSgkkW_5UdGUAtiPWqsne3g5XHoTUjt78z2z2J.pnghttps://remnote-user-data.s3.amazonaws.com/4xgQkzQPZrmwe6f_-aYjjP1CKd9G9ZdEhQ3HmQcy7lmItm6T7Dvl_hTdSuE8xW89djGIrcRg4iNrNmkdPypwWudPP6kanJNLHM4fIqQm-AkJutNcCEA64Us3QvMrN4HC.pnghttps://remnote-user-data.s3.amazonaws.com/4xgQkzQPZrmwe6f_-aYjjP1CKd9G9ZdEhQ3HmQcy7lmItm6T7Dvl_hTdSuE8xW89djGIrcRg4iNrNmkdPypwWudPP6kanJNLHM4fIqQm-AkJutNcCEA64Us3QvMrN4HC.pnghttps://remnote-user-data.s3.amazonaws.com/AQYmWWXoOj6gD5o_pcyDW6xHFwSnfvxTQfw1-9IEABNbJi6EkUAPC3B-JoMf6bKr6IN0iqn2Gwid0AnhQSs4fX4ag7aSgkkW_5UdGUAtiPWqsne3g5XHoTUjt78z2z2J.png
        - 
    - 
- Adversarial Search
    - Now let's talk Multiagent, Competitive environments!
    - 
    - Fundamentals
        - https://remnote-user-data.s3.amazonaws.com/gDEsO5KBpm5mdRQiXjQ9COJ7_X06pxL0j8fd2RN_Xvj1WYDH1lzIVy7_gT5k__-qtwWWloZDT4BbzuVLhhUImyEaB4A_KVCrqYwK0wu4TaEw8fhhSwyaEd5naCMgzxsD.png
        - https://remnote-user-data.s3.amazonaws.com/udov7IpVeMVzgdIlwEnktpeJiQlFaK2QujoxuALnO6wEVyN8FiKxr_OyzTiOoYx4IDS_JI9NrdJrR16tkuUkpz1w3rmXmWrbQkFxiVDaEhfDqJXl0ECOcGxXrpA88b7g.png
        - 
        - Game Theory
        - 
    - 
    - Minimax
        - A decision-making algorithm for Adversarial Search. Assumes the opponent plays optimally.
        - In basic example there are two players, one is trying to maximize the score (1), the other one is trying to minimize it (-1). (0) stands for a draw. This introduces adversity to the Search in Simple Environments problem.
        - 
        - The essence of minimax
        - Pseudocode implementing minimax  
        - https://remnote-user-data.s3.amazonaws.com/5WhOx8FPchSmV4Z69vmNP8T6T3FgTGL_Fzf397sj7BldfpgcTEDyqj1_11qPhrGRwAs_OgsxJvjHYc041fU0V27IK2857sz1cBDs_FpUBl17mPFxHsMnX8_tgjWuyyBa.png
        - https://remnote-user-data.s3.amazonaws.com/I2urvkKRFndXAVAU0zwW16AkbjhvWWFpYDZ_NDC46jsWLDKlsBXskoCuZ4sNtMtJ71dCfZXgxKFYzsJVz4uXB5TG7b-iTFb1GOnifFOCcj925eJM4_6ThlP69BW0vANU.png
        - 
    - Alpha-Beta Pruning
        - A search algorithm that reduces the search space in game trees by eliminating branches that cannot possibly affect the final decision.
        - Example
        - 
        - https://remnote-user-data.s3.amazonaws.com/_oS3c0VRfvnagnygYnnzwhC0N21bxUiHfFa6FtbPV3iZ6oMlsV2uGee4tMppXdxEDsAWX9BN6Y8NQy50VCGbikzWWU-fQenr8_mxa6MeT4x1qDJe7a1Xlc16ZeAHK1AE.png
        - https://remnote-user-data.s3.amazonaws.com/fN-qQfDjeoTPUZV2HLDVHaQ1_tEBdukuNKlH3buZ2uF_iSaONlElCurKnDU1h7jCzh0PuZxDoJwHjld6yrcUX19BFv1AdlQLAp1MDE9WuPT1whvwFCdcpv-OzzNHIfxY.png
        - 
    - Depth Limited Minimax
        - Minimax that explores game trees only up to a specified depth.
        - https://remnote-user-data.s3.amazonaws.com/LbjYv44UG4mxuljYpf1idFr_Oh_LwcJeF7EKRpsPX1PowuU_11_HnfznCqJXsUZak6K45Vy1FSpsRiQw__3K5sO8LKY4aZkMYjRr8coQB65AB7Vu81vYms7bPtsRi9Q5.png
        - def minimax_depth_limited(state, depth, maximizing_player):
    # Terminal condition: either game is over OR we've reached depth limit
    if is_terminal(state) or depth == 0:
        return evaluate(state)  # Use eval function instead of utility
    
    # Rest is identical to standard minimax
    if maximizing_player:
        value = -∞
        for action in legal_moves(state):
            value = max(value, minimax_depth_limited(result(state, action), depth-1, False))
        return value
    else:
        value = +∞
        for action in legal_moves(state):
            value = min(value, minimax_depth_limited(result(state, action), depth-1, True))
        return value

        - 
    - Evaluation Function
        - A Heuristic Function that estimates the value of a game state.
        - https://remnote-user-data.s3.amazonaws.com/mgpssW4sx8ir3kjSeHkLSCrkvYh3H2-WEGNh12D9SAmQ7aOjL4SR48BdtIj0AcTGBajJonz5LoG-Kw92_EPoYb861-kRZyoFXy04fucYE59u-bjWZb1JGsJGiISwfhTj.png
        - 
    - 
    - Monte Carlo Tree Search
        - https://remnote-user-data.s3.amazonaws.com/9zaCW31nbuX-SPSD5bpsr8z8Hgcgn3Ue43Ta9yb4mKx7fBQZem-86-T8cm3nqC2LTw6Gd-uXtKB-_ZYyq5xh62-4uAnNfdb4IDva5TUCJynbRZDEv55-Fck3FV3S3Zsa.png
        - 
        - https://remnote-user-data.s3.amazonaws.com/lo2-B0b531U2fXv9fVydK4FBDvjqe5BpekgQpq5CSIEsMs0PbR73ie-32zTHBgp3Sc1k3BDLKcdelIN_TANrK0xks6EknR1B8zwFobu_aX3kG3hQ_4weAbNc6A-ITtfC.pnghttps://remnote-user-data.s3.amazonaws.com/jlc02yW7mgpcOcTz_NMOGC-Xi0_NiOu3lPgisXFigAU-EuErMsHZfTY5D8nMu3n13LoP1IJ-fbX63JtLeQcImB-blt1jy_1Frq4ugvyWA1R5dRlYC4ELTOLMLoAwzYHl.pnghttps://remnote-user-data.s3.amazonaws.com/jlc02yW7mgpcOcTz_NMOGC-Xi0_NiOu3lPgisXFigAU-EuErMsHZfTY5D8nMu3n13LoP1IJ-fbX63JtLeQcImB-blt1jy_1Frq4ugvyWA1R5dRlYC4ELTOLMLoAwzYHl.pnghttps://remnote-user-data.s3.amazonaws.com/lo2-B0b531U2fXv9fVydK4FBDvjqe5BpekgQpq5CSIEsMs0PbR73ie-32zTHBgp3Sc1k3BDLKcdelIN_TANrK0xks6EknR1B8zwFobu_aX3kG3hQ_4weAbNc6A-ITtfC.png
        - https://remnote-user-data.s3.amazonaws.com/6WFW2lm8A9wfunFu1jsQq-uhvUzhTw1EaH_FSfKii-uE3I0_2vMLWZjzMf2XQz4JMVuV696XrkmSEOt-K1Ph-VPJ5ES6UdHhBKd5wHKM1H4FxmeRjwlWYZAZS27U6pOB.pnghttps://remnote-user-data.s3.amazonaws.com/6WFW2lm8A9wfunFu1jsQq-uhvUzhTw1EaH_FSfKii-uE3I0_2vMLWZjzMf2XQz4JMVuV696XrkmSEOt-K1Ph-VPJ5ES6UdHhBKd5wHKM1H4FxmeRjwlWYZAZS27U6pOB.png
        - 
        - (Monte Carlo Tree Search vs Monte Carlo Simulation)
        - 
    - Non-Deterministic (stochastic) games
        - https://remnote-user-data.s3.amazonaws.com/cZRLhmFeuD-Va4Z_4TloeMxJpBODZ_YsUn9QYdASHBBwlKPDxElHs7ypoZO-nCHAUFYh62_vgvgrsVJL4EBTcimfjvSYxSQjjo0cgZnGKijLF1tQsIiHs5dNx1f2CPu-.pnghttps://remnote-user-data.s3.amazonaws.com/UCl86FtfzcRC7ECOVAbDebnbbgoKnpInhOOypuDhk-lwk_qwDMvMfTf1--IpR5RJuwZUvjZJBA5UMbqExNBFWME-GQxn9v7PGaM0SKwI6HPvDfP3VcfgUuEIPL17geIW.pnghttps://remnote-user-data.s3.amazonaws.com/UCl86FtfzcRC7ECOVAbDebnbbgoKnpInhOOypuDhk-lwk_qwDMvMfTf1--IpR5RJuwZUvjZJBA5UMbqExNBFWME-GQxn9v7PGaM0SKwI6HPvDfP3VcfgUuEIPL17geIW.pnghttps://remnote-user-data.s3.amazonaws.com/cZRLhmFeuD-Va4Z_4TloeMxJpBODZ_YsUn9QYdASHBBwlKPDxElHs7ypoZO-nCHAUFYh62_vgvgrsVJL4EBTcimfjvSYxSQjjo0cgZnGKijLF1tQsIiHs5dNx1f2CPu-.png
        - 
        - https://remnote-user-data.s3.amazonaws.com/IQ0xrWPUzxtuH9CN20R1dDUV0N2Q5gYvgLZF0Zzj_14X9XH16iyJLWrIAyO2_cAGw4suX9-cCUZk5qy8Ga2NwQKzgJv92k7VZqCOhOrA7bOvCgZLgHx4jVW6NJ3nTN3c.png
        - https://remnote-user-data.s3.amazonaws.com/SP1uLqud6I-MS4R38HyF9LSZweCKZG2dBRQMeixIxXyvcw2lzPTmuk0P-ohtyMIUYajUYeJsNaF2crXGzVBZ7N9HSXhkJbe3eONnKa4_QifiYOVhDT60GwmguzmA1HUz.png
        - 
    - Partially Observable games
        - https://remnote-user-data.s3.amazonaws.com/KkbeU5rvH9TdgvhReVZoxg3N7yoz_GkKyHmS4Tn_oHyTXKqVKc1MEGokUFyAyeXOuWU1r--toeIQFTJnXSuLrqxgPZ_ZTDFLBbffidzUFRIGNV-iUYMwY9tIORYmJh51.pnghttps://remnote-user-data.s3.amazonaws.com/KkbeU5rvH9TdgvhReVZoxg3N7yoz_GkKyHmS4Tn_oHyTXKqVKc1MEGokUFyAyeXOuWU1r--toeIQFTJnXSuLrqxgPZ_ZTDFLBbffidzUFRIGNV-iUYMwY9tIORYmJh51.png
        - 
    - Limitations of game search algorithms
        - https://remnote-user-data.s3.amazonaws.com/rM7Lv0i238qQ17d9NJ0YeGLUQoebtyQkvpdekv7RuCF202PgkLJxX_3-xxOStF1vx7ld3M7ifjIhHZ0cSyHSYBBw3teKlB64wI4WHEYa7Q_JCNuyuWKejzrmmKoB95yl.png
        - https://remnote-user-data.s3.amazonaws.com/AyK8lF3sKNpJYlZ1YhXG1kzgly4SBVEZF5MvjzjBC3CmencNOSuurYkAUVP6hagiLhrUIl51pNGjwgnZ1_y_7o2ephKt-eNXLWnieox3byS79mgiOobYnIkWtGjT-G_2.pnghttps://remnote-user-data.s3.amazonaws.com/zS59Q5IFM1zO3SRcHV1JB_MYUoJgOX0_d2ZctY6_U3gvBZXFXKU4s60LAybw_NetERg5dqb-rMtfsP4Xz6ttYWIoKRxzS2uYVCisbcNmcFv7CbSnlJHBbD9b8D_LNDvX.pnghttps://remnote-user-data.s3.amazonaws.com/zS59Q5IFM1zO3SRcHV1JB_MYUoJgOX0_d2ZctY6_U3gvBZXFXKU4s60LAybw_NetERg5dqb-rMtfsP4Xz6ttYWIoKRxzS2uYVCisbcNmcFv7CbSnlJHBbD9b8D_LNDvX.pnghttps://remnote-user-data.s3.amazonaws.com/AyK8lF3sKNpJYlZ1YhXG1kzgly4SBVEZF5MvjzjBC3CmencNOSuurYkAUVP6hagiLhrUIl51pNGjwgnZ1_y_7o2ephKt-eNXLWnieox3byS79mgiOobYnIkWtGjT-G_2.png
        - 
    - 
    - Panoramic view and summary
        - https://remnote-user-data.s3.amazonaws.com/3KPu-xhPTBF4mxxo5rTNhd4NIKbPlDL3I9Iz_SQU2HaprDkIM4fl6tucUtyE0Rb6Sa8c0FNa2n9hMSUEfzZKYXfBWiQXQ0LQ2wP1IPO_QS_u942mjFmFmQeyCC7B7YM4.png
        - 
        - https://remnote-user-data.s3.amazonaws.com/ZI9suOdGYunqCWk5gVG02t-CPjIHX5JKnC_WCaUTw27cYUZdLFOVrynsUTk8UHqFChHRhLghg6Zbiph8lgP61FEJ__jXCcV1hqT2kw4tJXkZS7sA6FBcjk_pkMa3hUdQ.png
        - 
        - https://remnote-user-data.s3.amazonaws.com/hGhKIe6Mb0kImfa5-ygvBP_FQt5zcWSGVfGOW8UB6S9-Lcj9jvjNlnpisd8jYttdfz8Why8g5uuViqaHZNP0nQ2egn-mXSX_G1wXGOkngsyjSKAV1Qca_dTWMxfPhk1O.pnghttps://remnote-user-data.s3.amazonaws.com/q1l1SOAYNXQ5dUYUQ9p0jnffSNSDBkENtnIFcTQRDItLjiVidZsExPBc2qp_IucyZXRRk5LsBkAPX0rXuVSVt1HtooH_rtdXDrZoZyvUQzGQFjcCgaBhYhQTRWzC5cdT.pnghttps://remnote-user-data.s3.amazonaws.com/q1l1SOAYNXQ5dUYUQ9p0jnffSNSDBkENtnIFcTQRDItLjiVidZsExPBc2qp_IucyZXRRk5LsBkAPX0rXuVSVt1HtooH_rtdXDrZoZyvUQzGQFjcCgaBhYhQTRWzC5cdT.pnghttps://remnote-user-data.s3.amazonaws.com/hGhKIe6Mb0kImfa5-ygvBP_FQt5zcWSGVfGOW8UB6S9-Lcj9jvjNlnpisd8jYttdfz8Why8g5uuViqaHZNP0nQ2egn-mXSX_G1wXGOkngsyjSKAV1Qca_dTWMxfPhk1O.png
        - 
    - 
    - More
    - 
- 


