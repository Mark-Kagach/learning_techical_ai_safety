# Goal
Study and understand chapter 19 of AI modern approach (depending on how it goes pursue 20, 21, 22, 23).
The process is: 
1. make first round notes on the chapter
2. revisit the notes and making more cohesive ones
3. in last revision understand it all, connect to past knowledge and prepare everything for future maintenance via flashcards

# Notes

Spend last two days learning linear regression and classification, nonparametric models, and ensemble learning: 19.6-19.9; 694-722 pages.

So regarding linear functions it is: Univariate Linear Function, Multivariable Linear Function, and Classifier Linear Functions. Gradient descent, support vector machines, ensemble learning methods: bagging, random forests, boosted ensembles, gradient boosting.

I added notes directly to my folders on the topic, so it is harder to add exactly the things I wrote. Here are some of them:

### Gradient Descent:
- Gradient Descent is a method for minimizing loss that can be applied to any loss function, no matter how complex? Why not hypothesis function?
    - https://remnote-user-data.s3.amazonaws.com/pnm09GrqToIt3as8h1sPlZC4CWmq1B5mn7jx2wiKVdaOSxLxhuUggxFuRDAAqoGHxm1Loa74iBnUc0GkGNLfzvjX9w7expyHyyuZisWLTHceNZt22rjFklNDK7_ZJ9rG.pnghttps://remnote-user-data.s3.amazonaws.com/291DaIxfY5UeCQSSJ1NRdP_sC_zvlJa_Uq0q1BB8sEBFqRo2DNxYE7V_CVYhEffCeTbZv7YxMYQhWls0-6C_c7FdoKOJWOjVOf0HMZmhn3Fi-dbt0pyn5m7Z7mXE3aDL.pnghttps://remnote-user-data.s3.amazonaws.com/291DaIxfY5UeCQSSJ1NRdP_sC_zvlJa_Uq0q1BB8sEBFqRo2DNxYE7V_CVYhEffCeTbZv7YxMYQhWls0-6C_c7FdoKOJWOjVOf0HMZmhn3Fi-dbt0pyn5m7Z7mXE3aDL.pnghttps://remnote-user-data.s3.amazonaws.com/pnm09GrqToIt3as8h1sPlZC4CWmq1B5mn7jx2wiKVdaOSxLxhuUggxFuRDAAqoGHxm1Loa74iBnUc0GkGNLfzvjX9w7expyHyyuZisWLTHceNZt22rjFklNDK7_ZJ9rG.png
    - 
- Similar to Hill Climbing we're incrementally tweaking weights and traversing the Weight Space, only we're going downhill as we're minimizing loss. 
- We start from any place in the (w_0, w_1) plane, and then compute an estimate of the Gradient and move a small amount in the steepest downhill direction.
- \alpha Learning Rate is by how much we move in the steepest downhill direction, can be constant, or can change throughout learning process.
- Calculus, Chain Rule; Batch Gradient Descent (Gradient Descent calculated over all training examples every time), and Epoch ‒ step that covers all of the training examples.
    - https://remnote-user-data.s3.amazonaws.com/MlRYEhVfDlW7TBuVSutktQM3dy2dfn6Oc3WHCq_iGomKzsSpiw-o6kHT9I7gjL8gR2kUgE2XfJSXCSoTekXkZshGKHuPw51iu8yXxZgUtit-11P4gguznVOknlgvotuP.pnghttps://remnote-user-data.s3.amazonaws.com/i1rZqvK0CsWQ8h3HOr4Y52_Yl0FdSUhMxcwSqij_qLFaxWzH0saPsw0WoN03OVveHc0GUR814RQNg2DCWpRIyTJkS_-o9dfsxtVp0pdpUzdckseND4-awvGg_n-6fVkr.pnghttps://remnote-user-data.s3.amazonaws.com/i1rZqvK0CsWQ8h3HOr4Y52_Yl0FdSUhMxcwSqij_qLFaxWzH0saPsw0WoN03OVveHc0GUR814RQNg2DCWpRIyTJkS_-o9dfsxtVp0pdpUzdckseND4-awvGg_n-6fVkr.pnghttps://remnote-user-data.s3.amazonaws.com/MlRYEhVfDlW7TBuVSutktQM3dy2dfn6Oc3WHCq_iGomKzsSpiw-o6kHT9I7gjL8gR2kUgE2XfJSXCSoTekXkZshGKHuPw51iu8yXxZgUtit-11P4gguznVOknlgvotuP.png
- Minibatch Stochastic Gradient Descent (Minibatch SGD) ‒ a faster variant of Batch Gradient Descent that randomly selects a small number of training examples (aka Minibatch Gradient Descent, ) at each step. 
    - How that relates to Minibatch Gradient Descent?
    - Minibatch Stochastic Gradient Descent is about picking random training examples, can be one if you want (without minibatch it is just about random selections...).
    - And Minibatch Gradient Descent highlights that you can take just a small part of all the training examples. You combine the two and you get actually useful method. Can be called both ways, depending on the source.
    - https://remnote-user-data.s3.amazonaws.com/YhgsPK5O86-1mAXFMGPkwbFvUIYrNjh0uYE_mXp3OSoV0DCCgyI_ko34TzuUEqVAh4OudoPyDj-c9lkVFkcLg5qLvUtYvKu4TYNWxZnNzSL0KMs1Jh4DRhko3wZiqkHH.pnghttps://remnote-user-data.s3.amazonaws.com/YhgsPK5O86-1mAXFMGPkwbFvUIYrNjh0uYE_mXp3OSoV0DCCgyI_ko34TzuUEqVAh4OudoPyDj-c9lkVFkcLg5qLvUtYvKu4TYNWxZnNzSL0KMs1Jh4DRhko3wZiqkHH.png
    - 
- Minibatch SGD is not guaranteed to converge (settle), it might always oscillate around the minimum. To fix that we apply Simulated Annealing where \alpha Learning Rate eventually decreases throughout the learning process.
    - https://remnote-user-data.s3.amazonaws.com/7X__nn04JaY7EaVJeM20VPezWLulFJ0Opu4G0EUtfZFO6-yYDkmIgZ-GPKsm2GCtp6bjHwGLF-x4wm6WVgMuZ8jYlTTx18-feoMAYlYqxmTitupLY41T1puWFZf1m2Ql.pnghttps://remnote-user-data.s3.amazonaws.com/7X__nn04JaY7EaVJeM20VPezWLulFJ0Opu4G0EUtfZFO6-yYDkmIgZ-GPKsm2GCtp6bjHwGLF-x4wm6WVgMuZ8jYlTTx18-feoMAYlYqxmTitupLY41T1puWFZf1m2Ql.png
    - 
- Online Gradient Descent for Online Learning problems:
    - https://remnote-user-data.s3.amazonaws.com/0f4V3xbky_yaCRmpEXPZNeK7qTdbMUGbr5m67zns1YPWf4K68OJe70QM0tTCiThB1FC0LX7k6OeLILAUW455z5O0x1Wz3CSQNTa_tOchtPJ6g_Pcl2JTpdsydvVBH8PM.pnghttps://remnote-user-data.s3.amazonaws.com/0f4V3xbky_yaCRmpEXPZNeK7qTdbMUGbr5m67zns1YPWf4K68OJe70QM0tTCiThB1FC0LX7k6OeLILAUW455z5O0x1Wz3CSQNTa_tOchtPJ6g_Pcl2JTpdsydvVBH8PM.png
    - 
- 

### SVMs, Ensemble Learning

- Support Vector Machine
    - Three good properties of SVMs:
        - https://remnote-user-data.s3.amazonaws.com/Goq7TPUi-TLOd7m0P3Q0vzcA9F1aGzfYrisy1DK1A37SgNgy9L0VS74CcJNCGUtkDlQf3GoI0gtjx7GDveTeDeEIc4vlbCof6hRwNJTNjTSEi_iKH3NpX6RzFJZyNK0o.pnghttps://remnote-user-data.s3.amazonaws.com/Goq7TPUi-TLOd7m0P3Q0vzcA9F1aGzfYrisy1DK1A37SgNgy9L0VS74CcJNCGUtkDlQf3GoI0gtjx7GDveTeDeEIc4vlbCof6hRwNJTNjTSEi_iKH3NpX6RzFJZyNK0o.png
        - 
    - A good example showing how SVMs prioritize training examples ‒ not all training examples are made equal:
        - https://remnote-user-data.s3.amazonaws.com/Kf-KPAvTRJLTZPzXWmxXOfwa7iklMLJ17GBtGbLogpAEf-yPWuOdy8wWGs-nrUv8hPp8EIpshG5AAHiEEkYYLgh7I_H4Uhtx0PLCVwh0epNJqx2ty14vVLhU9xqW3BS-.pnghttps://remnote-user-data.s3.amazonaws.com/4e7pVuP1P5c9bIyM8aG36Xjc-qjCXnYU_6JMrun8xh6dUTOxJfw20XF-V93eb3pJFxWzePrXlCZJ-Ge4ZQ89vCOEPj2_x5A0GmRKk2WADUQxzkeYTkbC8GVAvGv0em6w.pnghttps://remnote-user-data.s3.amazonaws.com/4e7pVuP1P5c9bIyM8aG36Xjc-qjCXnYU_6JMrun8xh6dUTOxJfw20XF-V93eb3pJFxWzePrXlCZJ-Ge4ZQ89vCOEPj2_x5A0GmRKk2WADUQxzkeYTkbC8GVAvGv0em6w.pnghttps://remnote-user-data.s3.amazonaws.com/sv9JWYqHwNLaw-ECZByK3cvIJnTOoCAjAnctTPRtBA_Hy8ON_57fEBCpjoxuzK2M-_QWIdH5vORQEPe9TzGcfK7SfcAGx1erKFOzcxR-GSyrVaMYZaJk58xRYgKbWKMi.pnghttps://remnote-user-data.s3.amazonaws.com/sv9JWYqHwNLaw-ECZByK3cvIJnTOoCAjAnctTPRtBA_Hy8ON_57fEBCpjoxuzK2M-_QWIdH5vORQEPe9TzGcfK7SfcAGx1erKFOzcxR-GSyrVaMYZaJk58xRYgKbWKMi.pnghttps://remnote-user-data.s3.amazonaws.com/Kf-KPAvTRJLTZPzXWmxXOfwa7iklMLJ17GBtGbLogpAEf-yPWuOdy8wWGs-nrUv8hPp8EIpshG5AAHiEEkYYLgh7I_H4Uhtx0PLCVwh0epNJqx2ty14vVLhU9xqW3BS-.png
        - 
    - 
    - The main SVMs thing is that they choose separator that is farthest away from the training examples. We call this separator Maximum Margin Separator. You can see what Margin is below:
        - https://remnote-user-data.s3.amazonaws.com/4e7pVuP1P5c9bIyM8aG36Xjc-qjCXnYU_6JMrun8xh6dUTOxJfw20XF-V93eb3pJFxWzePrXlCZJ-Ge4ZQ89vCOEPj2_x5A0GmRKk2WADUQxzkeYTkbC8GVAvGv0em6w.pnghttps://remnote-user-data.s3.amazonaws.com/4e7pVuP1P5c9bIyM8aG36Xjc-qjCXnYU_6JMrun8xh6dUTOxJfw20XF-V93eb3pJFxWzePrXlCZJ-Ge4ZQ89vCOEPj2_x5A0GmRKk2WADUQxzkeYTkbC8GVAvGv0em6w.png
        - We call this separator, shown in Figure 19.21(b) the maximum margin separator. The margin is the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the separator to the nearest example point.We call this separator, shown in Figure 19.21(b) the maximum margin separator. The margin Maximum margin separator Marginis the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the separator to the nearest example point.
        - 
    - Interestingly, this is supported by Computational Learning Theory ‒ choosing the farthest away separator is suggested to minimize Generalization Loss. 
        - https://remnote-user-data.s3.amazonaws.com/c6yis06V0rH9Ut2c2vdNI5Y7DtXbjsyEgIWpYnQm7n-56D7IqnfBgcsPEoWaGSsOkZuYfNk8e-TxxFxLFhF7KxB46infxSGbxaQnwbqqeT4XbqwQiMvWzl-VhYPB8l9S.pnghttps://remnote-user-data.s3.amazonaws.com/c6yis06V0rH9Ut2c2vdNI5Y7DtXbjsyEgIWpYnQm7n-56D7IqnfBgcsPEoWaGSsOkZuYfNk8e-TxxFxLFhF7KxB46infxSGbxaQnwbqqeT4XbqwQiMvWzl-VhYPB8l9S.pnghttps://remnote-user-data.s3.amazonaws.com/vHGtSWTUMTFHgnA2SFEjUsKRlXFAwTgC5xGSWh-Lpqz8ZHhLtblpOS7PNrrLgXbhv8TxAT6nv336NHDM74ulyH6-Re2N9iQ0NDoe0Z2td0yKRBFYmpYnWmA9L0Ews-5-.pnghttps://remnote-user-data.s3.amazonaws.com/sv9JWYqHwNLaw-ECZByK3cvIJnTOoCAjAnctTPRtBA_Hy8ON_57fEBCpjoxuzK2M-_QWIdH5vORQEPe9TzGcfK7SfcAGx1erKFOzcxR-GSyrVaMYZaJk58xRYgKbWKMi.png
        - 
    - 
    - Calculations behind finding the Maximum Margin Separator (Quadratic Programming, Support Vector):
        - https://remnote-user-data.s3.amazonaws.com/CHeqSfNxOFPcjv5t-2VQVWxR30XCjfDjOJ9aWGm8-nZpxVl-4vunE8VjY1o1zi8LFhkpVo_NUQP9gDskFu_qk1mdDP-rXDgDIM_sJoVePCstK3bhzRNbk7YVWghfwHGA.pnghttps://remnote-user-data.s3.amazonaws.com/SkcWkfCq6NSYedAe5dBHXjsQUSdmCzBRN5yoXPlMt1-zMgyYM9lMZKyuhs9xdvH8sg3SGKVy7nj5uuqC0XC1iFu83J08DvNAoaES_Cd-DEpKfWxFnVgIXVQJrQKAkCBy.pnghttps://remnote-user-data.s3.amazonaws.com/SkcWkfCq6NSYedAe5dBHXjsQUSdmCzBRN5yoXPlMt1-zMgyYM9lMZKyuhs9xdvH8sg3SGKVy7nj5uuqC0XC1iFu83J08DvNAoaES_Cd-DEpKfWxFnVgIXVQJrQKAkCBy.pnghttps://remnote-user-data.s3.amazonaws.com/CHeqSfNxOFPcjv5t-2VQVWxR30XCjfDjOJ9aWGm8-nZpxVl-4vunE8VjY1o1zi8LFhkpVo_NUQP9gDskFu_qk1mdDP-rXDgDIM_sJoVePCstK3bhzRNbk7YVWghfwHGA.png
        - https://remnote-user-data.s3.amazonaws.com/Up-Baj0WbodJSOUsgj0PZnnApMU5w-JO5-EmdLf8j8Sd6HYX9hPjs0HkFGCSF2ylV2UpNHDgHdeeNa_QfkN4QaLR72Ydh5_-_nQVkOdcWj1c5dhyMFmLdB2p-XSFkOfl.pnghttps://remnote-user-data.s3.amazonaws.com/Xxqp15Wi8qVXDQzUMZzSNJ51F5vX2dWPw0y8ZtB2y76RSAzVM2VD2zRtuOhTvpjsKNKoVzW0P6LMrJnbjporgG_ijE47XLyRJbvzbvq7AFHxHPgTLW6gtsPohNc8EI2c.png
        - 
        - On {\mathbf{x} : \mathbf{w} \cdot \mathbf{x} + b = 0} Mathematical Notation:
        - 
    - 
    - On how SVMs find linear separation in where there is none (Kernel Function, Mercer's Theorem, Polynomial Kernel):
        - https://remnote-user-data.s3.amazonaws.com/7g_8d_ScaoV1XASZqJBnE5aNyCu3_bjS4HC4h0ith6ELDQU0aEE6_7IbKXxy8oZtN9C3ezRel75LG1KGd2KFqFVhjq8DzvW3jwZuM5kStnyfA4mrD7Bnc9YGJP3QhhoI.pnghttps://remnote-user-data.s3.amazonaws.com/l4YKLsro02y4rIt9BaoYd8Sd6vgOBn16Rt-vfxlIc7KwttL-Phip6gqPQuOKG53KIZJiPQ8o0Q5E0JzcTXxM3LflpfSks8ESVrRamU1DMhBp6TWj5J8U5QU12VxpWfV6.pnghttps://remnote-user-data.s3.amazonaws.com/l4YKLsro02y4rIt9BaoYd8Sd6vgOBn16Rt-vfxlIc7KwttL-Phip6gqPQuOKG53KIZJiPQ8o0Q5E0JzcTXxM3LflpfSks8ESVrRamU1DMhBp6TWj5J8U5QU12VxpWfV6.pnghttps://remnote-user-data.s3.amazonaws.com/7g_8d_ScaoV1XASZqJBnE5aNyCu3_bjS4HC4h0ith6ELDQU0aEE6_7IbKXxy8oZtN9C3ezRel75LG1KGd2KFqFVhjq8DzvW3jwZuM5kStnyfA4mrD7Bnc9YGJP3QhhoI.png
        - https://remnote-user-data.s3.amazonaws.com/ZLwZW4sCXqte9TmPEWsL8gjAEuw9QHRCDra0lbK919oiyTAvRW7HxrxqztlOFl2xrTjRxi6I7wKqyOPC4WdcsGqwFihy3O0S673Gf79AZ3hvHxqPotlkGeu_E3TYqONm.pnghttps://remnote-user-data.s3.amazonaws.com/ZLwZW4sCXqte9TmPEWsL8gjAEuw9QHRCDra0lbK919oiyTAvRW7HxrxqztlOFl2xrTjRxi6I7wKqyOPC4WdcsGqwFihy3O0S673Gf79AZ3hvHxqPotlkGeu_E3TYqONm.png
        - 
    - This phenomenon is actually fairly general: if data are mapped into a space of sufﬁciently high dimension, then they will almost always be linearly separable—if you look at a set of points from enough directions, you’ll ﬁnd a way to make them line up. This phenomenon is actually fairly general: if data are mapped into a space of sufﬁciently high dimension, then they will almost always be linearly separable—if you look at a set of points from enough directions, you’ll ﬁnd a way to make them line up. Here
    - Kernel Trick, Kernelization, Soft Margin:
        - https://remnote-user-data.s3.amazonaws.com/sUAMX0GBQFTmicXHnk3-U8yb8KSEYwBm8S_CO9qq0G_OlZMozYa-xooMJ7oPR6VaQNp37hjVzZd-zaFDCmwtfeScQA7HfrTI9K2tCdycPbbpDdIsPQc421mM7foL4PKK.pnghttps://remnote-user-data.s3.amazonaws.com/XI81mbvwX_xzxkpuV8HmOUpg3ASzF_8UPVMfqHsObbvsOv1PXNBZVmAJsT_r6hwh6TULVmTc9PtgmLx9c-MFVKcir8BxxjEFR2vtOILUhNED5WCtxofI72D6pfxU8N_l.pnghttps://remnote-user-data.s3.amazonaws.com/XI81mbvwX_xzxkpuV8HmOUpg3ASzF_8UPVMfqHsObbvsOv1PXNBZVmAJsT_r6hwh6TULVmTc9PtgmLx9c-MFVKcir8BxxjEFR2vtOILUhNED5WCtxofI72D6pfxU8N_l.pnghttps://remnote-user-data.s3.amazonaws.com/ft82VKkQispT5BDU3jyoVWl0BzMt_tJCApgAWW5Jfk-i_TCINU9vNFb8wKyivt_Kpb0wsC4hPgAdKnUZLHaGe1eB__kg2tjMViicfT1sS9eBXTsh9TRQcH2HEnPDAdWJ.pnghttps://remnote-user-data.s3.amazonaws.com/sUAMX0GBQFTmicXHnk3-U8yb8KSEYwBm8S_CO9qq0G_OlZMozYa-xooMJ7oPR6VaQNp37hjVzZd-zaFDCmwtfeScQA7HfrTI9K2tCdycPbbpDdIsPQc421mM7foL4PKK.png
        - 
    - 
    - Kernel Machine
    - Where kernel machine goes?
    - 
    - SVMs from CS50ai
    - 
- Ensemble Learning
    - So far we have looked at learning methods in which a single Hypothesis is used to make predictions. The idea of Ensemble Learning is to select a collection, or ensemble, of hypotheses, h_1, h_2, . . . , h_n, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses Base Models and their combination an Ensemble Model.So far we have looked at learning methods in which a single hypothesis is used to make predictions. The idea of ensemble learning is to select a collection, or ensemble, of hypotheses,Ensemble learning h1, h2, . . . , hn, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses base models and their combination anBase model ensemble model.
    - The two reasons for doing ensemble learning is: to reduce Bias (Underfitting) that any one model can introduce, and then also reduce Variance (Overfitting) as several models have to classify similarly the example.
        - https://remnote-user-data.s3.amazonaws.com/3mmPLro6-TwonkVYaPt2TEWne1gsm_JYYQVag-vVJ2-GDY1C1NgrecV4QNwYAtMG09llXW6HlJDL9fRHNbdbyiCUsgqnZcyhRla5vgoQOYaVv6s38OkF2yHD8ThqEJEH.pnghttps://remnote-user-data.s3.amazonaws.com/vq1DUT2X9vXZ7bXS96AajRmraWZpzAPzHmCbfd_DL5PTWeT_oho5WdZU4hdTAwsoJ84LLswdqCXbrNbysCnAaQr3ObjjsOLRRJNoSIZZDx-3h7g0ZS7UkFINct2FH2zO.pnghttps://remnote-user-data.s3.amazonaws.com/vq1DUT2X9vXZ7bXS96AajRmraWZpzAPzHmCbfd_DL5PTWeT_oho5WdZU4hdTAwsoJ84LLswdqCXbrNbysCnAaQr3ObjjsOLRRJNoSIZZDx-3h7g0ZS7UkFINct2FH2zO.pnghttps://remnote-user-data.s3.amazonaws.com/3mmPLro6-TwonkVYaPt2TEWne1gsm_JYYQVag-vVJ2-GDY1C1NgrecV4QNwYAtMG09llXW6HlJDL9fRHNbdbyiCUsgqnZcyhRla5vgoQOYaVv6s38OkF2yHD8ThqEJEH.png
        - https://remnote-user-data.s3.amazonaws.com/ft82VKkQispT5BDU3jyoVWl0BzMt_tJCApgAWW5Jfk-i_TCINU9vNFb8wKyivt_Kpb0wsC4hPgAdKnUZLHaGe1eB__kg2tjMViicfT1sS9eBXTsh9TRQcH2HEnPDAdWJ.pnghttps://remnote-user-data.s3.amazonaws.com/ft82VKkQispT5BDU3jyoVWl0BzMt_tJCApgAWW5Jfk-i_TCINU9vNFb8wKyivt_Kpb0wsC4hPgAdKnUZLHaGe1eB__kg2tjMViicfT1sS9eBXTsh9TRQcH2HEnPDAdWJ.png
        - 
    - 
    - Bagging ‒ training several models (same model Model Class) on different random training sets, and then combining them as an Ensemble Model. 
        - https://remnote-user-data.s3.amazonaws.com/XYiWDvxJjNBC37fgJdqSIhhiNVI4p_i_V3x6fI9Ph5k0BnP8jLBplzUcTbmqqFwv-fojrDVp46TzHVZ0-ifOlTaPfxi8UeOmMiQ7RmhCCKkrRufaVnw2auBTo8QFNE8l.pnghttps://remnote-user-data.s3.amazonaws.com/XYiWDvxJjNBC37fgJdqSIhhiNVI4p_i_V3x6fI9Ph5k0BnP8jLBplzUcTbmqqFwv-fojrDVp46TzHVZ0-ifOlTaPfxi8UeOmMiQ7RmhCCKkrRufaVnw2auBTo8QFNE8l.png
        - https://remnote-user-data.s3.amazonaws.com/i1WYuQSz9086n3z6rvx7wQOwRf_q1XEGE8kSye9pRxaWht0YJCNeiFPYfMk7yy3Ll3K5scwJUV7qUID4EH03czEBb_Vlp7g38B4WM8Zw369JagAbksQO0rQ723-npwnm.avif
        - 
    - Stacked Generalization/  Stacking ‒ training several different Model Classes on the same training set, and then combining them as an Ensemble Model. 
        - https://remnote-user-data.s3.amazonaws.com/GyT7dnZwivt37CLP3Jpf6DbCniv3y-5-0WGQrO96QVoTuQVQqs_JcqYl1zJBw_pyFdDbiMhUzlHZA7-t9m9nr8WNEaRMg9R1M3IBSUlICkYyDN2AeWXhpIjv1jp9avHY.pnghttps://remnote-user-data.s3.amazonaws.com/GyT7dnZwivt37CLP3Jpf6DbCniv3y-5-0WGQrO96QVoTuQVQqs_JcqYl1zJBw_pyFdDbiMhUzlHZA7-t9m9nr8WNEaRMg9R1M3IBSUlICkYyDN2AeWXhpIjv1jp9avHY.png
        - 
    - 
    - Random Forest
        - Random Forests ‒ a form of Decision Tree Bagging (Extremely Randomized Trees, Out-of-bag Error): 
            - https://remnote-user-data.s3.amazonaws.com/iX8x_0oL0OZZ2MoRt7p_AAvFjKI-77yDeOrGxlbTfe0G6ZZ89zvSBpFaHb5c3mYd5wTZWI28BJ6ZQoxEO7tabDjnJknmrorigoXzthpWuKSQVpYBRpmo7aeZTVl1jpAE.pnghttps://remnote-user-data.s3.amazonaws.com/ctuyLuwPHDoS3RT3Bp-tGK769trJthcphjT2j5O44XVUGrdjPCVOjeAhOeK4oTHztUZYG5wMcYl1fvdH_Y9qEfoy36vrDNtsu_dQzEV-3w8CjlUvp4Nb_GU0M6nl45db.pnghttps://remnote-user-data.s3.amazonaws.com/ctuyLuwPHDoS3RT3Bp-tGK769trJthcphjT2j5O44XVUGrdjPCVOjeAhOeK4oTHztUZYG5wMcYl1fvdH_Y9qEfoy36vrDNtsu_dQzEV-3w8CjlUvp4Nb_GU0M6nl45db.pnghttps://remnote-user-data.s3.amazonaws.com/iX8x_0oL0OZZ2MoRt7p_AAvFjKI-77yDeOrGxlbTfe0G6ZZ89zvSBpFaHb5c3mYd5wTZWI28BJ6ZQoxEO7tabDjnJknmrorigoXzthpWuKSQVpYBRpmo7aeZTVl1jpAE.png
            - 
        - 
    - Boosted Ensemble
        - Boosted Ensemble ‒ the most popular Ensemble Learning method. 
            - https://remnote-user-data.s3.amazonaws.com/sA5p900ZvGVJRoojIBlcCtkEAbqEBbJ-X_BsRrC24KG9qVNDhT6ozrktsJFypAsOGilOS1DQVzDeh8joly3lQPOKYG6AUkwXALmNYMX2V_eaHNRaH28c8BN-tUbNgdjm.pnghttps://remnote-user-data.s3.amazonaws.com/L6ghSG4o6ayraN0wTBuOOTIeQ3vSRwOxLqni5j0cBuFjuTm5tSD2urjvu4YNcqayk3MeVEC_q2GQqdiiZk-Or8giTPnGczQgHzcocmnTEbScddH8bL7ZhEOzvri233Wb.pnghttps://remnote-user-data.s3.amazonaws.com/L6ghSG4o6ayraN0wTBuOOTIeQ3vSRwOxLqni5j0cBuFjuTm5tSD2urjvu4YNcqayk3MeVEC_q2GQqdiiZk-Or8giTPnGczQgHzcocmnTEbScddH8bL7ZhEOzvri233Wb.pnghttps://remnote-user-data.s3.amazonaws.com/sA5p900ZvGVJRoojIBlcCtkEAbqEBbJ-X_BsRrC24KG9qVNDhT6ozrktsJFypAsOGilOS1DQVzDeh8joly3lQPOKYG6AUkwXALmNYMX2V_eaHNRaH28c8BN-tUbNgdjm.png
            - https://remnote-user-data.s3.amazonaws.com/SRoiVihsDqqlgkJC9FTdq3N-W16wNnW9ACjdtC0HQHQ3gIp0bLG0j7Ge4iUwIBoRC8MPtkmbLeJT2_xGhXRCJwGc3Xxp_7kJjBtnFZppng6_gag9eUL83R8OnM1pcoOg.pnghttps://remnote-user-data.s3.amazonaws.com/SRoiVihsDqqlgkJC9FTdq3N-W16wNnW9ACjdtC0HQHQ3gIp0bLG0j7Ge4iUwIBoRC8MPtkmbLeJT2_xGhXRCJwGc3Xxp_7kJjBtnFZppng6_gag9eUL83R8OnM1pcoOg.png
            - 
        - It uses Weighted Training Set, where each example has an associated weight with it.  
        - We make the first hypothesis where all training examples has the same weights. We then give bigger weights to misclassified examples. We repeat this loop k times.
        - The final ensemble let's each Hypothesis Function vote as in Bagging, but each hypothesis also has a weight based on how well it did on their corresponding weighted training set. 
        - 
        - AdaBoost, Weak Learning, Decision Stump (...Bayesian Learning):
            - https://remnote-user-data.s3.amazonaws.com/uOukgfKysRxp6CdmIE1o1-OJB65ArfD-VMjNJySvJoukDhGSmv3Vq0Zh9CKPzY3wJTL3ymPBXOoVrFWQeOvzB0UhqcE_lW_FGX7GpP7uJBqsOX2iYcOT0FuxhMRRK10V.pnghttps://remnote-user-data.s3.amazonaws.com/wkmnfGZYCSne-SyPNhOPvca4APgm7oLrFsekqzoV4F5zAx4TO7WMJlk3DLLgZx6AKbi70QWWaS7xbeT6WiNix0oxjlAVh_5NVwF6v6rvSLx9K3En5GlPrisU0UpT5Tcf.pnghttps://remnote-user-data.s3.amazonaws.com/wkmnfGZYCSne-SyPNhOPvca4APgm7oLrFsekqzoV4F5zAx4TO7WMJlk3DLLgZx6AKbi70QWWaS7xbeT6WiNix0oxjlAVh_5NVwF6v6rvSLx9K3En5GlPrisU0UpT5Tcf.pnghttps://remnote-user-data.s3.amazonaws.com/uOukgfKysRxp6CdmIE1o1-OJB65ArfD-VMjNJySvJoukDhGSmv3Vq0Zh9CKPzY3wJTL3ymPBXOoVrFWQeOvzB0UhqcE_lW_FGX7GpP7uJBqsOX2iYcOT0FuxhMRRK10V.png
            - https://remnote-user-data.s3.amazonaws.com/uJZ0AFPGyTibG-3qpIOBuaCohTuZJQYbRTUUVMOCCb5djGrJe5wx_De_n2EgLa685HVv7fvCviiLs3JKPFiPkLgySmlaT-0trJKwTveYROZLpVAF4A0F1zRzafL6RQsk.pnghttps://remnote-user-data.s3.amazonaws.com/uJZ0AFPGyTibG-3qpIOBuaCohTuZJQYbRTUUVMOCCb5djGrJe5wx_De_n2EgLa685HVv7fvCviiLs3JKPFiPkLgySmlaT-0trJKwTveYROZLpVAF4A0F1zRzafL6RQsk.png
            - 
        - 
    - Gradient Boosting
        - Gradient Boosting ‒ using Gradient Descent for Boosting: we generate new hypothesis functions, but instead of paying attention to previously got wrong examples, we pay attention to the Gradient between the right answer and the answers given by previous hypotheses.
        - As with any algorithm using Gradient Descent we have a Loss Function to optimize for.
        - We then build a Decision Tree, similar to AdaBoost.
        - Previously we have used Gradient Descent to iteratively change model's parameters so that they minimize Loss Function. 
        - For Gradient Boosting we use Gradient Descent to change parameters of the next decision tree, not the current one. And we similarly do it in the direction of minimizing Loss Function. 
            - https://remnote-user-data.s3.amazonaws.com/hkL180fRxuE6heHc9-sEF1ffvZcLfHkMO4jGqV3XWAbf5KYdIbJrtUukMrivpFHRZFCie0KtX0jQushXeboH2sKwCLmWf_tM51ybb2PISNt5Vkod7mSThllruKAfnaBI.pnghttps://remnote-user-data.s3.amazonaws.com/7ZQSavpCLdTGZf9nsR3bYAOFoJtqcqW5wubK8pSTR7Aky-ck-1ZPWuu_1Af3aQV-dC0HEujoOkKSWwbmX7L9SMzGiRmYSYpw2lp9b0rW77P16rhTgivgPVaP0QlcNUlY.pnghttps://remnote-user-data.s3.amazonaws.com/7ZQSavpCLdTGZf9nsR3bYAOFoJtqcqW5wubK8pSTR7Aky-ck-1ZPWuu_1Af3aQV-dC0HEujoOkKSWwbmX7L9SMzGiRmYSYpw2lp9b0rW77P16rhTgivgPVaP0QlcNUlY.pnghttps://remnote-user-data.s3.amazonaws.com/hkL180fRxuE6heHc9-sEF1ffvZcLfHkMO4jGqV3XWAbf5KYdIbJrtUukMrivpFHRZFCie0KtX0jQushXeboH2sKwCLmWf_tM51ybb2PISNt5Vkod7mSThllruKAfnaBI.png
            - 
        - 
        - XGBoost is a popular implementation of Gradient Boosting that balances pruning, Regularization, computational efficiency (Computational Complexity), careful memory organization to avoid cache misses, and allowing parallel computation.
            - https://remnote-user-data.s3.amazonaws.com/Xkj1F0AghBgvYhu41mpg_NToKkGUQAqgeWme-NhzaiuuYLm2pw99ze9Ce7_k93tL3gUDw7osrGb3Bp1rPI-wujP9pY7T57dF7oNYvYQajzgr3wDp1bTn9GIkeG6koTaU.pnghttps://remnote-user-data.s3.amazonaws.com/Xkj1F0AghBgvYhu41mpg_NToKkGUQAqgeWme-NhzaiuuYLm2pw99ze9Ce7_k93tL3gUDw7osrGb3Bp1rPI-wujP9pY7T57dF7oNYvYQajzgr3wDp1bTn9GIkeG6koTaU.png
            - 
        - XGBoost
            - 
        - 
    - 

## Linear Functions

- Linear Function
    - Fundamentals
        - Linear Function take continuous-valued inputs. Both can be regression and classification. It poses significant constraints to the Hypothesis Space \mathcal{H}.
            - https://remnote-user-data.s3.amazonaws.com/8nd7uGUzwnZ72gn3GXVmnpE83U1w_xSYLV8gUih3gajXq1E4wG3hvOc2ACpgtA4_NywUP9JbcF9BvXBfSmkBnBSijiFU58pJf966tDdeK30rE43RY8rneYRzIOHm1MqZ.pnghttps://remnote-user-data.s3.amazonaws.com/8nd7uGUzwnZ72gn3GXVmnpE83U1w_xSYLV8gUih3gajXq1E4wG3hvOc2ACpgtA4_NywUP9JbcF9BvXBfSmkBnBSijiFU58pJf966tDdeK30rE43RY8rneYRzIOHm1MqZ.png
            - 
        - h_w(x) = w_1x+w_0
        - Finding h_w that best fits data is called Linear Regression. To fit a line we have to find \langle w_0, w_1 \rangle that minimize the Empirical Loss.
            - (Angle braces are used to represent a Vector, or a Tuple of values.)
            - https://remnote-user-data.s3.amazonaws.com/TPbHhMhXc-yJ2AmNr7sLiRWrSLvYEC7fER_3gdg4AhwB3rfW7PdQ7eDf-ISaeUvR3uAWmi_3df43Z2iUiG1ySAp3pmSVOQFxRnJKLThIXOmcLJlf5Cbv_5eA5jyv7alZ.pnghttps://remnote-user-data.s3.amazonaws.com/JkWIucRH4ERZiDb2Xtxxis1A-xweZELtsrxyUBJWKvwcVTYcW4mGsmZfOPPhWtUoYBDG3R0hX7IR35cHCQ0EKqRyWXbke2ZYakYJxXrKR7HRjJBJqkNulxEnHntOkTph.pnghttps://remnote-user-data.s3.amazonaws.com/JkWIucRH4ERZiDb2Xtxxis1A-xweZELtsrxyUBJWKvwcVTYcW4mGsmZfOPPhWtUoYBDG3R0hX7IR35cHCQ0EKqRyWXbke2ZYakYJxXrKR7HRjJBJqkNulxEnHntOkTph.pnghttps://remnote-user-data.s3.amazonaws.com/TPbHhMhXc-yJ2AmNr7sLiRWrSLvYEC7fER_3gdg4AhwB3rfW7PdQ7eDf-ISaeUvR3uAWmi_3df43Z2iUiG1ySAp3pmSVOQFxRnJKLThIXOmcLJlf5Cbv_5eA5jyv7alZ.png
            - https://remnote-user-data.s3.amazonaws.com/vcnehH7QGvcdql6qbsnBn9K8nmmtY0i7RHwBUV9o6sT5L7GzmlIJPK4EPPci97bWkdL5a5jitqQLsdvmWxvBw3j7fKh2rJ7lE52JZwS_Xs1AJiLC6YS0w8c8n9VClJ5U.pnghttps://remnote-user-data.s3.amazonaws.com/vcnehH7QGvcdql6qbsnBn9K8nmmtY0i7RHwBUV9o6sT5L7GzmlIJPK4EPPci97bWkdL5a5jitqQLsdvmWxvBw3j7fKh2rJ7lE52JZwS_Xs1AJiLC6YS0w8c8n9VClJ5U.png
            - 
            - 
        - Weight Space ‒ the space defined by all possible values of Weights.
            - https://remnote-user-data.s3.amazonaws.com/EGFMDjIPs_4X1taqg1ahsyuwfdalAERBOozHwCfv1spyP8g8EcR8pZgoYoiSCWpTtG-Z0CyX_7oZoKwN55oikPGo5JHOZovUobqz4gmdDZ43ZFt-Fgy235wIswe9Npnh.pnghttps://remnote-user-data.s3.amazonaws.com/vcnehH7QGvcdql6qbsnBn9K8nmmtY0i7RHwBUV9o6sT5L7GzmlIJPK4EPPci97bWkdL5a5jitqQLsdvmWxvBw3j7fKh2rJ7lE52JZwS_Xs1AJiLC6YS0w8c8n9VClJ5U.pnghttps://remnote-user-data.s3.amazonaws.com/vcnehH7QGvcdql6qbsnBn9K8nmmtY0i7RHwBUV9o6sT5L7GzmlIJPK4EPPci97bWkdL5a5jitqQLsdvmWxvBw3j7fKh2rJ7lE52JZwS_Xs1AJiLC6YS0w8c8n9VClJ5U.pnghttps://remnote-user-data.s3.amazonaws.com/EGFMDjIPs_4X1taqg1ahsyuwfdalAERBOozHwCfv1spyP8g8EcR8pZgoYoiSCWpTtG-Z0CyX_7oZoKwN55oikPGo5JHOZovUobqz4gmdDZ43ZFt-Fgy235wIswe9Npnh.png
            - 
        - 
    - Univariate Linear Function
        - 
    - Multivariable Linear Function
        - Overfitting is a real problem for Multivariable Linear Function as in multidimensional Weight Space some trough might appear useful, when in fact it is just overfitting. Hence, to combat that we use Regularization.
            - https://remnote-user-data.s3.amazonaws.com/dbX-Q-w0oYeuyUiC1x6McKn21SsFs8aQWggpksognkOuPRZaIQvnVlQhzXeoB8nnL99PariwTEBlL1vJicRASLB2tZMtSHKNGblRXHmjVTD7VmUjQcMbv92C-BOQdJ6a.pnghttps://remnote-user-data.s3.amazonaws.com/dbX-Q-w0oYeuyUiC1x6McKn21SsFs8aQWggpksognkOuPRZaIQvnVlQhzXeoB8nnL99PariwTEBlL1vJicRASLB2tZMtSHKNGblRXHmjVTD7VmUjQcMbv92C-BOQdJ6a.png
            - 
        - L_1, L_2, Loss Function; Sparse Model ‒ model where many weights are zero, i.e. irrelevant.
            - https://remnote-user-data.s3.amazonaws.com/EToaRKa_LJMLWP9MC_GuiV4yzzT54BEoQ5YfG_ZPnng68V1PAD66CJy1MBliAFHjzSs6OB7hfwRTJS9MydPGIoC-451QCkfsUfL3zk8PMhNm2FNSYFJtgkIOFD83IdFi.pnghttps://remnote-user-data.s3.amazonaws.com/8aaJk9ijhTs0kJtmVko0qgCP2xDKCyKRX-dfqSmR8gOjfrGICsSTRr2YT6n224yZNZTHsWl0plON6mSDxSdVGBOs83JtKb0caoV9ngFooXOi3WE_g75qnb4NKzP-Lv1G.pnghttps://remnote-user-data.s3.amazonaws.com/BYwexT0fWC4IIjr9dwBQVTwQm8fojQaFD0Bq3F9gFK9aHCYTyKgmLQEZdzOP8bUMNkNwjaW1kVsvvRE8ho3cULroyyxhgPQEYOA9clcGgd95M_S4kmKh1J6O1tv7uD3k.pnghttps://remnote-user-data.s3.amazonaws.com/BYwexT0fWC4IIjr9dwBQVTwQm8fojQaFD0Bq3F9gFK9aHCYTyKgmLQEZdzOP8bUMNkNwjaW1kVsvvRE8ho3cULroyyxhgPQEYOA9clcGgd95M_S4kmKh1J6O1tv7uD3k.pnghttps://remnote-user-data.s3.amazonaws.com/8aaJk9ijhTs0kJtmVko0qgCP2xDKCyKRX-dfqSmR8gOjfrGICsSTRr2YT6n224yZNZTHsWl0plON6mSDxSdVGBOs83JtKb0caoV9ngFooXOi3WE_g75qnb4NKzP-Lv1G.pnghttps://remnote-user-data.s3.amazonaws.com/EToaRKa_LJMLWP9MC_GuiV4yzzT54BEoQ5YfG_ZPnng68V1PAD66CJy1MBliAFHjzSs6OB7hfwRTJS9MydPGIoC-451QCkfsUfL3zk8PMhNm2FNSYFJtgkIOFD83IdFi.png
            - 
            - First on L_1, L_2
            - 
        - 
        - Normal Equation/ Closed-Form Equation, using Data Matrix and Pseudoinverse:
            - https://remnote-user-data.s3.amazonaws.com/O96gFyH_D0wqog53do2AzdYP_ikW3bLz55LERg2yi9rU96IeTvClzQWCRIbV-bRPy2C5znC-GV2pfIUzV957IbFaSzeU7oCx4NriNH1BZTGotI7ppQmT-Qqzc0xvrTTc.pnghttps://remnote-user-data.s3.amazonaws.com/K3oD8GFWqtCN1mP2GhZ3AhrDgi7xmAVGEVJKqPpaAm_ec88KdS0eb2lOHe2vo1tsVICmcD5KpGl65bahDCkm3pqwWrBJF2pfBQQufyloZ0rHLJkKiAckcQi0p-5TfYxS.pnghttps://remnote-user-data.s3.amazonaws.com/K3oD8GFWqtCN1mP2GhZ3AhrDgi7xmAVGEVJKqPpaAm_ec88KdS0eb2lOHe2vo1tsVICmcD5KpGl65bahDCkm3pqwWrBJF2pfBQQufyloZ0rHLJkKiAckcQi0p-5TfYxS.png
            - 
        - Example from Supervised Machine Learning: Regression and Classification
            - https://remnote-user-data.s3.amazonaws.com/kkQuD4kTWdbpSZOG7R2eDccCDnTtFKdzs4I-8yrwJ1eEftugmmnsopUaRbjCliYM63I2AuxhO_wTHegRnu8N6NR_sTAEuMZwxorv2AW8Cv-FBzBKIyTx3SBQsMgJ8D7d.png
            - https://remnote-user-data.s3.amazonaws.com/qSzdZ6GUa4TZWORCuNHSmEQQmFrq75Qznx9byKTDTyyQcHPsQv0O1P9xdbUrFHUeOwStLT0tazKSH7qIQnU1ZrDqdz_GTJua5MdEHOFDjO8SJKKwZWjy2EA5YnPEaX4d.png
        - 
    - Classifier Linear Function
        - Linear Function can be used for Classification. Decision Boundary is a line that separates two classes. Linear Decision Boundary is called Linear Separator. Data that can be separated like that are called Linearly Separable.
            - https://remnote-user-data.s3.amazonaws.com/xV8hFFh_inLFJP_ygD513rbiIRmHGTDPckDJC8VCM1mc0er0I5ssYvvRd4VTPF2XWqfn71GZRgQi8bHvo3Xwzu8TSl0CMMqKY6ZWUulDxwsqrvOHQzaNUnajtgtKGAs8.pnghttps://remnote-user-data.s3.amazonaws.com/xV8hFFh_inLFJP_ygD513rbiIRmHGTDPckDJC8VCM1mc0er0I5ssYvvRd4VTPF2XWqfn71GZRgQi8bHvo3Xwzu8TSl0CMMqKY6ZWUulDxwsqrvOHQzaNUnajtgtKGAs8.png
            - https://remnote-user-data.s3.amazonaws.com/4LLscWM_U8dfbN1MRODcFGsheXDzg3DkGr95eM1y-ITfpNwp3i1POhklctucShm1CxrdNx0-aBQB9AhiEqbUu_e8ecr_WniQixDR_qyaXnxlEKFZicQpS1AHVeYE83Fa.pnghttps://remnote-user-data.s3.amazonaws.com/X9ogbfCFai1uTkLnXIcRS_RtoKGHIV44ZdDVy3Lm2g_-c5OOpKtlq8C4B1NH6EHywuI6SDfDcgCEe-jjFQeYtWUCCttYGP3yhds6uiHmucRgqFB7fDDQ3HzhGWriTm6M.pnghttps://remnote-user-data.s3.amazonaws.com/-8c4Y_MRr9nH9VWEbS8FL0IRHn3brBYoTEvK6XIWJ-k12ykZpQBwGv6DyL-3MTVUXd2bHc-clj36SxT75-Z67e_gbB2-nR6mfQNvcBp0XXQ_8A2PR_e00FHzHne-JC96.pnghttps://remnote-user-data.s3.amazonaws.com/4LLscWM_U8dfbN1MRODcFGsheXDzg3DkGr95eM1y-ITfpNwp3i1POhklctucShm1CxrdNx0-aBQB9AhiEqbUu_e8ecr_WniQixDR_qyaXnxlEKFZicQpS1AHVeYE83Fa.png
            - 
        - Alternatively, we can think of passing Classifier Linear Function through the Threshold Function.
            - https://remnote-user-data.s3.amazonaws.com/vnIYj_AzvxA5I5pEv2uRXJAVRYbFgDItMKSr4eKNFKFjoKN3xola4Bq7H4ux2FKrgKhfNtg_qK73HDsXr4JgJ8nhi_yJZYcU9qXHbMQVATgYiNuFu-yESLthjRS2fcxz.pnghttps://remnote-user-data.s3.amazonaws.com/DSMQ9qQtKSlg0o6Bof-zxeW7MKPbNeXBrYBtp1i_P-z9aW3yvXs1q4AXfN0EUFGjMjnWppgFhxlsgST_mSVoMkOi19BG3YTzK55T1m93z-364Zye4DObIlY4jR_ZScrf.pnghttps://remnote-user-data.s3.amazonaws.com/DSMQ9qQtKSlg0o6Bof-zxeW7MKPbNeXBrYBtp1i_P-z9aW3yvXs1q4AXfN0EUFGjMjnWppgFhxlsgST_mSVoMkOi19BG3YTzK55T1m93z-364Zye4DObIlY4jR_ZScrf.pnghttps://remnote-user-data.s3.amazonaws.com/vnIYj_AzvxA5I5pEv2uRXJAVRYbFgDItMKSr4eKNFKFjoKN3xola4Bq7H4ux2FKrgKhfNtg_qK73HDsXr4JgJ8nhi_yJZYcU9qXHbMQVATgYiNuFu-yESLthjRS2fcxz.png
            - 
        - To find appropriate weights interestingly we cannot use Gradient Descent, as nearly everywhere in this binary Classification Gradient will be zero.
            - https://remnote-user-data.s3.amazonaws.com/kxmjGi-gADrwNE5PamYpzS6wz8F3WiLsH-jvfW_Zyq-wHmUzbAucI7DeqQrpN8oqrIo-oU_LbbBYS_ChMGZm6tJeZigCwQfP3XlQRJKVCHShKuhkZKHpGGPeCIZhDwNV.pnghttps://remnote-user-data.s3.amazonaws.com/kxmjGi-gADrwNE5PamYpzS6wz8F3WiLsH-jvfW_Zyq-wHmUzbAucI7DeqQrpN8oqrIo-oU_LbbBYS_ChMGZm6tJeZigCwQfP3XlQRJKVCHShKuhkZKHpGGPeCIZhDwNV.png
            - 
        - Perceptron Learning Rule is how we still manage to find appropriate Weights that minimize Loss Function (also see its Training Curve):
            - https://remnote-user-data.s3.amazonaws.com/_S-WDV4eJ15wR70Z9Jg4vkQtnDSaEPl6XsdpEITxkbRE2EEWSPIUlHbSpj4Ir2cUzg11Qr2T0tjTy1dpvLE8Zqwvaqy4C1CJpP3OjUyctqnC4T6ZO5NMitJJBCHjz8bo.pnghttps://remnote-user-data.s3.amazonaws.com/_S-WDV4eJ15wR70Z9Jg4vkQtnDSaEPl6XsdpEITxkbRE2EEWSPIUlHbSpj4Ir2cUzg11Qr2T0tjTy1dpvLE8Zqwvaqy4C1CJpP3OjUyctqnC4T6ZO5NMitJJBCHjz8bo.png
            - https://remnote-user-data.s3.amazonaws.com/A-YFs5dbPc0pGRHZcyDaMrXnF_eKZwrXnCJj94hknkdwXkMnTVOMBeoIq6F6Rp72T1L46JRjoWv-5ATzZScvqHVnKQkHgC9IqLJ34UlSCAmt5ZCyBuQKSfulkfLYmn5s.pnghttps://remnote-user-data.s3.amazonaws.com/A-YFs5dbPc0pGRHZcyDaMrXnF_eKZwrXnCJj94hknkdwXkMnTVOMBeoIq6F6Rp72T1L46JRjoWv-5ATzZScvqHVnKQkHgC9IqLJ34UlSCAmt5ZCyBuQKSfulkfLYmn5s.png
            - https://remnote-user-data.s3.amazonaws.com/wOpoSB7GagE0gM2PxWfNubW_apIdgOXfqzPul3UXAVw6KqIK1ZLawABe7OxeC3p2HtfnEKACe_ZIhzJpbYJKoLKBEMjgwUr1gigk7W-W_dJ9VIF7SjoREGGQ3BFZT67R.pnghttps://remnote-user-data.s3.amazonaws.com/wOpoSB7GagE0gM2PxWfNubW_apIdgOXfqzPul3UXAVw6KqIK1ZLawABe7OxeC3p2HtfnEKACe_ZIhzJpbYJKoLKBEMjgwUr1gigk7W-W_dJ9VIF7SjoREGGQ3BFZT67R.png
            - 
        - 
        - Classifier Linear Function with Logistic Regression
        - For the problems mentioned above with hard Threshold Function, we can soften it with a continuous, differentiable function: Logistic Regression:
            - https://remnote-user-data.s3.amazonaws.com/0HopaMBS3FimSh71VDAbgCNRfiHHYE_asNnFMa4e63DCsxlvit3lb0IIFPAHGK5T5ifD4dcuYieD9MuS0vBg3x5Ek1IMfiJLNDQl-lXMmOn04vPy4XRhGCrmwMjMBeSM.pnghttps://remnote-user-data.s3.amazonaws.com/DSMQ9qQtKSlg0o6Bof-zxeW7MKPbNeXBrYBtp1i_P-z9aW3yvXs1q4AXfN0EUFGjMjnWppgFhxlsgST_mSVoMkOi19BG3YTzK55T1m93z-364Zye4DObIlY4jR_ZScrf.pnghttps://remnote-user-data.s3.amazonaws.com/MTlA7nb6EmaZrHLE4OukyHdjA9azh9pPfGccPoQGJdMoCcOwQEpOs_yjPldmFdrllikSNmXVfcfTKHMMT47RyxL3lkF0q1NpvlVZ3pz0xPKj4R5Pjm56kYejX5i7Rd7-.pnghttps://remnote-user-data.s3.amazonaws.com/MTlA7nb6EmaZrHLE4OukyHdjA9azh9pPfGccPoQGJdMoCcOwQEpOs_yjPldmFdrllikSNmXVfcfTKHMMT47RyxL3lkF0q1NpvlVZ3pz0xPKj4R5Pjm56kYejX5i7Rd7-.pnghttps://remnote-user-data.s3.amazonaws.com/DSMQ9qQtKSlg0o6Bof-zxeW7MKPbNeXBrYBtp1i_P-z9aW3yvXs1q4AXfN0EUFGjMjnWppgFhxlsgST_mSVoMkOi19BG3YTzK55T1m93z-364Zye4DObIlY4jR_ZScrf.pnghttps://remnote-user-data.s3.amazonaws.com/0HopaMBS3FimSh71VDAbgCNRfiHHYE_asNnFMa4e63DCsxlvit3lb0IIFPAHGK5T5ifD4dcuYieD9MuS0vBg3x5Ek1IMfiJLNDQl-lXMmOn04vPy4XRhGCrmwMjMBeSM.png
            - https://remnote-user-data.s3.amazonaws.com/urwJyHRujtnytC7qznT9Z3TWZYaQYn8GM9q2kINBoZEaREIVdMST9mNkjHiwW4BfZNt32Sz2qRBSeP3JHOtqTaf7iCPpFGhix9CixSORnWGVyhFp8Fvl4PTIg9FakNzT.pnghttps://remnote-user-data.s3.amazonaws.com/urwJyHRujtnytC7qznT9Z3TWZYaQYn8GM9q2kINBoZEaREIVdMST9mNkjHiwW4BfZNt32Sz2qRBSeP3JHOtqTaf7iCPpFGhix9CixSORnWGVyhFp8Fvl4PTIg9FakNzT.png
            - https://remnote-user-data.s3.amazonaws.com/0aGhA93iJkZwo4oZ7xo6VVenh5ZqCn_yopXVuQnrW8M2NQ6My07SrJzpNHQGIzomkCIllfJ8kzs6CdYy3tqfNK5zO96nSUs4YwqbaxMf7ft1dmFj9HNAfBGRxp9WC3w6.pnghttps://remnote-user-data.s3.amazonaws.com/0aGhA93iJkZwo4oZ7xo6VVenh5ZqCn_yopXVuQnrW8M2NQ6My07SrJzpNHQGIzomkCIllfJ8kzs6CdYy3tqfNK5zO96nSUs4YwqbaxMf7ft1dmFj9HNAfBGRxp9WC3w6.png
            - 
        - 
    - 


