# Goal

This is the final practice of explaining supervised and unsupervised learning before I study other topics like ai safety, deep learning, rl and so on.

# Debrief

# Explaining

Basically ML is can be divided into three parts based on the feedback we give to models. Supervised when we have input-output pairs, unsupervised when we only have inputs, and RL which we won't talk about now.

## Supervised Learning

### Linear Functions
Simplest supervised learning, fitting a line onto data, best place to talk about how almost all ml models get trained:

1. Make a guess based on randomly generated weights for all the training instances (if batch gradient descent).
2. Calculate feedback, i.e. how much mistakes have we made i.e. empirical loss, via loss-function, say mean squared errors.
3. Calculate how to update weights by plugging in empirical loss onto gradient descent that will calculate the derivative.
4. Repeat until derivative is zero meaning any change to weights would make things worse.

Reguralize by adding penalty to loss function. Either L1 lasso regression with mean of absolute weights that drives some weights to zero and acts frantically when features are correlated. Or l2 ridge regression with mean squared weights. Or elastic net which adds both.

â“ Wait, was it mean or sum? not that it matters, right? => It is sum, but fundamentally it doesn't matter. ðŸŸ¢

For classification can use hard threshold that prevents using gradient descent, so we use perceptron learning rule. 
Or logistic regression if you want probabilistic output.
For multiclassificatoin you can use multinomial logistic regression, or softmax that uses argmax at the end to pick the most likely class. Softmax uses cross entropy as loss function that calculates the minimum number of bits needed to transmit what class example is over the whole training set. The higher probability, say 90% vs 30% the less bits are needed to transmit that example is a class. You apply that over the training set and you have your most optimal probabilities that, ideally, match the real probability distribution.

ðŸŸ¢ => good recall.

### KNN
Find k-number of closest neighbors to an instance based on distance that is calculated depending on the feature types. Works well with a few 2-10 features because of curse of dimensionality. Works well for big datasets as you forego expensive upfront model training, and online learning as adding/removing instances is easy. (Instance based model obviously.)

You calculate how distance is calculated, whether when deciding the class or calculating regression by averaging target values of neighbors distance between neighbors and example is used to weight neighbors, and number of neighbors to consider.

You can do probabilistic classification by calculating the proportion of neighbors, i guess you can even using weighting with it.

ðŸŸ¢ => good recall

### SVMs
SVM's are trying to fit the widest linear street possible between two classes and put decision boundary in its middle. Computational learning theory actually supports that wider street would lead to better generalization.

It does so by taking the closest two points between the classes, called support vectors and using them to fit the street. Hard margin classification is when no exceptions allowed of instances either on the street/ margins or on the wrong side of decision boundary. Soft margin classification is when we balance some exceptions and fitting as wide street as possible.

If original data is not linearly separable we need to use curse of dimensionality to our advantage by adding features. We can use polynomial features (basic transformations of our features x^2...) and use that to add dimensions. But that makes our model bloated to use, store, and train. Kernel trick does the same job of kind of adding new features and finding a linear separator within it, but without actually adding the features so our model stays lean. 

â“Probabilistic classification? ðŸŸ¡ => Should've remembered that we can train logistic model on SVM's values to give probabilities. 
â“Regularization? ðŸŸ¡ => Hyperparameter C that control street's width. Should've thought that this would correspond as regularization. 

Regression using svm is about doing the reverse of the task, you're trying to fit as narrow street as possible and get in as many points as possible.

If line is no straight we use once again the kernel trick.

ðŸŸ¢ => good recall.

### Decision Trees
CART is the most popular algorithm. It is a greedy binary decision tree algorithm that searches for every feature and every possible threshold the most pure split. Purity is how much agreement within the split groups there are on the target values. Can be calculated with entropy (how many bits would be needed to explain what class/ target value is in the group), or gini coefficient. Does so recursively until reaching stopping criteria.

Very powerful model, likely to overfit so you have to control with hyperparameters. You can use for all kinds of things, from classification, to regression, to probabilistic regression.

ðŸŸ¢ => good recall.

### Ensemble Learning
Training several models at once to improve predictions. Works best with powerful models that are prone to overfitting, as putting models together prevents that. Collection of models doesn't reduce bias/ underfitting so weak models, say linear functions,won't work as well in an ensemble.

You can't train the same models on the same training data as they will just do the same predictions. So you need to introduce diversity somewhere. There are several possible places, either the data you give to models, or training different models on the same data, or training same models, on the same data, but models have randomness in training.

Let's first talk about same models trained on different data. Basically we can randomly sample from the training set some part for each base model. This is called bagging ensemble when we do it with replacing, i.e. putting back training instance before pulling a new one. And pasting ensemble when done without replacement. Mathematically, 37% of those instances won't be pulled for every base model. So instead of using validation set, we don't split it and use those out of bag instances as validation set. Out of bag score is just validation score but calculated with those out of bag instances for every base models and then taking the average for the whole ensemble.

Stacking ensemble is when we take very different models, knn/ decision trees/ svm's and train them on the same data. Then we take their predictions and create another model that aggregates those predictions and gives the final prediction, i.e. the voting model.

Other voting method is hard voting when majority wins, and soft voting when weights/ probabilities/ confidence with which models make prediction is taken into consideration.

Then there are boosting ensembles which are about somehow training new models accounting for the mistakes of the past models. There are two main ways. AdaBoost which starts with equal weights for all training instances, and then allocates more weights to instances that have been missclassified, not to make it too prone for overfitting we control impact of ending models with shrinkage, i.e. learning rate. XGBoost are decision trees that train on past base model's residual errors. The first tree is trained as always, all the next one on the past trees' residual errors. This is one of the most powerful ML model, and it usually scores the highest on most of the tasks.

Lastly random forests which is a big collection of decision trees that randomly pick sqrt(n) features when deciding on the split. Extra-trees/ extra random trees is when decision threshold is also picked in random. 

ðŸŸ¢ => good recall.

## Unsupervised Learning

### Dimensionality Reduction
Projection -- straight axis vs manifold learning -- unrolling, unfolding.

#### PCA
Recursive search of straight axis that explain the most variance, and search in orthogonal spaces not to double explain some variance.

#### Random Projection
Randomly generating the axis based on number of features and epsilon dilution between any two points we're willing to accept.

#### LLE
Local linear embeddings. For every instance calculating its knn, say 10 neighbors, and using them in a linear equation to recreate the instance. Sum(w_0*x_0+w_1*x_1...) Then putting all those weights into a weight matrix. Now you have local relationships of all points. And now you take this W matrix and search for lower dimensional representation Z to keep this relationship between points. 

Doesn't scale as too expensive, preserves local relationships, not universal between points. Manifold learning, so works best for unrolling.

### Clustering
#### K-means Clustering
Randomly generating centroid location and calculating total inertia, then moving centroids until converging. Can find local optima so we have to do several runs. Works for spherical data, otherwise suboptimal.

You can use probabilistic k-means clustering where each instance receive the probability distribution of being in different clusters. This can be used as very effective dimensionality reduction if you use those probability distributions as features instead.

#### DBSCAN
For every instance find the number of neighbors it has within some e space. Every instance that passes the min_num of neighbors is a core insatnce, instances within space of a core instance must be of the same cluster. If instance is in none of the classes it is an anomaly.

#### Gaussian Mixture Models
Assuming data was generated by several probability distributions, and your goal is to find their mean, shape, size etc. Works best for ellipsoidal data, as this is how gaussian distributions look like. K-means works best for spherical clusters.

Similarly to k-means randomly generates some probability means and iterates on them until finding optimal solution.

### Anomaly Detection
Basically just gaussian mixture models for anomaly detection where you set the decision threshold based on % of anomalies you'd like to have. Say know you have around 2% of anomalies in your data, then you set this value to get around 2% of anomalies to get detected.


ðŸŸ¢ => all good with unsupervised learning. And this concludes our ML fundamentals ! May do 1-2 L1 projects just to show I can do it, but next is either ai safety, deep learning, or rl. See you there cowboy!
