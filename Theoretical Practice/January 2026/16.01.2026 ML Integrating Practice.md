# Goal
The goal is to continue with supervised learning integration. Just finished with understanding SVM's want to practice recalling right away.

# Debrief
1. 

### Questions
1. 

### Answers
1. 

# Practice

### SVMs
SVM is another supervised learning algorithm. It's idea is to fit as wide separator between classes as possible. Think about two separate clouds of examples which differ in class. We're trying to find the closest training examples between the two and then fit as wide street as possible. Street is linear. The separator between the classes is in the middle. Making street wider is also supported by computational learning theory that it should generalize better. 

Hard margin classification is when we don't accept any violations of having examples on the street or classes on the wrong side.
Soft margin is when we accept some and try to balance exceptions with wide street.
We can even output probabilities for classification by training logistic function on top of SVM classifier that would take classifier values and transform them to probabilities.

But not all datasets are linear separable originally. To still use SVMs and find linear separator we use curse of dimensionality to our advantage! If we add enough dimensions eventually there will be a linear separator between the classes. 

The problem is that adding more polynomial features (basic transformations of our existing features, say x^2 or sqrt(x)) leads to bunch of problems with training, storing, and using the model -- it becomes too clunky. So to fix that we use kernel trick which achieves the same result of kind of adding polynomial features and finding separator in higher dimensional space, without actually adding them so model stays "lean".

1. First basic approach is adding polynomial features. It works as we eventually find linear separator in higher-d space, but model becomes to clunky.
2. So second approach is kernelized polynomial features.
3. Third approach is use similarity function, which is about defining some landmark, and calculating how far training instances are from it as a new feature. How to define good landmark? Popular approach is to use for each training instance a landmark. But then we have m examples dataset with m features (if you drop n original features). Again clunkiness problem.
4. So forth approach is again to kernalize similarity function approach. Specifically it is called Gaussian RBF Kernel. 

All right, this is tackling classification of nonlinearly separable datasets, which fundamentally is about adding dimensions, and then not bloating the model by using kernels.

Now let's talk regression. Basically we flip the goal, trying to fit on as narrow of a street, as many training examples as possible. If datasets are not linear, we, once again!, use kernels!

游릭 => I think this is a good recall of SVMs.

### Ensemble Learning

Recalling ensemble learning. It is a supervised learning method where we train several ml models and then combine them together to make prediction, this should help both with lowering bias and variance. In practice if you combine simple models it doesn't help with combating underfitting, but if you combine many powerful models, like decision trees, you can get less likely overfitting. So usually ensemble learning is about combination of powerful models prone to overfitting, which we combat with collection of them.

There are several ways to make an ensemble model. You can't train the same model on the same training instances without any randomness as you'll just get a bunch of same models -- you won't have any improvement!

So there are several ways to get diversity in our models. 

1. First approach is to train same model class, say decision trees, on different training sets. For example we randomly sample 10% of training instances for each decision tree model we train. If put sampled instances back for next random pick, this is called bagging models, if not, pasting. The final result is defined by soft (with probabilities and confidences of models) or hard voting (just majority).
2. Second approach is stacking. Instead of introducing variance in what training sets we give, we train very different models, say SVM, KNN, and decision trees, on the same training set. And then to get final output aggregate their predictions with another ML model. 
3. Third approach is to give each training instance weight, and increase it for next base model if the previous model got it wrong. This way we pay more attention to examples that we got wrong. This is called boosted ensemble. 游리 => actually got that partially wrong. 
4. Last and most popular approach is gradient boosting, which is about training first model as always, say decision tree to predict our data set, and then train the next ones on their mistakes (residuals) recursively. So next trees just have to predict/ aka fix mistakes of the past ones.

In bagging, when we randomly pick training instances with replacement (putting them back for next random pick) we mathematically get 37% of instances to never be picked. We use them as validation set, where for each base model we give instrances it hasn't seen and then average out the accuracy scores for all of them to get the final one.

Random forests is a collection of decision trees with some randomness, either in features considered for each node sqrt(n), or extra random trees when also picking random thresholds.

游리 => the only feedback is about boosting. Boosting is the general approach of training base models to fix errors of their predecessors. AdaBoost achieves it with weighted training set, where missclassified instances get more weights with time (though outlier susceptible, we have learning rate/ shrinkage to control for that). Gradient boosting is when we train models to predict errors/ residuals of past models. Not that boosting is about just weighted training sets.

### Linear Functions

Now let's recall linear functions.

Basically this is the simplest supervised learning method.

Let's consider just straight lines. The training algorithm is:
1. Make guesses for all training instances (if batch gradient descent) using random weights at first.
2. Get feedback by calculating the total empirical loss via loss function, say Mean Squared Errors.
3. Update weights based on plugging in Empirical loss into gradient descent. 
4. Repeat until loss function derivative is zero -- we reached the minimum.

If the model is overfitting, we apply regularization by adding absolute or squared sum of weights to loss function as penalty.
1. Lasso Regression, absolute sum of weights, tends to drive weights to zero, natural feature selection, but behaves poorly on highly correlated features.
2. Ridge Regression, squared weights sum.
3. Elastic Net -- adding both to loss function.

So this explains simple and complex linear functions, what about classification?

Basically we either use hard threshold, where depending if the value is say equal to zero or more we assign binary class to example. Yet because hard threshold just says which class, there's no gradient to use as information of how to shift weights. So instead we use perceptron learning rule which shifts weights when examples are missclassified.

Logistic function allows for probability output, and as function it has gradient to go off, so we can use gradient descent for it.

If we have several classes at once, we need softmax, which is a popular multinomial logistic regression method. It converts values into mutually exclusive probabilities for each class which sum up to one. argmax simply picks the class with highest value. How softmax is trained? I want to say cross-entropy loss function, which compares model's probability distributions vs the reality one, penalizing the number of bits needed transmit what class instance is. But, I'm a bit unsure it is cross entropy, even though it would make sense. So it is a point of improvement: 游리

That's mainly it for linear functions.

游릭 => Great recall, only softmax's loss function with entropy needs further improvement.

### KNN

KNN is instance based model where we find closest neighbors and define output with them. Well fitting for large datasets because we forego expensive upfront process of training, and well fitting for online learning as easy to add/ remove training instances.

How it works:
1. We get example to classify/ regress, we calculate its distance for all training examples given. Depending on features we pick different distance calculating functions.
2. We then pick k closest.
3. And use them to classify, either with weights for how close the neighbor is, or without. And regress say with averaging out neighbors, also with weight or not based on closeness.

Frankly not sure what else is there to talk about :)

游릭 => and practically that's all to remember.

### Decision Trees

Now decision trees. Popular, powerful supervised learning method. Prone to overfitting, so we need to watch out for it with hyperparameters. Very versatile.

How its training works, specifically most popular CART algorithm
1. We pick all features and their possible thresholds and calculate all possible binary splits and their impurity values.
2. Impurity is basically the loss function we're optimizing for. If we're doing a regression task it is mean squared errors, if classification gini/ entropy sum. Basically impurity is about disagreement between split classes within each other: impurity with 1 split group + impurity within 2 split group. Impurity is either calculated via gini coefficient, or entropy, which is just about disorder within group vs cross-entropy which compares model's probability distribution to reality's.
3. Then we greedily pick split with lowest impurity and do so recursively until stopping criteria for children.

We can introduce randomness by taking sqrt(n) of features, or randomly picking decision thresholds etc.

We can even output class probabilities by calculating class proportions within each node!
Regression would work by averaging target values within the node.

What else about decision trees... hm.

游릭 => it actually is it, which is nice.

---
Let's recall again SVM with which our day started. 

### SVMs

The idea is to fit as wide street between classes as possible, and put separator in the middle. Generalizability with wider street is also supported by computational learning theory.

Hard margin classification is when we search for separator without excemptions, no one can be on the street, and no examples can be on the wrong side.
Soft margin is when we balance such exceptions for the street width.

Support vectors are the closest training instances between the classes, we use them to construct the street. Street is linear. 

The issue is that many datasets are not originally linearly separable. So to use SVM we need to add dimensions to them, using curse of dimensionality to our advantage: eventually, with enough dimensions, space becomes so big that there will be a linear separation.

We can do that by adding polynomial features (basic transformations with features, like x^2 or sqrt(x)), but that leads to bloated models with too many features which are hard to train, use, and store. 
So we use kernel trick which basically adds polynomial features to find linear separator but without adding additional features, so our model stays lean to use!
Another approach is similarity function, where we add some landmark and compare how close training examples are to it. Yet what landmark to choose? Popular approach is to pick one for each training examples. While that would add a lot of features, making our dataset m with training examples have m training features (if we drop original n features), it is again too bloated. 
Kernel trick comes again for saving. This is called gaussian rbf kernel.

Oh also we can do probabilitic classification by building a logistic model on top of svm that takes svm values and computes probabilities.

Now what about regression?

To use SVM for regression we flip the objective, we try to fit on as narrow street as many training examples as possible, controlling the width of the street with hyperparameter.

And if the data is not linear, once again kernel trick comes to our savior! ...

That's  it with SVMs :)

Well that was a pretty good integrating session :)


