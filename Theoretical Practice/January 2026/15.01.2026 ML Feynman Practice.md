# Goal
The goal is to continue with supervised learning integration.

# Debrief
1. Good recall of linear functions, forgot to mention cross entropy, but recalled it correctly. Also just really cool that cross entropy can be use as some sort of "epistemological accuracy".

### Questions
1. Can you then use cross entropy to evaluate accuracy of say most probability functions vs real events?
2. How saying that cross entropy measures the average surprise between model's distribution and real distribution, squares with saying that cross entropy measures the average n of bits you actually send per option?
3. How entropy in a DT loss function is different from cross entropy?

### Answers
1. Cursor: "Yes! Cross-entropy is widely used to evaluate probabilistic forecasts against real events. It's commonly used in:
    - Weather forecasting (e.g., probability of rain vs. actual rain)
    - Economic predictions (e.g., probability of recession vs. actual recession)
    - Sports betting models (e.g., win probabilities vs. actual outcomes)
    - Any scenario where you have predicted probabilities and ground truth outcomes

    Cross-entropy measures calibration: how well-calibrated your probabilities are. A model is well-calibrated if when it predicts 70% probability, the event actually occurs about 70% of the time. Lower cross-entropy means better calibration.

    Note: Cross-entropy is different from accuracy. Accuracy only cares if you predicted the most likely class correctly. Cross-entropy cares about the entire probability distribution - it penalizes you more for being confidently wrong (e.g., predicting 0.99 probability for the wrong class) than for being uncertain (e.g., predicting 0.51 probability for the wrong class)."
2. Both are about probability discrepancies between model and reality. When p = q, cross-entropy equals entropy H(p), which is the minimum possible. When p â‰  q, you need more bits/surprise because you're using the wrong code/model.
3. Basically in DTs we are interested only in disorder of one probability -- the group of instances in a node. In cross entropy we're interested in model's probability distribution and how well it represent reality's probability distribution. Cursor:
    **Entropy (in Decision Trees):**
- Measures the **purity/disorder of ONE distribution** (the class proportions in a node)
- Formula: H(p) = -Î£ p_i Ã— logâ‚‚(p_i)
- Used to evaluate: "How mixed are the classes in this node?"
- Lower entropy = purer node = better split

**Cross-Entropy (in Softmax/Neural Networks):**
- Measures the **discrepancy between TWO distributions** (predicted probabilities vs true labels)
- Formula: H(p, q) = -Î£ p_i Ã— log(q_i) where p = true, q = predicted
- Used to evaluate: "How far off are my predictions from reality?"
- Lower cross-entropy = better predictions

# Practice

ML can be divided into three parts based on how we give feedback to models:
1. Supervised Learning, input-output pairs
2. Unsupersived Learning, no output values
3. Reinforcement Learning

Now we'll talk about the main algorithms for supervised learning.

### Linear Functions
On basic linear function we will illustrate steps of training models.

1. Guess -- with random starting weights we make predictions for all training instances.
2. Feedback -- we compare those predictions with training target values, and calculate the loss, or feedback based on the loss function. The calculated error is called empirical loss.
3. Update -- we calculate the gradient descent of empirical loss and weight value to decide how to update weights.
4. Repeat -- we repeat this process until derivative is zero and we have no improvement in empirical loss by shifting weights.

Besides training models, it is important to talk about regularization of them and classification using linear functions.

Models can overfit the data, we can regularize them by adding to loss function a penalty for big weights. In linear regressions we can use:
1. Lasso Regression, L1, adding the sum of absolute weights which drives weight values to zero, which is feature selection!, but it behaves poorly with highly correlated features.
2. Ridge Regression, L2, summing of squared weight values.
3. Elastic Net, adding both to loss function.

Now on classifier linear functions. You can use line that acts as decision boundary, and you run its output through hard threshold function which returns the class, based on say whether line is more or equal and less than 0. 
If you use hard threshold you can't use gradient descent, because the derivative is almost always zero. Say the class is either zero or 1 due to hard threshold, we don't have slope to move along. Hence, we use perceptron learning rule which update weights only in missclasification cases.

You can also use logistic regression which also gives you probabilities to work with, and with it you can use gradient descent.

If you want multiclassification, the popular algorithm is softmax, which is a multinomial logistic regression, you can use it only for mutually exclusive classes, and you run its output of class probabilities through argmax to pick the highest likelihood one.

ðŸŸ¡ => All correct for linear regression but forgot to mention that cross entropy is loss function for softmax. Basically it measures how many bits are needed to transmit instances' class based on our probabilities. For example if our prior probabilities are 0.90 for some class, and it is the class of the training instance at hand, then we need just a few bits. If say it is the other 0.1 class, then we need more bits. More bits is worse. Cross entropy tries to minimize the sum of total bits needed, as then our probability is pretty accurate.

### KNN

Cool, now KNN. It is about nearest neighbors, can be both classifying and regression. It is an instance-based model so it is efficient for large datasets as you skip the upfront training cost, and it fits well for online learning as you can easily add and remove training examples. As a model it works best with less features because of curse of dimensionality, think 2-10. 

How does it work?

When we get an example to predict, we calculate its distance between all training examples. Depending on the type of features we'll use different distance calculating methods. Then we pick the K closest neighbors and make prediction for an example. It can be either by doing majority voting for class, or maybe with weights (soft voting/ using probabilities) giving more weight to closer neighbors. If it is regression we might calculate the average of neighbors, again maybe with using weights based on proximity. 

Important hyperparameters we can control is of course neighbor count, weights or not not weight based on neighbor proximity, and how we calculate the distance.

ðŸŸ¢ => good recall, that's the main stuff I want to integrate and remember for knn, the rest can be looked up on demand.

### Decision Trees

Decision tree is one of the most popular supervised learning techniques. It is a powerful, intepretable, and pretty universal model. 

It's most popular training algorithm is greedy CART. Basically based on the training instances, feature, and their thresholds available we are trying all feature+threshold possible splits to get maximum purity per split. Splits are binary, and we always go for the biggest purity split, we repeat that until some stopping criteria as DTs are very prone to overfitting and we should control for it with hyperparameters.

What is purity? Basically an agreement between the training instances groups. Say we split 100 examples, into 60 and 40, we're trying to measure how much "agreement" there's in those 40 and 60 examples. So say we are trying to classify some example, we'd know that in those group of 40 there are only 2 examples which are not the same class. That higher agreement vs if there are 5 different, pretty well represented classes. Based on such class proportions in each group we by the way can output class probabilities. Say how likely some example to be each class if it got to that node. For regression that could be the variance in target values. And we could return regression values by averaging those in the group.

If i recall correctly purity is measured either by gini or entropy. And loss function is basically the sum of left and right group's purity. Entropy is again probability surprise.

ðŸŸ¢ => also good recall of DT's, that's at the level I'd like to keep it. Good to learn differences between entropy and cross entropy.
