# Goal

Practice dimensionality reduction I understood, then all the supervised learning techniques.

# Debrief
Generally good integrating session, becoming solid in all supervised learning, and feel solid in dimensionality reduction. Next will focus on understanding other unsupervised learning techniques.

### Questions
1. Wait, but how much sense that makes. If I use my training instances to give predictions this is closer to instance-based ml model, then model-based model. Did I all this time misclassified DT as model type, when it always was instance-based ? !
2. What is entropy in this case? I know it is concerned only with the inrgoup distribution, but how it calculates the entropy in it? Is it how many bits would be needed in total to signal classes? Not sure.

### Answers
1. Cursor: "**Answer:** Decision Trees are **model-based** (eager learning), not instance-based! Here's why:
    - **Instance-based models** (like KNN) store ALL training data and make predictions at query time by searching through stored instances. No model structure is built during training.
    - **Model-based models** (like Decision Trees) build an explicit model structure during training (the tree with splits and rules). After training, you can discard the training data and only keep the tree structure.
    - The fact that Decision Trees use class ratios or averages from training instances in leaf nodes doesn't make them instance-basedâ€”it's just how the model stores its learned patterns. The key difference is that the tree structure (the model) is built upfront, and predictions are made by traversing this fixed structure, not by searching through stored instances.

    **Clarification:** Decision Trees do NOT store training instances! During training, when a leaf node is created, the algorithm calculates and stores:
    - **For classification**: The class distribution (ratios/proportions) or the majority class label
    - **For regression**: The average target value of all training instances that reached that leaf
    These statistics are computed once during training and stored in the leaf node. During prediction, you simply traverse the tree following the split rules, and when you reach a leaf, you return the pre-calculated stored value(s)â€”no access to original training instances needed!"
2. Cursor: " **Answer:** Yes, you're exactly right! Entropy measures the average number of bits needed to encode the class label of a randomly selected instance from that node.

    **Formula:** H(X) = -Î£ p(x) logâ‚‚ p(x)
    - Where p(x) is the proportion of class x in the node
    - The sum is over all classes present in the node

    **Intuition:**
    - If a node is pure (100% one class): p(class) = 1, so H = -1 Ã— logâ‚‚(1) = 0 bits (no uncertainty, no information needed)
    - If a node is perfectly mixed (50% class A, 50% class B): H = -0.5 Ã— logâ‚‚(0.5) - 0.5 Ã— logâ‚‚(0.5) = 1 bit (maximum uncertainty for binary classification)
    - For a fair 4-class distribution (25% each): H = -4 Ã— (0.25 Ã— logâ‚‚(0.25)) = 2 bits

    **In Decision Trees:** We calculate entropy for each node based on the class distribution of training instances that reached that node. Lower entropy = more pure = better split. Information Gain = Entropy(parent) - Weighted Average Entropy(children), and we choose splits that maximize Information Gain (minimize child entropy)."

# Practice

## Dimensionality Reduction

We want to reduce dimensions (features) of data in hope that this will make information transformation easier.

The two mani types of approaches are projection and manifold learning. In projection we assume that has straight-line patterns, and we're just trying to "slice" given data to us with straight axis dimensions. Manifold learning is about curvy, folded, rolled up patterns, where straight axis won't do the job, think like a swiss roll/ cinabon.

Now I'll talk about 3 most popular methods.

### PCA
Projection method focused on finding straight axis that explain the most amonut of variance of data recursively. Now let's explain what that actually means.

Basically we first search for straight axis in our data that would explain the most variance. Once we find it we search for next straight axis that would explain the most amount of variance that wasn't yet explained by the first axis. 
To do that we search only orthogonal positions to the first axis, otherwise we'll explain the same variance twice. Once we find this second orthogonal axis we keep on doing that recursively, searching for 3rd axis that explains the most variance that first two didn't by making sure it is orthogonal to both of them. 
We do this recursion the same number of times as number of dimensions in our original data. That would give us the same number of dimensions -- there's no reduction in that :)
To reduce dimensions we pick the most variance explaining dimensions by choosing a threshold of variance we want to keep explained, say 95%. Then pick in descending order dimensions until we reach 95%.

Also you can do very cool thing of looking at what dimensions explain the most variance, this could be very insightful.

Finding first axis can be expensive, so we also could find approximate axis for it. I think this method is called incremental projection. ðŸŸ¡

### Random Projection 

Random projection is a pretty cool method. It randomly generates straight axis, and if you do that enough times you develop strong confidence that you have covered most of variance. Here's how it works:

You select how much difference between any two points before and after dim. red. you're willing to accept, say e=10%.
Then you say how many dimensions you have and how many training examples.

Then using formula we calculate how many dimensions we need to randomly generate to say with sufficient confidence that you'll be within that acceptable delusion range.

Say for 20,000 features and 10% it is 7,300 dimensions. And then we randomly generate them and voila!

The power of statistics is pretty cool!

### LLE

Now LLE, abreviation of which I absolutely don't remember. This is a manifold learning method, so no straight axis.

Basically its first step is to take every training instance and compute its k nearest neighbors, say 10. Then use those 10 neighbor instances to recreate the original instance (x_i) with linear function. 
So: x_i=w0x^j ....

Once we find appropriate weights for that, for every training point. Well once that happens we have spend shit load of compute that's for sure. So this method doesn't scale well. Anyways. 
We put all those weights in a big matrix, and now we have weight matrix that describes local relationships between points.

Now we will do the reverse. If previously we held dimensions fixed and searched for weights that would describe relationships between training instances. Now we'll search for dimensions that could fit the fixed weights that describe relationships between training instances!

Pretty cool ha! But that would only preserve local relationships between points, not universal as we're taking the closest neighbors.

--

Well, what else about dimensionality reduction? There are a few other methods I won't cover. I think that's it honestly.

ðŸŸ¢ It is! => Only feedback is that incremental pca is about mini-batch trainings because of memory constraints for PCA, and randomized PCA is about finding first approximate axis.

## Supervised Learning
Cool, now let's go into supervised learning. You can divide ML based on how we give feedback to models, which would make it supervised with input-output pairs, unsupervised, without output pairs (there's also semi-supervised...), and reinforcement learning.

### Linear Functions
The most basic supervised learning. Let's consider on the simplest example of fitting a line to data how we train data.

1. Make guesses with randomly initiated weights for all training instances (if it is batch gradient descent).
2. Calculate feedback for our guesses based on loss function, all together this is called empirical loss. For linear functions that could be the mean squared errors. 
3. Now based on empirical loss we calculate how to update our weights to make less mistakes, specifically with gradient descent.
4. Repeat this loop until we plateau with the amount of mistakes we make, because empirical loss derivative is zero -- i.e. changing would make it worse.

Now more on linear functions, in supervised learning you can do classification and regression. We might have too powerful model that could overfit our data, then we'd need to regularize it. What that means is that we add a penalty to loss function for having big weights, so model would optimize for small weights. 

The penalty could be either sum of squared weights (ridge regression, l2), or sum of absolute weights (lasso regression, l1, drives weights to zero - natural feature selection, but behaves poorly if features are correlated), or both (elastic net).

Now on how we could do classification. We could use linear function with hard threshold, say outputs with less or equal values are class 0, and with more class 1. This would make impossible to learn good weighs for classification with gradient descent, because the graph would be just always on 0, and then jump to class 1 because of hard threshold. So derivative is nearly always zero! So instead we use perceptron learning rule which shifts weights everytime we missclasify a training example.

We could also use logistic function which would give us probability outputs and we could use gradient descent on it.

If we'd like to use multiclassification, so not binary classes, but several mutually exclusive classes we could use softmax which is a multinomial logistic function. It takes input values and transforms them into probability distribution of likelihood for each class. With argmax we pick the highest probability class.

How do we train softmax? The interesting part is loss function. We use cross entropy which is about comparing probability distribution our model gives and probability distribution of "reality", and if the difference is too big it penalizes our model.

Basically how it calculates that is the number of bits needed to tell which class training instance is. If our model has prior probabilities of 90% for some class, then we need just a few more bits to tell that this is the class, and if some class has 1% chance, then we need much more. As loss function we're trying to minimize the total number of needed bits to transmit classes over the training set. 

That's mostly it for linear functions.

ðŸŸ¢ => perfect recall for linear functions !

### KNN
Now KNN, shall we. This is an instance based model, so it scales well and works well for online learning as we don't need to train costly model and we can easily add/remove training instances. But number of features has to stay small, as otherwise dimensionality reduction would make any distances between points huge and we'll practically never have enough training instances to make good guesses.

Here's how KNN works. We give it an example to predict. It calculates its distance between all training insatnces and picks K closest. The distance calculating method depends on feature type. Then if it is classification we make a guess either with hard majority voting, or softly by taknig account of closeness of each neighbor and giving more weight to closer ones. Or if regression taking the average, also possibly with weights for closer neighbors. (Can we give class probabilities by looking at the ratio in our neighbors? I guess so.)

In hyperparameters we control number of neighbors, whether use weights for closeness, and how distance is calculated.

That's it for KNN.
ðŸŸ¢ => It is indeed, perfect recall.

### Decision Trees

Now DT's a very powerful supervised learning method. Prone to overfitting as it is a powerful model, and we can use it in variety of cases, classification, regression, probability classification, you have it all! And it indeed works well for such cases!

The most popular training algorithm is CAMP, here's how it works.
For every feature and its every possible threshold split of training instance at hand you calculate the impurity of each group which it would split. It is always binary splits. The total impurity is the sum of weighted impurities for each group, say left and right. What is impurity? It is basically the disagreement in the group for the target value. If disagreement is high, impurity is high. We calculate that using entropy, or gini coefficient. 
Then we take the lowest impurity split, so we're greedily choosing. And then do go recursively into children nodes doing the same search until stopping criteria.

For classification you can use again majority voting, or look at ratio of classes in the node and output it as probability, or you can do regression and average target values of training instances in a node.

â“ Wait, but how much sense that makes. If I use my training instances to give predictions this is closer to instance-based ml model, then model-based model. Did I all this time misclassified DT as model type, when it always was instance-based ? !

â“ What is entropy in this case? I know it is concerned only with the inrgoup distribution, but how it calculates the entropy in it? Is it how many bits would be needed in total to signal classes? Not sure.

ðŸŸ¢ => In total good recall, and clarified a few questions.

### SVM's
SVM's are trying to fit the biggest street between classes for classification. They pick closest points between classes, calleld support vectors, and try to fit straight line as wide as possible between the two, putting in the middle the decision boundary. Computational learning theory suppports that wider margin makes better generalizability.

But what if our data is not originally linearly separable? Well, curse of dimensionality comes to save us. Eventually, as we add more dimensions we'll find the appropriate linear split as space is so big.

We could that with some "manual" techniques, like polynomial transformations of our features, but that bloats the model, making it hard to store, train, and use. So instead we use kernel trick which achieves the goal of kind of adding dimensions and finding appropriate linear split, but without actually adding features so the model doesn't bloat.

Ideally the split would have no violations, no points on the margins, and all instances are on the right class side. This is hard margin. Soft margin is when we balance exceptions with fitting big margin.

And if we want to use regression then we flip the task and try to fit as many training instances as possible, on as narrow margin as possible. And if data is not linear we once again use kernel trick to fit it without the bloat, with regression that technique is called gaussian rbf kernel.

ðŸŸ¡ => Partially mixed up things. Gaussian RBF kernel is another approach for classification which comes from similarity function computation. All else is fine. Oh, actually forgot to mention we can output class probabilities by fitting on top our svm output values logistic regression.

### Ensemble Learning

And now lastly ensemble learning. It is about putting models together, not training just one which usually gives us better results. Mainly, it works best for powerful models that tend to overfit as putting them together mitigates that. Yet, it doesn't help to reduce underfitting so weak models don't work that well. 
(â“ Wait, but what about putting together many weak learners which together produce a better strong learner?)

Basically we need to train different models, we can't use same model, on the same training data as we'll just train 100s of times the same model. So, we need to introduce randomness to our process to get diversity.

One way to do that is to give different training samples to the same models. Such ensemble learning method is called bagging when you put back picked instances, and pasting when you don't. Using bagging as this is random picking leads to 37% of examples to be never picked. We use them as validation set for its every corresponding base models, averaging across all base models to get the final validation score. These 37% examples are called out of bag, and this validation score can also be called out of bag score.

So here we train same models on different training instances. When we put them together we can either do hard voting where just majority wins, or soft voting where we look at confidence our base models give. 

We can also train a separate model to aggregate predictions from base models to give the final output. This technique is called stacking, as we stack models on top of each othher, and it is done with ideally very different based models, say svm vs decision tree, but models get the same training data.

What else.
Random forests is a collection of decision trees that is random because we randomly pick sqrt(n) of features when deciding on splits to consider at each node. Extra random trees also pick thresholds randomly.

ðŸŸ¡ => forgot to talk about boosted ensembel and gradient boosting.
I haven't looked into the notes, so I'll try to recall them now.

Boosted ensemble is a class of ensemble techniques that try to fix mistakes of past base models in some way. One popular way of doing that is adaboost algorithm where training instances that were missclassified get bigger weights for next base models to train on. Gradient boosting is a boosted ensmeble technique which is about training first base model as any model, and all the other ones as correcting models on residual errors of the first one. So second model is trying to predict residual errors of the first one, and so on recursively. XGBoost is the most popular algorithm for that.

ðŸŸ¢ Besides forgetting to talk about boosted ensemble and gradient boosting on my own all else was good, and then recalled those two correctly.

Pretty good integrating session!
