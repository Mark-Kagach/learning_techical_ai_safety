# Goal
The goal is to understand chapter 6 of HOMLP and make notes. Then practice explaining and applying learned concepts.

# Notes
Here are the notes I made that day:

- Intro
    - So far we have looked at learning methods in which a single Hypothesis is used to make predictions. The idea of Ensemble Learning is to select a collection, or ensemble, of hypotheses, h_1, h_2, . . . , h_n, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses Base Models and their combination an Ensemble Model.So far we have looked at learning methods in which a single hypothesis is used to make predictions. The idea of ensemble learning is to select a collection, or ensemble, of hypotheses,Ensemble learning h1, h2, . . . , hn, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses base models and their combination anBase model ensemble model.
    - The two reasons for doing ensemble learning is: to reduce Bias (Underfitting) that any one model can introduce, and then also reduce Variance (Overfitting) as several models have to classify similarly the example.
    - Ensemble Models work best when Base Models are independent and manage to make different kind of errors. This way when put together they rarely make any kind of error. If they are perfectly dependent than your ensemble model would just make the same errors as any base model. 
    - (One way to achieve that is by using a very different set of base models, think logistic regression vs random forest...)
    - In practice Ensemble Models usually have similar Bias as Base Models, but lower Variance, so it works best for low-bias, high-variance models, like Decision Trees, not Linear Functions.
    - 
- 
- Handling Training Sets
    - Bagging ‒ training several models (same model Model Class, think all decision trees) on different random training sets (to achieve Base Models independence), and then combining them in an Ensemble Model. 
    - When sampling is done with Replacement, it is Bagging, when without, it is called Pasting. 
    - Bagging and Pasting in sklearn:
    - 
    - Random Patches and Random Subspaces:
    - 
- Handling Base Models
    - Stacked Generalization/  Stacking ‒ training several different Model Classes on the same training set, and then combining them as an Ensemble Model.
    - In HOMLP Stacking is about instead of using basic functions like Hard Voting to aggregate base model predictions, we train a model to do this aggregation.
    - 
- Handling Voting
    - Voting Classifier ‒ you train several models (one of which could even be Ensemble Model itself), and then do voting (Hard Voting, Soft Voting) on what output to choose. 
    - 
- Out-of-bag Evaluation
    - When using Bagging only about 63% on average out of all training set would get selected for each base model. The remaining 37% are called Out-of-bag Instances, you can use them to test accuracy of Ensemble Model, it plays the role of Validation Set (you still need to split data prior for test set).
    - We get the Out-of-bag Evaluation score by testing Base Models based on their corresponding Out-of-bag Instances, then averaging out their accuracy for all base models to get the Ensemble Model score.
    - Out-of-bag Evaluation in sklearn:
    - 

