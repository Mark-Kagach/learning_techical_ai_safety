# Goal
The goal is to understand chapter 6 of HOMLP and make notes. Then practice explaining and applying learned concepts. 
I finished making notes on the chapter, next I'll practice explaining and applying its concepts.

# Notes
Here are most of my notes on ensemble learning:

- Intro
    - So far we have looked at learning methods in which a single Hypothesis is used to make predictions. The idea of Ensemble Learning is to select a collection, or ensemble, of hypotheses, h_1, h_2, . . . , h_n, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses Base Models and their combination an Ensemble Model.So far we have looked at learning methods in which a single hypothesis is used to make predictions. The idea of ensemble learning is to select a collection, or ensemble, of hypotheses,Ensemble learning h1, h2, . . . , hn, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses base models and their combination anBase model ensemble model.
    - The two reasons for doing ensemble learning is: to reduce Bias (Underfitting) that any one model can introduce, and then also reduce Variance (Overfitting) as several models have to classify similarly the example.
    - Ensemble Models work best when Base Models are independent and manage to make different kind of errors. This way when put together they rarely make any kind of error. If they are perfectly dependent than your ensemble model would just make the same errors as any base model. 
    - (One way to achieve that is by using a very different set of base models, think logistic regression vs random forest...)
    - In practice Ensemble Models usually have similar Bias as Base Models, but lower Variance, so it works best for low-bias, high-variance models, like Decision Trees, not Linear Functions.
    - 
- 
- Handling Training Sets
    - Bagging ‒ training several models (same model Model Class, think all decision trees) on different random training sets (to achieve Base Models independence), and then combining them in an Ensemble Model. 
    - When sampling is done with Replacement, it is Bagging, when without, it is called Pasting. 
    - Bagging and Pasting in sklearn:
    - 
    - Random Patches and Random Subspaces:
    - 
- Handling Base Models
    - Stacked Generalization/  Stacking ‒ training several different Model Classes on the same training set, and then combining them as an Ensemble Model.
    - In HOMLP Stacking is about instead of using basic functions like Hard Voting to aggregate base model predictions, we train a model to do this aggregation.
    - 
- Handling Voting
    - Voting Classifier ‒ you train several models (one of which could even be Ensemble Model itself), and then do voting (Hard Voting, Soft Voting) on what output to choose. 
    - 
- Out-of-bag Evaluation
    - When using Bagging only about 63% on average out of all training set would get selected for each base model. The remaining 37% are called Out-of-bag Instances, you can use them to test accuracy of Ensemble Model, it plays the role of Validation Set (you still need to split data prior for test set).
    - We get the Out-of-bag Evaluation score by testing Base Models based on their corresponding Out-of-bag Instances, then averaging out their accuracy for all base models to get the Ensemble Model score.
    - Out-of-bag Evaluation in sklearn:
    - 
- 
- Random Forest
    - Random Forests ‒ a form of Decision Tree Bagging (Extremely Randomized Trees, Out-of-bag Error): 
    - 
    - sklearn, and using random subset of \sqrt{n} features from which to choose tree node:
    - Extra-Trees/ Extremely Randomized Trees, not only picking from random subset, but also picking random thresholds:
    - 
    - Measuring Feature Importance, sklearn:
    - 
- Boosted Ensemble
    - Boosted Ensemble ‒ the most popular Ensemble Learning method. 
    - 
    - Boosting refers to any Ensemble method where several Weak Learners are combined into a Strong Learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. By far the most popular ones areAdaBoost and Gradient Boosting.
    - 
    - AdaBoost
    - 
    - AdaBoost, Weak Learning, Decision Stump (...Bayesian Learning):
    - 
- Gradient Boosting
    - Gradient Boosting is about iteratively training base models on their predecessors Residual Errors. 
    - After creating the first base model, you look at its Residual Errors and ask the next model to predict them (aka correcting model). You run this loop as long as you'd like. 
    - Shrinkage ‒ you use \alpha Learning Rate to control the impact of correcting models. 
    - To get prediction you sum up prediction of the first base model, and correcting models predictions of base model's errors.
    - 
    - Gradient Boosting in sklearn (Early Stopping with patience):
    - Stochastic Gradient Boosting is about training each predictor tree on a subset of all training examples, say 25% selected randomly. This speeds up training, and trades higher underfitting for low chance of overfitting. 
    - 
    - Histogram-based Gradient Boosting another version of gradient boosting regression trees that is optimized for large datasets.
    - It bins inputs features and replaces them with integers. Binning can greatly reduce the number of possible thresholds that training algorithm needs to evaluate.
    - So continuous features like house prices are binned into say 255 bins (max number), and gradient boosting is run on these binned features
    - Histogram-based Gradient Boosting, sklearn:
    - 
    - 
    - Gradient Boosting ‒ using Gradient Descent for Boosting: we generate new hypothesis functions, but instead of paying attention to previously got wrong examples, we pay attention to the Gradient between the right answer and the answers given by previous hypotheses.
    - As with any algorithm using Gradient Descent we have a Loss Function to optimize for.
    - We then build a Decision Tree, similar to AdaBoost.
    - Previously we have used Gradient Descent to iteratively change model's parameters so that they minimize Loss Function. 
    - For Gradient Boosting we use Gradient Descent to change parameters of the next decision tree, not the current one. And we similarly do it in the direction of minimizing Loss Function. 
    - 
    - 
    - XGBoost is a popular implementation of Gradient Boosting that balances pruning, Regularization, computational efficiency (Computational Complexity), careful memory organization to avoid cache misses, and allowing parallel computation.
    - XGBoost
    - 
- 
- Summary of all Ensemble Models:
    - https://remnote-user-data.s3.amazonaws.com/OgIcErcqsdnQvsv2dTEzclZiZ3wKOAn_cfCFEWMX4NpKAQuVg-wyzLyJJBHoAxSneTihOAQyqtOjchyiOE5xzOS7CIuBz-0bHzUR7XWfynLE2wOjqO__in5dLM6foMZU.pnghttps://remnote-user-data.s3.amazonaws.com/zyQh7FlXOnhyDcSyl_iCvNensKzMsy8crL-K_DCaVKsMZYKUQcWohkVzHGMr9q2arAqeSOxU8JuXMCrUPeKQHzCJL8SL8-r6h9BV0P-kSJeBl7OVjglcpLmnt4VAy1WF.pnghttps://remnote-user-data.s3.amazonaws.com/OgIcErcqsdnQvsv2dTEzclZiZ3wKOAn_cfCFEWMX4NpKAQuVg-wyzLyJJBHoAxSneTihOAQyqtOjchyiOE5xzOS7CIuBz-0bHzUR7XWfynLE2wOjqO__in5dLM6foMZU.png
    - 
- 
