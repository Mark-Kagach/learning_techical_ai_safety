# Goal

Just want to share my notes on dimensionality reduction.

# Dimensionality Reduction

- There are different approaches to dimensionality reduction, we do them in hope of making data manipulations easier.
- Projection is a simpler method focused on "flattening" the data, like shining a flashlight on a 3D object and looking at its 2D shadow. It assumes data sits on a linear subspace inside a high-dimensional space. So to reduce dimensions it searches for the best angle to look at the data and project it on a flat plane.
- Manifold Learning is a more complex method, focused on "unrolling" the data. It assumes data has curved, non-linear shape, so it tries to learn (hence the manifold learning) the shape of the curve and then unroll it.
- 
- Principal Component Analysis
    - Principal Component Analysis (PCA) is by far the most popular Dimensionality Reduction technique.
    - It searches for the axis that preserves the most Variance (Highest Variance Model). When it finds one, next axis should explain the most of remaining variance, and so on recursively for as many dimensions as in the dataset.
    - To force principal components to explain remaining variance, and not repeat past axis we force them to search only Orthogonal lines to all past axis. Orthogonality explained:
    - Once you have all possible principal components for the data, you reduce dimensions picking d principal components. 
    - You pick d by setting a threshold of variance you want to preserve, say 95%, then pick whatever number of dimensions that get you there. sklearn, MNIST Dataset example:
    - 
    - More
        - Explained Variance Ratio is a super useful information, showing how much variance each dimension explains:
        - (Basically what you're doing with PCA is recreate dimensions from scratch based on descending variance it explains, and then pick top dimensions sum of which passes some variance threshold like 95%.)
        - 
        - PCA compression, decompression; Reconstruction Error:
        - Randomized PCA for faster search of first approximate axis:
        - Incremental PCA to solve the memory constraint issue by splitting training sets into mini-batches:
        - 
        - (i'th axis is the i'th principal component of the data.)
        - Singular Value Decomposition (SVD):
        - 
    - 
- Random Projection
    - Random projection randomly generates linear dimensions. And it does that for the minimum necessary quantity to ensure, with high probability, that distances between instances won't change more than a given threshold.
    - Suppose you have 5,000 training instances, with 20,000 features, and you don't want the squared distance between any two instance to change by more than e=10%. Then you should project the data onto 7,300 dimensions! ...
    - Damn...
    - 
- Locally Linear Embedding (LLE) 
    - Locally Linear Embedding is a nonlinear dimensionality reduction technique.
    - LLE determines how each training instance linearly relates to its nearest neighbors. Then its looks for a low-dimensional representation of the training set where these local relationships are best preserved.
    - This works well for unrolling twisted manifolds, especially with low noise. Yet computationally it doesn't scale, working well only for small and medium datasets.
    - Example of LLE unrolling Swiss roll and preserving local distances, but not larger scale ones:
    - 
    - For each training instance x_i LLE identifies its k-nearest neighbors. Then it tries to reconstruct the instance as linear function (that's why linear embedding!) of those neighbors. Searching for weights so that \sum_{j=1}^k w_{i,j}x^{(j)} \approx x^{(i)}
    - Putting those weights together we have the weight matrix \widehat{W} that encodes the local linear relationships between training instances.
    - Now LLE searches for lower dimensions Z that would preserve those weight relationships between training instances.
    - 
    - HOMLP, sklearn, LLE:
    - 
- 
- More:
    - HOMLP on Dimensionality Reduction, Curse of Dimensionality:
    - HOMLP, other Dimensionality Reduction techniques. Multidimensional Scaling, Isomap, t-SNE, Linear Discriminant Analysis, UMAP,
    - 
    - Understanding is, in some way, about performing Dimensionality Reduction, where you can compress information into just a few variables compared to original. Certainly related to Perspective.
    - 
