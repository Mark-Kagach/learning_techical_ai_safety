# Goal
I have made serious changes to my notes on supervised learning in my knowledge base over the past few days, I figured I should share them here, as part of "understanding". As lways this is rough as I copy directly  from remnote.

# Supervised Learning Notes

- Fundamentals
    - Supervised Learning is about giving a dataset of input-output pairs, so computer can learn a function (Model) to map inputs to outputs. 
    - When the output is one of finite set of values, it is Classification, when it is a number, it is Regression (better called function approximation, or numeric prediction).
    - 
    - https://remnote-user-data.s3.amazonaws.com/WXxlwelXyLSYi0HwG7aP8aaZwnZllGKzb5KuJLoF6IOXD0_rEJFhd8LQL7SVWIHWi5XLDQNZJPssTa-rDT5YyQLKhOYxfRat8Sd9beeRZJjfQx9NFUVvBgjw3As1jOba.pnghttps://remnote-user-data.s3.amazonaws.com/WXxlwelXyLSYi0HwG7aP8aaZwnZllGKzb5KuJLoF6IOXD0_rEJFhd8LQL7SVWIHWi5XLDQNZJPssTa-rDT5YyQLKhOYxfRat8Sd9beeRZJjfQx9NFUVvBgjw3As1jOba.png
    - 
    - The true measure of a Hypothesis Function h is not how it performs on the Training Set, but how well it handles inputs it hasn't seen, i.e. Test Set (x_i, y_i) pairs. 
    - We say Hypothesis Generalizes if it accurately predicts outputs of the test set.
    - The Hypothesis Function h that Learning Algorithm discovers depends on the Hypothesis Space \mathcal{H} it considers and on the given Training Set, see indented picture.
    - 
    - When picking Hypothesis Function we can even choose the most probable function Hypothesis h^* 
    - Why not let \mathcal{H} be the class of all computer programs, or all Turing machines? 
    - 
- 
- Linear Function
    - Fundamentals
        - Linear Function take continuous-valued inputs. Both can be regression and classification. It poses significant constraints to the Hypothesis Space \mathcal{H}. 
        - h_w(x) = w_1x+w_0
        - Finding h_w that best fits data is called Linear Regression. To fit a line we have to find \langle w_0, w_1 \rangle that minimize the Empirical Loss. 
        - 
        - We start with random weights and using them to make predictions on all training set (in case of Batch Gradient Descent). 
        - Then we use Loss Function of Mean Squared Errors to calculate the error (i.e. feedback):
        - Then calculate Gradient Descent to decide how to update the weights:
        - We repeat this loop until loss function change stalls, i.e. loss function derivative is zero.
        - 
        - More
        - 
    - Multivariable Linear Function
        - Univariate Linear Function
        - 
        - Regularization of Linear Functions 
        - Overfitting is a real problem for Multivariable Linear Function as in multidimensional Weight Space some trough might appear useful, when in fact it is just overfitting. 
        - To combat that we use Regularization, which is a penalty in Loss Function for big weights: 
        - J(w,b) = (1/2m)Σ(f(x⁽ⁱ⁾) - y⁽ⁱ⁾)² + (λ/2m)Σwⱼ²
        - 
        - Lasso Regression (L1) ‒ sum of absolute weights, can drive weights to zero, i.e. automatic Feature Selection! But performs poorly when features are correlated.
        - Ridge Regression (L2) ‒ sum of squared weights, doesn't shrink weights to zero ‒ good when many features matter.
        - Elastic Net Regression ‒ combining both!
        - λ: Controls regularization strength. Larger λ = more regularization = simpler model. (Tune via cross-validation.)
        - 
        - More
        - 
    - Classifier Linear Function
        - We also can use linear functions to predict classes by using the it as Decision Boundary.
        - 
        - To predict classes we can pass linear function output through Threshold Function, say more or equal to 0. This determines instances class.
        - In such cases we can't use Gradient Descent because almost everywhere threshold function is either zero or one, so derivative almost everywhere is zero. Hence gradient doesn't know in which direction to move:
        - Instead we use Perceptron Learning Rule which moves weights only if output is misclassified:
        - 
        - Other approach is to use Logistic Regression, as it will give us probabilistic output.
        - Because it returns probability, derivative is not zero in all parts of the graph, so we have signal to use Gradient Descent.
        - 
        - Softmax is a Multinomial Logistic Regression that does Multiclassification and returns probabilistic outputs. (Argmax plays the role of Threshold Function returning the class based on probabilities.)
        - Cross Entropy is the Loss Function of Softmax. It measures "average surprise" between model's distribution p approximating real distribution q. (Cross entropy measures the average number of bits you actually send per option.)  
        - For example if model with p=0.99 predicted the wrong class loss will be huge, but when the class is right loss is practically zero.
        - Softmax explained and in sklearn, HOMLP:
        - 
        - (We also can use "binary style" classifier with hard-thresholds to do Multiclassification:)
        - More
        - 
        - 
    - 
- KNN
    - Given a new instance x, KNN computes distance from x to all existing training instances.
    - It sorts and picks K closest. Then classifies by majority vote, or in case of regression takes the average of neighbors' target values.
    - Depending on the features KNN relies on different ways for calculating distances: Minkowski Distance, Hamming Distance, Mahalanobis Distance.
    - In hyperparameters you can choose number of neighbors, how to calculate the distance, and whether closer neighbor should have strong vote, i.e. weights.
    - 
    - KNN works well with a few features (2-10), but fails with high-dimensional data because of the Curse of Dimensionality:
    - , and Instance-Based Learning (Nonparametric Model) in general are designed well for large datasets because you forego expensive upfront training. It also fits perfectly for Online Learning when new examples are frequently added.
    - 
    - More:
    - KNN with K-d Tree, and Locality-sensitive Hash, to speed the O(N) execution time without these methods:
    - 
- Decision Tree
    - CART is the most popular Decision Tree training algorithm. It builds a binary tree (2 children per split) greedily. 
    - That means for each node it tries all possible splits (feature + threshold) and picks one that minimizes the Loss Function (impurity of MSE). (More on trying all possible thresholds:)
    - Then recurses on child subset until stopping criteria.
    - 
    - Purity is how much does the group agree on the class label, or on the continuous target values.
    - For Regression it is usually minimizing weighted MSE:  J(k,t_{k}) = \frac{m_{left}}{m} MSE_{left} + \frac{m_{right}}{m} MSE_{right}
    - And for Classification minimizing weighted child impurity: J(k,t_k) = \frac{m_{left}}{m} G_{left} + \frac{m_{right}}{m} G_{right}
    - Gini Impurity vs Entropy for Classification (Gini is faster and both produce similar trees):
    - 
    - Decision Trees can even produce class probabilities by calculating ratio of classes in a node.
    - Decision Trees are very prone to Overfitting, usually you control for that through hyperparameters, from max_depth to max_features:
    - More on Decision Trees limitations:
    - 
    - More
    - 
- SVM
    - s find the closest points between classes (support vectors), and then find separator that is furthest from both. Think of it as finding the widest possible street between classes: Large Margin Classification.
    - SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds to thousands of instances), especially for classification tasks. However, they don’t scale very well to very large datasets, as you will see.SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds to thousands of instances), especially for classification tasks. However, they don’t scale very well to very large datasets, as you will see.
    - This large-margin approach improves generalization, backed by Computational Learning Theory linking margin size to lower generalization error.
    - SVM methods are susceptible to scaling.
    - 
    - Hard Margin Classification is when we impose that all instances must be strictly off the "street" (margin hyperplane) and on the correct side.
    - Linear SVM Classification is the most basic SVM method doing Hard Margin Classification. Yet, that makes it susceptible to outliers.
    - Soft Margin Classification is when we balance Margin Violations (when instances are on the street or on the wrong side), while still fitting the widest separator possible:
    - 
    -  in sklearn, and hyperparameter c that is responsible for street width:
    - Linear SVM Classification doesn't predict probabilities. Yet, if you use SVC (support vector classifier) class, and set probability hyperparameter to True, it will fit an extra model (logistic function) at the end to transform SVM function scores into probabilities: 
    - 
    - 
    - Nonlinear SVM Classification take care of datasets that originally are not linearly separable. 
    - Mainly through Curse of Dimensionality: with more dimensions eventually there should be a linear separator!
    - 
    - First approach is adding polynomial features, that is transformation of existing features, say \phi(x_1, x_2) = (x_1^2, 2x_1x_2, x_2^2).  
    - The problem is that this causes combinatorial explosion of features, which makes model training, storing, and processing very hard. Here comes the Kernel Trick:
    - Second approach is the Kernel Trick. It gets the same result as if you have added polynomial features without actually doing so, hence no costly combinatorial explosion.
    - 
    - Third approach is adding features computed using similarity function. It measures how much each instance resembles a particular landmark. 
    - Popular approach is to add landmark for every instance in a dataset which just increases dimensions. But that usually means you get dataset with m training instances, and m features, which is too many features again!
    - Forth approach is Gaussian RBF Kernel, which performs Kernel Trick on the similarity approach to fix feature explosion:
    - 
    - 
    - To train SVM Regression (i.e. SVR) you flip the objective, instead of trying to fit as wide street between classes without violations, you're trying to fit as narrow street fitting as many instances as possible:
    - For nonlinear regression task you can, once again!, use Kernels to fix it:
    - 
    - More
    - Archive 
    - 
    - 
- Ensemble Learning
    - Intro
        - So far we have looked at learning methods in which a single Hypothesis is used to make predictions. The idea of Ensemble Learning is to select a collection, or ensemble, of hypotheses, h_1, h_2, . . . , h_n, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses Base Models and their combination an Ensemble Model.So far we have looked at learning methods in which a single hypothesis is used to make predictions. The idea of ensemble learning is to select a collection, or ensemble, of hypotheses,Ensemble learning h1, h2, . . . , hn, and combine their predictions by averaging, voting, or by another level of machine learning. We call the individual hypotheses base models and their combination anBase model ensemble model.
        - The two reasons for doing ensemble learning is: to reduce Bias (Underfitting) that any one model can introduce, and then also reduce Variance (Overfitting) as several models have to classify similarly the example.
        - Ensemble Models work best when Base Models are independent and manage to make different kind of errors. This way when put together they rarely make any kind of error. If they are perfectly dependent than your ensemble model would just make the same errors as any base model. 
        - To achieve that we can use a very different set of base models, think logistic regression vs random forest. Or give base models different training examples, or introduce randomness in how base models make decisions.
        - Ensemble Models usually have similar Bias as Base Models, but lower Variance, so it works best for low-bias, high-variance models, like Decision Trees, not Linear Functions.
        - 
    - 
    - Handling Training Sets
        - Bagging ‒ training several models (same model Model Class, think all decision trees) on different random training sets (to achieve Base Models independence), and then combining them in an Ensemble Model. 
        - When sampling is done with Replacement, it is Bagging, when without, it is called Pasting. 
        - Bagging and Pasting in sklearn:
        - 
        - Random Patches and Random Subspaces:
        - 
    - Handling Base Models
        - Stacked Generalization/  Stacking/ Stacked Ensemble ‒ training several different Model Classes on the same training set, and then aggregating them with another ml model on top to produce output. 
        - So instead of using basic functions like Hard Voting to aggregate base model predictions, we train a model to do this aggregation.
        - We can do such aggregation several times, creating something closer to a neural net.
        - 
    - Handling Voting
        - Voting Classifier ‒ you train several models (one of which could even be Ensemble Model itself), and then do voting (Hard Voting, Soft Voting) on what output to choose. 
        - Hard Voting, Soft Voting:
        - 
    - Out-of-bag Evaluation
        - When using Bagging only about 63% on average out of all training set would get selected for each base model. The remaining 37% are called Out-of-bag Instances, you can use them to test accuracy of Ensemble Model, it plays the role of Validation Set (you still need to split data prior for test set).
        - We get the Out-of-bag Evaluation score by testing Base Models based on their corresponding Out-of-bag Instances, then averaging out their accuracy for all base models to get the Ensemble Model score.
        - Out-of-bag Evaluation in sklearn:
        - 
    - 
    - Random Forest
        - Random Forests ‒ a form of Decision Tree Bagging (Extremely Randomized Trees, Out-of-bag Error): 
        - sklearn, and using random subset of \sqrt{n} features from which to choose tree node:
        - Extra-Trees/ Extremely Randomized Trees, not only picking from random subset, but also picking random thresholds:
        - 
        - Measuring Feature Importance, sklearn:
        - 
    - Boosted Ensemble
        - Boosted Ensemble ‒ the most popular Ensemble Learning method. 
        - Boosting refers to any Ensemble method where several Weak Learners are combined into a Strong Learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. By far the most popular ones are AdaBoost and Gradient Boosting.
        - 
        - AdaBoost
        - AdaBoost, Weak Learning, Decision Stump (...Bayesian Learning):
        - 
    - Gradient Boosting
        - Gradient Boosting is about iteratively training base models on their predecessors Residual Errors. 
        - After creating the first base model, you look at its Residual Errors and ask the next model to predict them (aka correcting model). You run this loop as long as you'd like. 
        - Shrinkage ‒ you use \alpha Learning Rate to control the impact of correcting models. 
        - To get prediction you sum up prediction of the first base model, and correcting models predictions of base model's errors.
        - 
        - Gradient Boosting in sklearn (Early Stopping with patience):
        - Stochastic Gradient Boosting is about training each predictor tree on a subset of all training examples, say 25% selected randomly. This speeds up training, and trades higher underfitting for low chance of overfitting. 
        - 
        - Histogram-based Gradient Boosting another version of gradient boosting regression trees that is optimized for large datasets.
        - It bins inputs features and replaces them with integers. Binning can greatly reduce the number of possible thresholds that training algorithm needs to evaluate.
        - So continuous features like house prices are binned into say 255 bins (max number), and gradient boosting is run on these binned features
        - Histogram-based Gradient Boosting, sklearn:
        - 
        - More
        - 
        - 
        - XGBoost is a popular implementation of Gradient Boosting that balances pruning, Regularization, computational efficiency (Computational Complexity), careful memory organization to avoid cache misses, and allowing parallel computation.
        - XGBoost
        - 
    - 
    - Summary of when to use each Ensemble Model:
        - https://remnote-user-data.s3.amazonaws.com/OgIcErcqsdnQvsv2dTEzclZiZ3wKOAn_cfCFEWMX4NpKAQuVg-wyzLyJJBHoAxSneTihOAQyqtOjchyiOE5xzOS7CIuBz-0bHzUR7XWfynLE2wOjqO__in5dLM6foMZU.pnghttps://remnote-user-data.s3.amazonaws.com/zyQh7FlXOnhyDcSyl_iCvNensKzMsy8crL-K_DCaVKsMZYKUQcWohkVzHGMr9q2arAqeSOxU8JuXMCrUPeKQHzCJL8SL8-r6h9BV0P-kSJeBl7OVjglcpLmnt4VAy1WF.pnghttps://remnote-user-data.s3.amazonaws.com/OgIcErcqsdnQvsv2dTEzclZiZ3wKOAn_cfCFEWMX4NpKAQuVg-wyzLyJJBHoAxSneTihOAQyqtOjchyiOE5xzOS7CIuBz-0bHzUR7XWfynLE2wOjqO__in5dLM6foMZU.png
        - 
    - Summary of all Ensemble Models:
        - https://remnote-user-data.s3.amazonaws.com/oCn6YFmb_iIz3LSPHD2-13M1-mWZ7mStIJxMy1DEwuLFa2uT8BAAAjiP47ZNJ3oDSPHf0QuF7uBQq33Nb2LhduHwzOUncS-ukNK7xBWSD0C-qnQxnc9eVXRMK-hSvvHb.pnghttps://remnote-user-data.s3.amazonaws.com/l7ZTk9V2rfgpw_qCQpxPJsnEr0ht0FXxPekUCfY0RTKqc6S3-R59LuTx4kWSRCoUutreGz3xlWu0s4jchRoNi46__oPYV383W5UvOgQt3zzR_7CQ5iUmxXFOZOtWD6gU.pnghttps://remnote-user-data.s3.amazonaws.com/mI_UM6nf_G4FvnASOKGRceGTp17jsiIuTh7WmkYFwPU50wcul5oHwb87qXqM0S6WRDL9N5Uwf1qujrLQBrc4S9bGZD5KfpJ1aMNGO-cfV1LFe32RrBTrqxQurMChfnOD.png
        - 
    - 
- Online Learning
    - https://remnote-user-data.s3.amazonaws.com/c_-vmLEFwL0ltI72jwxyAAH5JKt9_i7WU3_GiQnXh2-5k03aVGewyKxcIOPAGAlk6v-CurED3JydzY1GZfLvXQ5EyCOAYiqTnUyatgexIwjgAgnAqGAgisuHV3m0n8-X.pnghttps://remnote-user-data.s3.amazonaws.com/mCwcWARgoH--Jf90lodFTl8Ju6kfzhoFBEGBcQ5_zvG2DYi48Vu2X156yr8iPib7_L7HWlaS8Ddk7JwEpgA0bYlD9vqBDDn-xu05-ihOpcRT4ll4BgKV1M_63QvPocw3.pnghttps://remnote-user-data.s3.amazonaws.com/mCwcWARgoH--Jf90lodFTl8Ju6kfzhoFBEGBcQ5_zvG2DYi48Vu2X156yr8iPib7_L7HWlaS8Ddk7JwEpgA0bYlD9vqBDDn-xu05-ihOpcRT4ll4BgKV1M_63QvPocw3.pnghttps://remnote-user-data.s3.amazonaws.com/c_-vmLEFwL0ltI72jwxyAAH5JKt9_i7WU3_GiQnXh2-5k03aVGewyKxcIOPAGAlk6v-CurED3JydzY1GZfLvXQ5EyCOAYiqTnUyatgexIwjgAgnAqGAgisuHV3m0n8-X.png
    - 
    - 
    - 
- 
- Classification
    - Classification is a Supervised Learning task of learning a function (Model) mapping an input point to a discrete category.
    - Multiclassification
        - Some models can right away handle Multiclassification, others can only do binary classification, but there are some techniques to make them work on several classes: One-vs-rest, One-vs-one.
        - An example of One-vs-rest and One-vs-one implementation in sklearn:
        - 
    - Multilabel Classification
        - Multilabel Classification is a class of classification problems where model needs to classify several independent labels at once, say what people are on the picture.
        - Here's an example of its implementation in sklearn:
        - 
        - There is also a sub-class of multilabel classification problems called Multioutput Classification. This is when model need to classify several independent labels at once, and each label can take several values (i.e. not binary). (See its basic sklearn implementation for image noise cleaning.)
        - 
    - 
    - Notes
        -  is a nearest-neighbors Classification algorithm that, given an input, choose its class based on classes of the k nearest data points (neighbors) to that input. 
        - Weight and input Vectors.
        - Perceptron Learning Rule
        - Decision Threshold
        - 
        - Terminology
        - Logistic Regression
            - Basics
            - Cost Function for Logistic Regression
            - Gradient Descent for Logistic Regression
            - 
        - 
    - 
- Regression
    - Regression is a Supervised Learning task of learning a function (Model) mapping an input point to a continuous value.
    - 
    - Nonparametric Regression (Instance-Based Learning)
        - Piecewise-linear Nonparametric Regression, aka connect the dots.; K-nearest Neighbors Regression, Locally Weighted Regression, Kernel:
            - https://remnote-user-data.s3.amazonaws.com/j3xyd1f18__qRiVtZy_zHXKRTejGXa8N3xgkVUcNuzzd9z3NEZaJCd6zZe1JQs8_vMlUr7iS5FMZsOjs015VNBEx0VlQhtRGGXI2bpdTZFmR64awRIJaZED39lpBOFMs.pnghttps://remnote-user-data.s3.amazonaws.com/gdcidZ6zCbMnrnw2g5yytyJVgDKQvV8eW4qqg1t-biEmcXkU92A0ruqC3i2lk0c_XQdbymKMeY_3PdCj2yGvapQx9Y6AwQE32z8DvVKLD-eS_G0vQi2RdkoYMIW31Yqr.pnghttps://remnote-user-data.s3.amazonaws.com/gdcidZ6zCbMnrnw2g5yytyJVgDKQvV8eW4qqg1t-biEmcXkU92A0ruqC3i2lk0c_XQdbymKMeY_3PdCj2yGvapQx9Y6AwQE32z8DvVKLD-eS_G0vQi2RdkoYMIW31Yqr.pnghttps://remnote-user-data.s3.amazonaws.com/j3xyd1f18__qRiVtZy_zHXKRTejGXa8N3xgkVUcNuzzd9z3NEZaJCd6zZe1JQs8_vMlUr7iS5FMZsOjs015VNBEx0VlQhtRGGXI2bpdTZFmR64awRIJaZED39lpBOFMs.png
            - https://remnote-user-data.s3.amazonaws.com/Q0jWmJ4tjp7VKa9k0n-LiXkb3drZUr-yXfZixWikVcMXdXA_unRTMD3GiaVZ0LTzrKDiFwPmKw5e6m-VtSOkczweh-Hkfc7ZlsVflmyUns9tXxktma24Jp_fUP9w078j.pnghttps://remnote-user-data.s3.amazonaws.com/Q0jWmJ4tjp7VKa9k0n-LiXkb3drZUr-yXfZixWikVcMXdXA_unRTMD3GiaVZ0LTzrKDiFwPmKw5e6m-VtSOkczweh-Hkfc7ZlsVflmyUns9tXxktma24Jp_fUP9w078j.png
            - 
        - 
        - 
    - 
- 
- Notes
    - AIMA 19 chapter summary:
    - 
    - So we have all those Models (algorithms), the important question is: How do we evaluate them?
    - In a way, this question is an Optimization Problem. As if we can evaluate them, we can optimize them based on that metric. Here comes the:
    - Loss Function is a function that expresses how poorly our hypothesis performs.
    - Yet, when we introduce any function to optimize for, we have to address the Overfitting problem.
    - Overfitting is a model that fits too closely to a particular data set and therefore may fail to generalize to future data.
    - Regularization is a process of penalizing hypotheses that are more complex to favor simpler, more general hypotheses.
    - Besides Regularization, another way to counter Overfitting is to do Cross-Validation.
    - Cross-Validation is a process of splitting data into a Training Set and a Test Set, such that learning happens on the training set and is evaluated on the test set.
    - K-fold Cross-validation is a process of splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data as training set.
    - 
- 
