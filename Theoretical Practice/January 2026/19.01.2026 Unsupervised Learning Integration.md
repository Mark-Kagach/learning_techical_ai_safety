# Goal

The goal is to practice explaining all concepts from unsupervised learning. If time allows, then explain all supervised learning concepts. 

# Debrief
1. 

## Questions
1. 
## Answers
2. 

# Practice

So, ml can be divided into supervised, unsupervised, and reinforcement learning parts which is based on how we give feedback to models. 

Unsupervised learning is when there are no target labels. There are three main parts of unsupervised learning: dimensionality reduction, clustering, and anomaly detection. Let's go one by one.

## Dimensionality Reduction
There are two main approaches in anomaly detection, projection and manifold learning. They differ in what assumption about the data they make, projection assumes we can reduce dimensions through straight axis, while manifold through unrolling/ unfolding data. With some data you can't do good dimensionality reduction with straight axis because it is rolled-up, in such cases we use manifold learning, and vice versa.

We reduce dimensions in hope that it will be easier to transform simpler data.

There are three main dimensionality reduction techniques we'll talk about: PCA, Random Projection, LLE.

### PCA
PCA is a projection technique, it uses straight axis as dimensions. It works somewhat simply. 

First it searches for axis that would preserve the most amount of variance compared to original data. Then it searches for the second best axis that would preserve the most variance. Yet, it takes into consideration the first axis, and to avoid double-preservation of the same variance by different axis it searches in purely independent area: orthogonal to first axis. Then it does the same with the third axis, searching in orthogonal space to first two axis. And so on for the same number of axis as in original data dimensions.

Then we choose how much variance we'd like to preserve from original data, say 95%, and pick in descending order the top axis that would sum up to 95%. That usually leads to serious dimensionality reduction.

First search can be quite costly, so in incremental pca we search for first good enough approximation of most-variance preserving axis, and go from there. In mini-batch pca we feed parts of training data, foregoing the requirement of all training data being loaded up in memory for pca algorithm to work.

I think that's mostly it for pca.

ðŸŸ¢ => All good, only mixed up incremental and random pca, should be in reverse. Not mini-batch pca but incremental, and randomized pca where I said incremental pca.

### Random Projection
Random projection is one of the most vivid examples of power of statistics theory I have seen. Basically given the number of dimensions and how much dilution (in correct distance) between any two points you're willing to accept. You can plug them in a formula and get the number of random projections you need to generate that would with sufficient confidence do not exceed this dilution. 

Say with 20,000 features, and 10% dilution acceptance, we'll randomly generate 7,300 dimensions and they would be enough.

ðŸŸ¢ => good.

### LLE
I wanted to joke how I don't remember LLE meaning, but I actually do: linear local embeddings, or local linear embeddings. Pretty telling name once you know what's it about. So, what's it about?

Basically take every instance (say x) and find its k-nearest neighbors, say 10. Then try to recreate that x as a linear function of those 10 nearest neighbors: Sum(w0*neighbor_0+w1*neighbor_1 etc...) So you're searching for appropriate neighbor weights in that linear function.
Now do that for every instance in your training data (as you can see this is quite an expensive operation, and if I recall correctly LLE doesn't scale well cause of that).
Put all those weights in a separate matrix W_hat. This matrix has weights that are about local relationships between each instance in your training data.

Now you take this weight matrix and ask a smaller dimension Z to approximate it well. So we're trying to fit this relationship between points onto smaller dimensions. And ideally it works.

The limitation is that we have local relationships as we're taking k-nearest neighbors, so universal relationships between points that are far away from each other won't hold.

What else about LLE? idk, its manifold learning, 

ðŸŸ¢ => correct on all fronts, was right on that it doesn't scale, just recall more confidently next time.

## Clustering
Now clustering. There are 3 main approaches to talk about.

### K-means
Basically using k-means for clustering. spherical data...
Randomly pick centroid spots, calculate the inertia i.e. the distance between all closest points, then shift centroids to minimize inertia. Do until convergence. May lead to local optimum's, so do several runs.

To pick number of clusters we can use elbow of total inertia and number of clusters, but better silhouette score which is the mean of silhouette coefficients of all instances. And ideally silhouette diagram. Silhouette coefficient is a coefficient that compares how distance between instance's cluster and its other closest cluster. With -1 meaning its closer to other cluster than it's assigned to, with +1 meaning its close to the assigned cluster, and 0 that it's in-between. ðŸŸ¡ => silh. coef. compares how close instance is to instances of its own cluster, vs the instances of other closest class.

Is that generally it for k-means clustering? ðŸŸ¡ => there's hard clustering and soft clustering when each instance gets assigned probability distribution of being in each class vs just giving it a class. Soft clustering is a very efficient dim. red. technique if you use prob. distr. of each instance's cluster likelihood as features.

ðŸŸ¡ => was having hard time recalling. 

### DBSCAN
For every training instance find the number of closest neighbors that pass some set e threshold distance. If instance has more or equal number of minimum_neighbors, it is considered to be a core instance. Instances that are neighbors to the core instances should belong to the same cluster.

If some instance is not part of any core instance it is considered an anomaly.

We use not silhouette score, but theoretical information gain, which I don't know what means, but I don't mind that... Or was that for gaussian mixture models? ðŸ”´ => It is gaussian mixture models, also what would we even use silhouette score here for? Was writing habitually without a thought.

ðŸŸ¡ => was having hard time recalling. 

### Gaussian Mixture Models
Assuming that data was generated randomly by several gaussian distributions and our goal is to find them. If you randomly generate data with gaussian distribution you'll see it looks like an ellipsoid. Hence, gaussian mixture models work best for this kind of clustering.

The algorithm finds appropriate clusters in a similar process to k-means clustering. It randomly guesses cluster's mean, variance, etc. Then calculates some loss function value I don't remember, then tries again until converges.

ðŸŸ¢ => good recall.

## Anomaly Detection
Anomaly detection is just taking gaussian mixture models and setting the epsilon threshold value beyond which instance should be considered an anomaly. Usually you'll have idea the amount of anomalies in your data, say 2%, and then you just set threshold so it would detect this amount of anomalies.

ðŸŸ¢

Cool, now let's repeat supervised learning one more time.

## Supervised Learning

SL is the main part of ML, it is about data that has output labels associated with inputs, and algorithms that we can get from those inputs to these specific outputs.

The main algorithms are:

### Linear Functions
Simplest SL algorithm that create a linear model based on training data. It's a perfect place to talk about the general algorithm nearly all ML algorithms run:

1. Make guesses based on random weights for all of the training instances (if batch gradient descent).
2. Calculate feedback, i.e. emprical loss based on loss function, i.e. the amount of mistakes made. For linear regression that would be mean squared error.
3. Calculate how to update weights for a new guess by plugging empirical loss into gradient descent. That would compute new weights based on derivative of emprical loss.
4. Repeat until derivative is zero meaning any change would worsen empirical loss and you have reached the best weights.

The issue is that we could overfit the data, so we to regularize our model. We do that by adding a penalty for big weights to loss function, so weights would be optimized to be small.

The penalty can be either sum of absolute weights, sum of squared weights, or adding both. â“ => is it sum or mean?
1. Lasso Regression, L1, absolute weights, tends to drive weights to zero, good because feature selection, but can be bad if too little features or features are correlated.
2. Ridge Regression, L2, squared weights
3. Elastic Net, adding both to loss function.

Besides regression we can do classification with linear functions.

Either by using hard threshold, say more or less and equal to some value of linear function, say zero. But then we can't use gradient descent to train model as because of hard-threshold it would always be say either 0th class, or 1st class, so derivative would be zero at all times but that jump between the two classes. So instead we use perceptron learning rule which shifts the weights everytime we missclasify an example.

We can also use logistic regression to get probabilistic output and we could use gradient descent on it.

Yet, if we want multiclassification we need multinomial logistic regression, and softmax is the most popular method. It uses argmax in the end to pick the most likely class. Softmax's loss function is cross entropy that compares probability distribution our model created for classes, and "reality's probability distirbution" based on classes we receive. Basically we're trying to minimize the necessary number of bits we need to transmit to tell what class example is over all training set. Where examples with higher probabilities in our model, say 90% need less bits to transmit to confirm their class than those that have say 5%. 

That's generally it for linear functions.

ðŸŸ¢ Good recall.

### KNN
KNN is about picking closest neighbors to example based on the training set. It is an istance based model so it scales well because we forego costly upfront model training, and works well for online learning as it is easy to add/remove examples. It works well only with a few features 2-10, as with more we have curse of dimensionality and space becomes too big, training instances are too far from each other. 

Distance calculated between instances depends on feature type and we can specify it in hyperparameters. We can also specify neighbors number obviously. And we can specify whether in deciding class/ regression it counts how close neighbors are based on weights hyperparameter.

â“ Can we do regression with knn? weird that I haven't asked that question before. => Yes ofc, averaged of target values of k-neighbors.
â“ Also, can we do probabilistic classification with knn? Yes, based on neighbors n? => Yes absed on neighbor proportion.

ðŸŸ¢ Good recall, but weird how long it took me to ask those obvious questions about regression and probabilistic classification.
