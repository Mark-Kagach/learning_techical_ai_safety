# Goal
I'm continuing with focusing on integrating existing material before adding new one. Specifically integrating explaining all supervised ml models.

# Debrief
1. Search in complex environments is about environments that are: continuous, nondeterministic, partially observable, and dynamic.
2. âš ï¸ In general, it shows this is my first proper practice explaining supervised algorithms. I did slack on integrating it with explanations. Now I'll go through my notes and write here what's worth integrating.

### Questions 
1. Do instance based models follow guess, feedback, update loop? They don't right, only model-based algorithms do.
2. ML is divided based on types of feedback we give to models?
3. Difference between lasso and ridge regressions? Why use one or the other?

### Answers
1. Indeed they don't, so keep that in mind when explaining.
2. Cursor: Yes, ML is divided into three main categories based on feedback:
   - **Supervised Learning**: Feedback is explicit labels/answers for each training example (e.g., "this image is a cat")
   - **Unsupervised Learning**: No explicit feedback; model finds patterns/structure in data without labels (e.g., clustering, dimensionality reduction)
   - **Reinforcement Learning**: Feedback is delayed rewards/penalties based on actions taken in an environment (e.g., game score, agent performance)

# Explanations

### Yesterday Recalling
I want to start by recalling my yesterday feynman practice mistakes. 

First, I was having hard time recalling ai sub-fields well. There are 3: search; logic, knowledge, reasoning and planning; uncertain knowledge and reasoning.

Search is divided into:
1. Search in simple environments
    Fully observable, discrete, known, episodic?
    Mainly it is about informed and uninformed search, with heuristic function or without.

2. Search in complex environments
    Here we are just one by one exploring more complex environments than one in first part.
    The main one is local search which is done over continuous states, gradient descent is a type of local search.

    Then we have search in partially observable env., ... ðŸŸ¡ => I don't really remember that part well. => continuous, nondeterministic, partially observable, and dynamic.

3. Constraint Satisfaction Problem
    If in the first two parts we represent environment as atomic representation, here we use factored, so states have variables with values and we're searching for the right state given some requirements.

4. Adversarial Search
    Any multiplayer competitive games.

That is with search.

For logic basically we're trying to create agents that know and reason about things. We need knowledge representation language and logic fits the bill well. We start with propositional logic which is very simple and thus not so expressive. We then move onto propositional logic which also establishes connections between objects. The holy grail of this ai subfield is to find explicitly defined expressive language that can be applied to many problems. We have knowledge base we construct, and program then does inference on it. We also can do planning with such programs, one thing I forgot yesterday. I think this is a sufficient recall of logic subfield.

Now uncertain knowledge and reasoning. Basically we introduce likelihood of events happening, which is very useful, and utility of states. This way we get decision theory, and agents can navigate environments more efficiently based on expected value. We're still heavily hand-crafting those agents. I think this is also a sufficient recall of this subfield.

---
What are other mistakes I have made?

Environment types, forgot deterministic vs nondeterministic, and static vs dynamic which is about being able to think and environment not changing while you do that. Agent types, the simple-reflex one, and the model based one. 

I explained panoramic stuff pretty well. Talking about computers, turing's insight, ai is just implicit coding cause of learning alg... Talking about ML project steps, choice of the right model based on hypothesis space, using data and compute to search the space, data as model's whole world, guess, feedback, update loop.

ok, let's see what I could be missing and then go to supervised ml.

=> Actually I recalled the main parts. Let's get on with supervised ml.

### ML Supervised Learning

ML seems to be divided into three parts based on the types of feedback we give to models. 

With supervised this is about having clear outputs we're trying to get based on the inputs we have. 

The main techniques for supervised learning are:

1. Linear functions
2. KNN
3. Decision Trees
4. Support Vector Machines
5. Ensemble Learning
6. Online learning -- recommender systems?

â“ Do instance based models follow guess, feedback, update loop? They don't right, only model-based algorithms do. => Indeed they don't.

In linear function we have a mathematical model that has some hypothesis space, which is very narrow. We are trying to minimize residual errors on the given training set 

In knn we take the closest neighbors, there are different ways of calculating it.

In decision trees? ... We pick the threshold that decreases most entropy? we could do that randomly too based on \sqrt n of input features? or training examples?

In support vector machines we ... pick the line that is the furtherst away from points?

In ensemble learning I feel safer. We put models together hoping that they do better, and they usually do. Putting models together doesn't help to solve underfitting if we have not powerful models (like linear regressions), but it does help to fix overfitting, so ensemble learning is better with powerful models like decision trees. 

An ensemble of decision trees is called random forests. Random because to get different trees we need to introduce some randomness to how we create trees. One way is by giving different training sets (â“ is that so?), another is by picking sqrt n of input features and choosing on them, another is extra trees when we pick thresholds randomly. 

Stacking, boosting, 

One approach is to train very different models on the same training data, another is to train same models but on different parts of the training data. We could use simple voting functions like hard with just N of votes, or soft with probabilities, or we could train a model on its own based on model predictions.

---

âš ï¸ In general, it shows this is my first proper practice explaining supervised algorithms. I did slack on integrating it with explanations. Now I'll go through my notes and write here what's worth integrating.

---

# Debrief

This practice is all basically about debriefing as I'm just going over my supervised learning notes and deciding what matters for long-term integration.

First, machine learning is divided based on the types of feedback (or lack thereof) we give to models when training. This gives us three parts: supervised, unsupervised, and RL. I like to differentiate DL and RL into separate parts, and I haven't studied them, so we won't talk about it now.

In fact we'll talk just about supervised, where we have clear output values for each training instance. (In unsupervised we don't.)

Picking model is about what hypothesis space you want to consider. You need to watch out the complexity-expressiveness tradeoff.

## Linear Functions

So let's start with linear functions. Say we have defined our mathematical model with two weights, w0 and w1. Now we need to find those that do best on our training set.

Guess
    We start either with weights being zero or random values. 
    Using these weights we make a prediction of y based on x.
    (Assume we don't do closed form solution.)

Feedback
    We define loss function, function that gives feedback to our guesses. 
    For example, sum of squared residuals. 
    Empirical loss is loss function applied to the training set.
    Loss function generates feedback.

Update
    We use that feedback to update the weights. For example, we use gradient descent to update the weights.

We try again. Eventually we reach plateau, where loss function doesn't change of improve, we pick corresponding weights.

> Cursor: "### Detailed Walkthrough with Example
>
> Let's make this concrete with a simple example. Say we have training data:
> - x = 2, y = 5 (actual value)
> - x = 4, y = 9 (actual value)
> - x = 6, y = 13 (actual value)
>
> Our model is: **Å· = wâ‚€ + wâ‚Â·x** (where Å· is our prediction)
>
> **Step 1: GUESS (Initialization)**
> - We start with random weights, say: wâ‚€ = 1, wâ‚ = 1
> - Now we make predictions:
>   - For x=2: Å· = 1 + 1Â·2 = 3 (but actual y=5, so error = 5-3 = 2)
>   - For x=4: Å· = 1 + 1Â·4 = 5 (but actual y=9, so error = 9-5 = 4)
>   - For x=6: Å· = 1 + 1Â·6 = 7 (but actual y=13, so error = 13-7 = 6)
>
> **Step 2: FEEDBACK (Calculate Loss)**
> - Loss function = Sum of Squared Residuals = Î£(actual - predicted)Â²
> - Loss = (5-3)Â² + (9-5)Â² + (13-7)Â² = 2Â² + 4Â² + 6Â² = 4 + 16 + 36 = **56**
> - This tells us: "Your predictions are way off! Total squared error is 56."
>
> **Step 3: UPDATE (Gradient Descent)**
> - Gradient descent asks: "Which direction should I move wâ‚€ and wâ‚ to reduce the loss?"
> - It calculates the gradient (derivative) of the loss with respect to each weight
> - The gradient points in the direction of steepest increase, so we move in the opposite direction
> - For example, it might tell us:
>   - "Increase wâ‚€ by 0.5" â†’ wâ‚€ becomes 1.5
>   - "Increase wâ‚ by 0.8" â†’ wâ‚ becomes 1.8
> - (The actual amounts depend on the learning rate and gradient calculation)
>
> **Step 4: REPEAT**
> - With new weights (wâ‚€=1.5, wâ‚=1.8), we make new predictions:
>   - For x=2: Å· = 1.5 + 1.8Â·2 = 5.1 (error = 5-5.1 = -0.1, much better!)
>   - For x=4: Å· = 1.5 + 1.8Â·4 = 8.7 (error = 9-8.7 = 0.3)
>   - For x=6: Å· = 1.5 + 1.8Â·6 = 12.3 (error = 13-12.3 = 0.7)
> - New loss = (-0.1)Â² + (0.3)Â² + (0.7)Â² = 0.01 + 0.09 + 0.49 = **0.59** (much better than 56!)
> - We update again, and again, each time getting closer to the optimal weights
>
> **Step 5: CONVERGENCE**
> - After many iterations, the loss stops decreasing significantly
> - We've found weights (say wâ‚€â‰ˆ1, wâ‚â‰ˆ2) that minimize the loss
> - The model Å· = 1 + 2x fits our data well!
>
> **Key Insight**: The loss function is like a "score" that tells us how bad our current guess is. Gradient descent uses calculus to find the direction that will improve this score fastest, then we take small steps in that direction until we can't improve anymore."

Sometimes we'll have too powerful model for our data, say 12 degree polynomial for 12 data points. We'll memorize that training data perfectly, but fail to learn generalized patter for test set. To avoid that, we try to constraint hypothesis space and make our model simpler (i.e. smalller hypothesis space? => yes.). We do that by either simplifying the model directly, say doing only a 2-degree polynomial, or by adding regularization to our loss function.

> Cursor: "Basically, regularization will penalize complexity, so the model is discouraged from learning large weights/parameters. This effectively constrains the hypothesis space (making it smaller) and helps prevent overfitting. Common regularization techniques add penalty terms to the loss function:
> - **L2 regularization (Ridge)**: Penalizes the sum of squared weights (Î»âˆ‘wÂ²)
> - **L1 regularization (Lasso)**: Penalizes the sum of absolute weights (Î»âˆ‘|w|)
> 
> By adding these penalty terms, the model must balance fitting the training data well (low training loss) with keeping weights small (low regularization penalty). This trade-off naturally leads to simpler, more generalizable models."






