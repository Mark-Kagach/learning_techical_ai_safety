# Goal
To understand unsupervised learning chapter in HOMLP. And make notes on it from which I could proceed with integration step.

# Notes

- Unsupervised Learning is an algorithm that learns to identify patterns from raw data, without target labels or human supervision.
- Yann LeCun's Cake:
- 
- Dimensionality Reduction
    - There are different approaches to dimensionality reduction, we do them in hope of making data manipulations easier.
    - Projection is a simpler method focused on "flattening" the data, like shining a flashlight on a 3D object and looking at its 2D shadow. It assumes data sits on a linear subspace inside a high-dimensional space. So to reduce dimensions it searches for the best angle to look at the data and project it on a flat plane.
    - Manifold Learning is a more complex method, focused on "unrolling" the data. It assumes data has curved, non-linear shape, so it tries to learn (hence the manifold learning) the shape of the curve and then unroll it.
    - 
    - Principal Component Analysis
        - Principal Component Analysis (PCA) is by far the most popular Dimensionality Reduction technique.
        - It searches for the axis that preserves the most Variance (Highest Variance Model). When it finds one, next axis should explain the most of remaining variance, and so on recursively for as many dimensions as in the dataset.
        - To force principal components to explain remaining variance, and not repeat past axis we force them to search only Orthogonal lines to all past axis. Orthogonality explained:
        - Once you have all possible principal components for the data, you reduce dimensions picking d principal components. 
        - You pick d by setting a threshold of variance you want to preserve, say 95%, then pick whatever number of dimensions that get you there. sklearn, MNIST Dataset example:
        - 
        - More
            - Explained Variance Ratio is a super useful information, showing how much variance each dimension explains:
            - (Basically what you're doing with PCA is recreate dimensions from scratch based on descending variance it explains, and then pick top dimensions sum of which passes some variance threshold like 95%.)
            - 
            - PCA compression, decompression; Reconstruction Error:
            - Randomized PCA for faster search of first approximate axis:
            - Incremental PCA to solve the memory constraint issue by splitting training sets into mini-batches:
            - 
            - (i'th axis is the i'th principal component of the data.)
            - Singular Value Decomposition (SVD):
            - 
        - 
    - Random Projection
        - Random projection randomly generates linear dimensions. And it does that for the minimum necessary quantity to ensure, with high probability, that distances between instances won't change more than a given threshold.
        - Suppose you have 5,000 training instances, with 20,000 features, and you don't want the squared distance between any two instance to change by more than e=10%. Then you should project the data onto 7,300 dimensions! ...
        - Damn...
        - 
    - Locally Linear Embedding (LLE) 
        - Locally Linear Embedding is a nonlinear dimensionality reduction technique.
        - LLE determines how each training instance linearly relates to its nearest neighbors. Then its looks for a low-dimensional representation of the training set where these local relationships are best preserved.
        - This works well for unrolling twisted manifolds, especially with low noise. Yet computationally it doesn't scale, working well only for small and medium datasets.
        - Example of LLE unrolling Swiss roll and preserving local distances, but not larger scale ones:
        - 
        - For each training instance x_i LLE identifies its k-nearest neighbors. Then it tries to reconstruct the instance as linear function (that's why linear embedding!) of those neighbors. Searching for weights so that \sum_{j=1}^k w_{i,j}x^{(j)} \approx x^{(i)}
        - Putting those weights together we have the weight matrix \widehat{W} that encodes the local linear relationships between training instances.
        - Now LLE searches for lower dimensions Z that would preserve those weight relationships between training instances.
        - 
        - HOMLP, sklearn, LLE:
        - 
    - 
    - More:
        - HOMLP on Dimensionality Reduction, Curse of Dimensionality:
        - HOMLP, other Dimensionality Reduction techniques. Multidimensional Scaling, Isomap, t-SNE, Linear Discriminant Analysis, UMAP,
        - 
        - Understanding is, in some way, about performing Dimensionality Reduction, where you can compress information into just a few variables compared to original. Certainly related to Perspective.
        - 
    - Do exercises
    - 
- Clustering
    - K-means Clustering 
        - It start with random placement of centroids for each cluster and computes some Loss Function, like MSE. Then computes Gradient Descent for new position of centroids, which are like weights. Eventually it will converge on position with minimum Empirical Loss.
        - It might converge on local optimum. So you mitigate this risk by doing several runs with random centroid initializations:
        - 
        - You have to specify k number of clusters. One way to do that is to look at Elbow on the graph of inertia and number of clusters:
        - Using Elbow is rather rugged. A more accurate technique is Silhouette Score which is the mean Silhouette Coefficient over all training instances. 
        - Silhouette Coefficient measures how close instance is to instances of its own cluster vs of the other closest cluster. It ranges from +1 meaning it is well inside its own cluster, to -1 meaning point is closer to other cluster, with 0 meaning somewhere in between:
        - Silhouette Diagram is most informative for deciding on cluster number:
        - 
        - Hard Clustering is when each instance is assigned to a single cluster. 
        - Soft Clustering is when each instance gets a score for each cluster. The score can be distance to the centroid, or similarity score.
        - Soft Clustering can be used as a very efficient nonlinear Dimensionality Reduction, where the scores of each instance become new dimensions.
        - 
        - K-means Clustering is limited when clusters are not identical or spherical, we might want to use Gaussian Mixture Models (elliptical clusters) instead:
        - 
        - More
            - K-means Clustering Inertia:
            - K-means++ is an improved K-means Clustering algorithm that does smarter centroid initialization, keeping them distant from one another:
            - Elkan K-means, Minibatch K-means:
            - 
        - 
    - DBSCAN
        - DBSCAN takes every instance and counts the number of instances located within its \epsilon neighborhood. (You define \epsilon.) 
        - If instance has at least min_samples in its neighborhood it is considered a core instance.
        - All instances within a core instance's neighborhood belong to the same cluster. Instances outside of any core instances clusters are considered anomalies.
        - 
        - Such algorithm allows DBSCAN to identify clusters of arbitrary shapes, not just spherical like K-means Clustering.
        - DBSCAN in sklearn:
        - 
    - Gaussian Mixture Models for Clustering 
        - Gaussian Mixture Model is a probabilistic model that assumes instances were generated by a mixture of several Gaussian Distributions whose parameters are unknown.
        - Instances generated from a single Gaussian Distribution form a cluster that usually looks like an ellipsoid. That's why Gaussian Mixture Models are better at identifying ellipsoidal clusters.
        - 
        - In the simplest Gaussian Mixture Model variant you need to know in advance the k number of Gaussian Distributions:
        - 
        - You can't select clusters based on Silhouette Score as it isn't reliable for non-spherical data. Instead you use Theoretical Information Criterion:
        - Gaussian Mixture Model is trained by Expectation-Maximization Clustering Algorithm. It initializes cluster parameters randomly, then it assigns instances to clusters, and then updates cluster weights until convergence. Quite similar to K-means Clustering it finds cluster centers (\mu), but also their size, shape, orientation, and relative weights.
        - 
        - More
            - Gaussian Mixture Models in sklearn:
            - More on Theoretical Information Criterion, likelihood function:
            - Bayesian Gaussian Mixture Models:
            - 
        - 
    - More
        - Clustering vs Anomaly Detection vs Density Estimation: One is about grouping instances, the other about identifying out of norm instances, and the last one about modeling the underlying probability function that generated the data.
        - 
        - Use-cases for Clustering:
        - 
        - Clustering for Image Segmentation (Color Segmentation, Semantic Segmentation, Instance Segmentation):
        - Clustering for Semi-Supervised Learning:
        - Active Learning, Uncertainty Sampling:
        - 
        - Other Clustering algorithms:
        - 
    - 
- Anomaly Detection
    - Gaussian Mixture Models for Anomaly Detection
        - Any instance located in a low-density region can be considered an anomaly. To define which ones are you specify the density threshold.
        - You should have in mind the ratio of anomalies in your data, say 2%. Then you pick density threshold value that would classify around 2% of instances as anomalies.
        - sklearn example:
        - 
    - Other algorithms for Anomaly Detection and Novelty Detection:
    - 
- Semi-Supervised Learning
    - 
- 
