# Goal
Continuing with ML algorithms integration.

# Debrief
1. 

# Practice

### Linear Functions

Let's recall Linear Functions.

Following the training process:
1. Guess using random weights all training examples.
2. Feedback from loss function on all the training examples calculating empirical loss. (sum of squared residuals)
3. Update weight via batch gradient descent calculating the derivative of loss function and weight values
4. Repeat until derivative is zero, we reached the minima.

Now talking about regularization to avoid overfitting.
We add to loss function a penalty for big weights, either via squared weights, or absolute, or both.
1. Ridge Regression (L2), Squared ðŸ”´ (Lasso drives weights to zero, not ridge!)
2. Lasso Regression (L1), Absolute weight values => drives weights to zero, nice as feature selection, bad if features correlate.
3. Elastic Net -- adding both to loss function.

Now talking about classification.
Can be using hard threshold, but then we can't train with gradient descent as graph is almost always either 1 or 0. So we use perceptron rule which shifts weights when we missclasify them.
If we want probabilities we use logistic regression where we can use gradient descent!

If we want multiclassification we use softmax which is a multinomial logistic regression, where argmax acts as threshold return classes based on highest probability. 
Loss function for softmax is cross entropy, which calculates based on our probabilities how much additional bits are needed to transmit the target class. If a lot, then loss is high, think our probability in some training example is 1% for target class, so we need to add a lot of bits. VS when we have 99% for target class.

That's it.

### KNN

KNN is an instance based model. When given new instance x, it computes distance between x and all training examples available to it. It then picks K closest, to either classify x with majority vote, or do regression by taking the average of neighbors' target values. 

In hyperparameters you can decide n of neighbors, how you want to calculate distance (method depends on features type), and whether closer neighbors should have more weight.

Instance based models and KNN are better suited for large datasets because you avoid the expensive upfront cost. Also it works great for online learning where you continuously add/remove new examples.

That's it for KNN.

### Decision Trees

Decision tree is a very powerful supervised learning algorithm. CART is the most popular training algorithm. 
1. For each training subset at hand it takes every feature and all of its possible thresholds. (Because it tries all splits of training instances it is not an endless amount of thresholds.)
2. Then it calculates loss function for each threshold, for each feature. If classification it is the sum of weighted impurity of right and left splits, if regression weighted MSE. (Impurity is the disagreement in the node between target classes or target regression values.)
3. It greedily picks the best split. 
4. Then recurses into children until stopping criteria. It does only binary splits.

Decision trees are very prone to overfitting as it doesn't make a lot of assumptions about data. We control for it via hyperparameters.

DT can even approximate class probabilities by calculating the proportion of classes within a node.

That's it for decision trees.

### Support Vector Machines

